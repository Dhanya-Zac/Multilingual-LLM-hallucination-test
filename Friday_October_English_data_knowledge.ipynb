{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanya-Zac/Multilingual-LLM-hallucination-test/blob/main/Friday_October_English_data_knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNbpJcdFx-jP",
        "outputId": "5ed593b4-9ce6-4d17-ca87-448384acdf7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 15000 rows from English.csv to English_dataset.json\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple Usage Example for CSV to JSON Converter\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Simple, direct approach\n",
        "def simple_csv_to_json(csv_file, json_file, max_rows=15000):\n",
        "    \"\"\"\n",
        "    Simplest possible implementation\n",
        "    \"\"\"\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Keep first 15000 rows (delete everything from row 15001 onwards)\n",
        "    df = df.iloc[:max_rows]\n",
        "\n",
        "    # Convert to JSON (array of objects format)\n",
        "    df.to_json(json_file, orient='records', indent=2)\n",
        "\n",
        "    print(f\"Converted {len(df)} rows from {csv_file} to {json_file}\")\n",
        "\n",
        "\n",
        "# One-liner version (for small files)\n",
        "def oneliner_csv_to_json(csv_file, json_file):\n",
        "    \"\"\"Ultra-compact version\"\"\"\n",
        "    pd.read_csv(csv_file).iloc[:15000].to_json(json_file, orient='records', indent=2)\n",
        "\n",
        "\n",
        "# Usage examples\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Simple usage\n",
        "    simple_csv_to_json('English.csv', 'English_dataset.json')\n",
        "\n",
        "    # Example 2: One-liner\n",
        "    # oneliner_csv_to_json('data.csv', 'output.json')\n",
        "\n",
        "    # Example 3: With custom row limit\n",
        "    # simple_csv_to_json('data.csv', 'output.json', max_rows=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "4dPniGzB2n3F",
        "outputId": "8c47da8e-d8ce-4651-cd14-d12bd3d3e614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your dataset file (JSON or CSV)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f96ee309-b629-4933-9141-9af7a06cf610\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f96ee309-b629-4933-9141-9af7a06cf610\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving English_dataset.json to English_dataset (1).json\n",
            "Uploaded file: English_dataset (1).json\n",
            "Loaded 15000 records from English_dataset (1).json\n",
            "Example 0: question:  In which U.S. state were Henry Ford, Ma... -> Michigan\n",
            "Example 1: question:  The ‘Ring of Fire’ is in which ocean?\n",
            "\n",
            "... -> Pacific Ocean\n",
            "Example 2: question:  What is the main constituent of natural... -> Methane\n",
            "Example 3: question:  \"Who wrote the 12 volume novel sequence... -> Anthony Powell\n",
            "Example 4: question:  Conceived in 1999 in Melbourne, Austral... -> Moustaches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:   0%|          | 1/15000 [00:03<14:28:19,  3.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 0: question:  In which U.S. state were Henry Ford, Ma...\n",
            "Target: Michigan\n",
            "Correct count: 6/6\n",
            "Greedy: Michigan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 2/15000 [00:07<14:52:57,  3.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1: question:  The ‘Ring of Fire’ is in which ocean?\n",
            "\n",
            "...\n",
            "Target: Pacific Ocean\n",
            "Correct count: 6/6\n",
            "Greedy: The ‘Ring of Fire’ is in the Pacific Ocean\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 3/15000 [00:09<11:51:18,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 2: question:  What is the main constituent of natural...\n",
            "Target: Methane\n",
            "Correct count: 6/6\n",
            "Greedy: Methane\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:   0%|          | 70/15000 [02:13<7:53:29,  1.90s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2754312249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     dataset_creator = KnowledgeDatasetGPT(\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mpath_to_knowledge_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"datasets/gpt4o_mini/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"English\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2754312249.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_knowledge_dataset, dataset_name, model_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Create knowledge dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_knowledge_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_knowledge_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2754312249.py\u001b[0m in \u001b[0;36mcreate_knowledge_dataset\u001b[0;34m(self, initial_dataset, path_to_save)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mtemp_generations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_with_temperature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mgreedy_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mall_generations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_generations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgreedy_generation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2754312249.py\u001b[0m in \u001b[0;36mgenerate_greedy\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;34m\"\"\"Generate with greedy decoding (temperature=0)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Colab-specific imports ---\n",
        "from google.colab import userdata, files\n",
        "import io\n",
        "\n",
        "class KnowledgeDatasetGPT:\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_to_knowledge_dataset: str = \"datasets/\",\n",
        "        dataset_name: str = \"English\",\n",
        "        model_name: str = \"gpt-4o-mini\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the knowledge dataset creator for GPT-4o-mini\n",
        "\n",
        "        Args:\n",
        "            path_to_knowledge_dataset: Path to save the datasets\n",
        "            dataset_name: Name of the dataset (default: \"English\")\n",
        "            model_name: OpenAI model name\n",
        "        \"\"\"\n",
        "        # --- Get API key securely from Colab ---\n",
        "        api_key = userdata.get('gptapi')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key not found in Colab userdata. Please set it with userdata.set('gptapi', 'your_key').\")\n",
        "\n",
        "        # Set seeds for reproducibility\n",
        "        random.seed(42)\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Initialize tiktoken tokenizer\n",
        "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(path_to_knowledge_dataset, exist_ok=True)\n",
        "\n",
        "        # Load initial dataset\n",
        "        initial_dataset = self.load_manual_dataset()\n",
        "\n",
        "        # Create knowledge dataset\n",
        "        self.create_knowledge_dataset(initial_dataset, path_to_knowledge_dataset)\n",
        "\n",
        "\n",
        "    def load_manual_dataset(self) -> List[Tuple]:\n",
        "        \"\"\"\n",
        "        Allow manual upload of a dataset file (English.json or .csv)\n",
        "        Each row should contain at least 'question' and 'answer'.\n",
        "        \"\"\"\n",
        "        print(\"Please upload your dataset file (JSON or CSV)...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded. Please upload your English dataset.\")\n",
        "\n",
        "        file_name = list(uploaded.keys())[0]\n",
        "        print(f\"Uploaded file: {file_name}\")\n",
        "\n",
        "        # --- Parse JSON ---\n",
        "        if file_name.endswith(\".json\"):\n",
        "            data = json.load(io.BytesIO(uploaded[file_name]))\n",
        "        # --- Parse CSV ---\n",
        "        elif file_name.endswith(\".csv\"):\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "            data = df.to_dict(orient=\"records\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Please upload a JSON or CSV file.\")\n",
        "\n",
        "        print(f\"Loaded {len(data)} records from {file_name}\")\n",
        "\n",
        "        dataset = []\n",
        "        for i, row in enumerate(data):\n",
        "            if \"question\" not in row or \"answer\" not in row:\n",
        "                continue\n",
        "\n",
        "            prompt = f\"question: {row['question']}\\nanswer:\"\n",
        "            answer = str(row[\"answer\"]).strip()\n",
        "            answer_tokens = self.tokenize(answer)\n",
        "            dataset.append([prompt, answer, answer_tokens])\n",
        "\n",
        "            if i < 5:\n",
        "                print(f\"Example {i}: {prompt[:50]}... -> {answer}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"Tokenize text using tiktoken\"\"\"\n",
        "        return self.tokenizer.encode(text)\n",
        "\n",
        "\n",
        "    def generate_with_temperature(self, prompt: str, temperature: float = 0.5, n: int = 5) -> List[str]:\n",
        "        \"\"\"Generate n completions with specified temperature using OpenAI API\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=temperature,\n",
        "                n=n,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            return [choice.message.content.strip() for choice in response.choices]\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return [\"\"] * n\n",
        "\n",
        "\n",
        "    def generate_greedy(self, prompt: str) -> str:\n",
        "        \"\"\"Generate with greedy decoding (temperature=0)\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=0,\n",
        "                n=1,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during greedy generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "    def create_knowledge_dataset(self, initial_dataset: List[Tuple], path_to_save: str):\n",
        "        \"\"\"Create knowledge, non-knowledge, and else datasets based on model responses\"\"\"\n",
        "        knowledge_dataset, non_knowledge_dataset, else_dataset = [], [], []\n",
        "\n",
        "        few_shot_examples = [\n",
        "            \"question: What is the capital of France?\\nanswer: Paris\\n\",\n",
        "            \"question: Who wrote 'Romeo and Juliet'?\\nanswer: William Shakespeare\\n\",\n",
        "            \"question: What is the square root of 64?\\nanswer: 8\\n\",\n",
        "            \"question: Which element has the chemical symbol 'H'?\\nanswer: Hydrogen\\n\",\n",
        "            \"question: What is the currency of Japan?\\nanswer: Japanese Yen\\n\"\n",
        "        ]\n",
        "\n",
        "        for idx, point in enumerate(tqdm(initial_dataset, desc=\"Processing examples\")):\n",
        "            prompt, target_answer, answer_tokens = point\n",
        "\n",
        "            # Save progress periodically\n",
        "            if idx % 1000 == 0 and idx > 0:\n",
        "                print(f\"\\nProgress: {idx}/{len(initial_dataset)}\")\n",
        "                self._save_datasets(knowledge_dataset, non_knowledge_dataset, else_dataset, path_to_save)\n",
        "\n",
        "            few_shot_prompt = \"\".join(random.sample(few_shot_examples, 3))\n",
        "            full_prompt = few_shot_prompt + prompt\n",
        "\n",
        "            temp_generations = self.generate_with_temperature(full_prompt, temperature=0.5, n=5)\n",
        "            greedy_generation = self.generate_greedy(full_prompt)\n",
        "\n",
        "            all_generations = temp_generations + [greedy_generation]\n",
        "            count_correct = sum(target_answer.lower() in gen.lower() for gen in all_generations)\n",
        "\n",
        "            if count_correct == 6:\n",
        "                knowledge_dataset.append([prompt, target_answer, answer_tokens, count_correct])\n",
        "            elif count_correct == 0:\n",
        "                non_knowledge_dataset.append([prompt, target_answer, answer_tokens, count_correct])\n",
        "            else:\n",
        "                else_dataset.append([prompt, target_answer, answer_tokens, count_correct])\n",
        "\n",
        "            if idx < 3:\n",
        "                print(f\"\\nExample {idx}: {prompt[:50]}...\")\n",
        "                print(f\"Target: {target_answer}\")\n",
        "                print(f\"Correct count: {count_correct}/6\")\n",
        "                print(f\"Greedy: {greedy_generation}\")\n",
        "\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        print(f\"\\nFinal Results — Knowledge: {len(knowledge_dataset)}, Non-knowledge: {len(non_knowledge_dataset)}, Else: {len(else_dataset)}\")\n",
        "        self._save_datasets(knowledge_dataset, non_knowledge_dataset, else_dataset, path_to_save)\n",
        "\n",
        "\n",
        "    def _save_datasets(self, knowledge_dataset, non_knowledge_dataset, else_dataset, path_to_save):\n",
        "        \"\"\"Helper function to save datasets\"\"\"\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        for name, data in {\n",
        "            \"knowledge\": knowledge_dataset,\n",
        "            \"non_knowledge\": non_knowledge_dataset,\n",
        "            \"else\": else_dataset\n",
        "        }.items():\n",
        "            path = os.path.join(path_to_save, f\"{model_name_safe}_{dataset_name_safe}_{name}_dataset.json\")\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "            print(f\"Saved {name} dataset to {path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_creator = KnowledgeDatasetGPT(\n",
        "        path_to_knowledge_dataset=\"datasets/gpt4o_mini/\",\n",
        "        dataset_name=\"English\",\n",
        "        model_name=\"gpt-4o-mini\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GKW7iBSRznGK",
        "outputId": "cc26d64a-8c79-4ba3-b0f3-6887b40e64da"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your dataset file (JSON or CSV)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-661cb195-fcf0-4bda-af36-acea8cb8850c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-661cb195-fcf0-4bda-af36-acea8cb8850c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving English_dataset.json to English_dataset (1).json\n",
            "Uploaded file: English_dataset (1).json\n",
            "Loaded 15000 records from English_dataset (1).json\n",
            "Example 0: question:  In which U.S. state were Henry Ford, Ma... -> Michigan\n",
            "Example 1: question:  The ‘Ring of Fire’ is in which ocean?\n",
            "\n",
            "... -> Pacific Ocean\n",
            "Example 2: question:  What is the main constituent of natural... -> Methane\n",
            "Example 3: question:  \"Who wrote the 12 volume novel sequence... -> Anthony Powell\n",
            "Example 4: question:  Conceived in 1999 in Melbourne, Austral... -> Moustaches\n",
            "Processing 15000 examples in batches of 2000...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:   0%|          | 1/7000 [00:03<7:12:14,  3.71s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 0: question:  \"In which Sunday newspaper would you ge...\n",
            "Target: Mail on sunday\n",
            "Correct count: 1/6\n",
            "Greedy: The \"You Magazine\" supplement is found in the Sunday Times\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 2/7000 [00:07<6:52:07,  3.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1: question:  Famous for its military marches, in whi...\n",
            "Target: Wiltshire\n",
            "Correct count: 6/6\n",
            "Greedy: Wootton Bassett is located in Wiltshire, England\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 3/7000 [00:08<5:13:31,  2.69s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 2: question:  Also the name of a popular TV character...\n",
            "Target: Zebedee\n",
            "Correct count: 6/6\n",
            "Greedy: Zebedee\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  29%|██▊       | 2000/7000 [1:01:37<2:12:49,  1.59s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 5 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_knowledge_batch_0005.json (1381 examples)\n",
            "Saved non_knowledge batch 5 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_non_knowledge_batch_0005.json (454 examples)\n",
            "Saved else batch 5 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_else_batch_0005.json (165 examples)\n",
            "\n",
            "--- Batch 5 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge: 1381\n",
            "Non-knowledge: 454\n",
            "Else: 165\n",
            "Total processed so far: 2000/7000\n",
            "Running totals - K: 1381, NK: 454, E: 165\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  57%|█████▋    | 4000/7000 [2:03:58<1:36:35,  1.93s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 6 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_knowledge_batch_0006.json (1338 examples)\n",
            "Saved non_knowledge batch 6 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_non_knowledge_batch_0006.json (471 examples)\n",
            "Saved else batch 6 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_else_batch_0006.json (191 examples)\n",
            "\n",
            "--- Batch 6 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge: 1338\n",
            "Non-knowledge: 471\n",
            "Else: 191\n",
            "Total processed so far: 4000/7000\n",
            "Running totals - K: 2719, NK: 925, E: 356\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  86%|████████▌ | 6000/7000 [3:09:48<26:33,  1.59s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 7 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_knowledge_batch_0007.json (1334 examples)\n",
            "Saved non_knowledge batch 7 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_non_knowledge_batch_0007.json (476 examples)\n",
            "Saved else batch 7 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_else_batch_0007.json (190 examples)\n",
            "\n",
            "--- Batch 7 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge: 1334\n",
            "Non-knowledge: 476\n",
            "Else: 190\n",
            "Total processed so far: 6000/7000\n",
            "Running totals - K: 4053, NK: 1401, E: 546\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples: 100%|██████████| 7000/7000 [3:44:19<00:00,  1.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 8 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_knowledge_batch_0008.json (627 examples)\n",
            "Saved non_knowledge batch 8 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_non_knowledge_batch_0008.json (266 examples)\n",
            "Saved else batch 8 to datasets/gpt4o_mini/batches/gpt-4o-mini_English_else_batch_0008.json (107 examples)\n",
            "\n",
            "--- Batch 8 Summary ---\n",
            "Examples in batch: 1000\n",
            "Knowledge: 627\n",
            "Non-knowledge: 266\n",
            "Else: 107\n",
            "Total processed so far: 7000/7000\n",
            "Running totals - K: 4680, NK: 1667, E: 653\n",
            "\n",
            "\n",
            "Saved batch metadata to datasets/gpt4o_mini/gpt-4o-mini_English_metadata.json\n",
            "\n",
            "=== Final Results ===\n",
            "Total batches created: 8\n",
            "Total Knowledge: 4680\n",
            "Total Non-knowledge: 1667\n",
            "Total Else: 653\n",
            "Total examples processed: 7000\n",
            "\n",
            "Consolidating batches...\n",
            "Loaded 1381 examples from batch 5 for knowledge\n",
            "Loaded 1338 examples from batch 6 for knowledge\n",
            "Loaded 1334 examples from batch 7 for knowledge\n",
            "Loaded 627 examples from batch 8 for knowledge\n",
            "Saved consolidated knowledge dataset to datasets/gpt4o_mini/gpt-4o-mini_English_knowledge_dataset_consolidated.json (4680 total examples)\n",
            "Loaded 454 examples from batch 5 for non_knowledge\n",
            "Loaded 471 examples from batch 6 for non_knowledge\n",
            "Loaded 476 examples from batch 7 for non_knowledge\n",
            "Loaded 266 examples from batch 8 for non_knowledge\n",
            "Saved consolidated non_knowledge dataset to datasets/gpt4o_mini/gpt-4o-mini_English_non_knowledge_dataset_consolidated.json (1667 total examples)\n",
            "Loaded 165 examples from batch 5 for else\n",
            "Loaded 191 examples from batch 6 for else\n",
            "Loaded 190 examples from batch 7 for else\n",
            "Loaded 107 examples from batch 8 for else\n",
            "Saved consolidated else dataset to datasets/gpt4o_mini/gpt-4o-mini_English_else_dataset_consolidated.json (653 total examples)\n",
            "\n",
            "Consolidation complete!\n",
            "\n",
            "Do you want to delete the individual batch files? (y/n): n\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Colab-specific imports ---\n",
        "from google.colab import userdata, files\n",
        "import io\n",
        "\n",
        "class KnowledgeDatasetGPT:\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_to_knowledge_dataset: str = \"datasets/\",\n",
        "        dataset_name: str = \"English\",\n",
        "        model_name: str = \"gpt-4o-mini\",\n",
        "        batch_size: int = 2000\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the knowledge dataset creator for GPT-4o-mini with batch processing\n",
        "\n",
        "        Args:\n",
        "            path_to_knowledge_dataset: Path to save the datasets\n",
        "            dataset_name: Name of the dataset (default: \"English\")\n",
        "            model_name: OpenAI model name\n",
        "            batch_size: Number of examples to process before saving a batch (default: 2000)\n",
        "        \"\"\"\n",
        "        # --- Get API key securely from Colab ---\n",
        "        api_key = userdata.get('gptapi')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key not found in Colab userdata. Please set it with userdata.set('gptapi', 'your_key').\")\n",
        "\n",
        "        # Set seeds for reproducibility\n",
        "        random.seed(42)\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize tiktoken tokenizer\n",
        "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(path_to_knowledge_dataset, exist_ok=True)\n",
        "\n",
        "        # Create batch directory\n",
        "        self.batch_dir = os.path.join(path_to_knowledge_dataset, \"batches\")\n",
        "        os.makedirs(self.batch_dir, exist_ok=True)\n",
        "\n",
        "        # Load initial dataset\n",
        "        initial_dataset = self.load_manual_dataset()\n",
        "\n",
        "        # Create knowledge dataset with batch processing\n",
        "        self.create_knowledge_dataset(initial_dataset, path_to_knowledge_dataset)\n",
        "\n",
        "\n",
        "    def load_manual_dataset(self) -> List[Tuple]:\n",
        "        \"\"\"\n",
        "        Allow manual upload of a dataset file (English.json or .csv)\n",
        "        Each row should contain at least 'question' and 'answer'.\n",
        "        \"\"\"\n",
        "        print(\"Please upload your dataset file (JSON or CSV)...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded. Please upload your English dataset.\")\n",
        "\n",
        "        file_name = list(uploaded.keys())[0]\n",
        "        print(f\"Uploaded file: {file_name}\")\n",
        "\n",
        "        # --- Parse JSON ---\n",
        "        if file_name.endswith(\".json\"):\n",
        "            data = json.load(io.BytesIO(uploaded[file_name]))\n",
        "        # --- Parse CSV ---\n",
        "        elif file_name.endswith(\".csv\"):\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "            data = df.to_dict(orient=\"records\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Please upload a JSON or CSV file.\")\n",
        "\n",
        "        print(f\"Loaded {len(data)} records from {file_name}\")\n",
        "\n",
        "        dataset = []\n",
        "        for i, row in enumerate(data):\n",
        "            if \"question\" not in row or \"answer\" not in row:\n",
        "                continue\n",
        "\n",
        "            prompt = f\"question: {row['question']}\\nanswer:\"\n",
        "            answer = str(row[\"answer\"]).strip()\n",
        "            answer_tokens = self.tokenize(answer)\n",
        "            dataset.append([prompt, answer, answer_tokens])\n",
        "\n",
        "            if i < 5:\n",
        "                print(f\"Example {i}: {prompt[:50]}... -> {answer}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"Tokenize text using tiktoken\"\"\"\n",
        "        return self.tokenizer.encode(text)\n",
        "\n",
        "\n",
        "    def generate_with_temperature(self, prompt: str, temperature: float = 0.5, n: int = 5) -> List[str]:\n",
        "        \"\"\"Generate n completions with specified temperature using OpenAI API\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=temperature,\n",
        "                n=n,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            return [choice.message.content.strip() for choice in response.choices]\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return [\"\"] * n\n",
        "\n",
        "\n",
        "    def generate_greedy(self, prompt: str) -> str:\n",
        "        \"\"\"Generate with greedy decoding (temperature=0)\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=0,\n",
        "                n=1,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during greedy generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "    def create_knowledge_dataset(self, initial_dataset: List[Tuple], path_to_save: str):\n",
        "        \"\"\"Create knowledge, non-knowledge, and else datasets with batch processing\"\"\"\n",
        "\n",
        "        # Batch tracking\n",
        "        batch_num = 0\n",
        "        examples_in_batch = 0\n",
        "\n",
        "        # Current batch data\n",
        "        knowledge_dataset = []\n",
        "        non_knowledge_dataset = []\n",
        "        else_dataset = []\n",
        "\n",
        "        # Overall statistics\n",
        "        total_knowledge = 0\n",
        "        total_non_knowledge = 0\n",
        "        total_else = 0\n",
        "\n",
        "        # Metadata for tracking batches\n",
        "        batch_metadata = {\n",
        "            \"model\": self.model_name,\n",
        "            \"dataset_name\": self.dataset_name,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"batches\": []\n",
        "        }\n",
        "\n",
        "        few_shot_examples = [\n",
        "            \"question: What is the capital of France?\\nanswer: Paris\\n\",\n",
        "            \"question: Who wrote 'Romeo and Juliet'?\\nanswer: William Shakespeare\\n\",\n",
        "            \"question: What is the square root of 64?\\nanswer: 8\\n\",\n",
        "            \"question: Which element has the chemical symbol 'H'?\\nanswer: Hydrogen\\n\",\n",
        "            \"question: What is the currency of Japan?\\nanswer: Japanese Yen\\n\"\n",
        "        ]\n",
        "\n",
        "        print(f\"Processing {len(initial_dataset)} examples in batches of {self.batch_size}...\")\n",
        "        start_batch = 5  # ← change this number to resume from any batch you want\n",
        "        start_index = (start_batch - 1) * self.batch_size\n",
        "        initial_dataset = initial_dataset[start_index:]\n",
        "        batch_num = start_batch - 1\n",
        "        for idx, point in enumerate(tqdm(initial_dataset, desc=\"Processing examples\")):\n",
        "            prompt, target_answer, answer_tokens = point\n",
        "\n",
        "            # Generate few-shot prompt\n",
        "            few_shot_prompt = \"\".join(random.sample(few_shot_examples, 3))\n",
        "            full_prompt = few_shot_prompt + prompt\n",
        "\n",
        "            # Generate completions\n",
        "            temp_generations = self.generate_with_temperature(full_prompt, temperature=0.5, n=5)\n",
        "            greedy_generation = self.generate_greedy(full_prompt)\n",
        "\n",
        "            # Count correct answers\n",
        "            all_generations = temp_generations + [greedy_generation]\n",
        "            count_correct = sum(target_answer.lower() in gen.lower() for gen in all_generations)\n",
        "\n",
        "            # Classify and add to appropriate dataset\n",
        "            if count_correct == 6:\n",
        "                knowledge_dataset.append([prompt, target_answer, answer_tokens, count_correct])\n",
        "            elif count_correct == 0:\n",
        "                non_knowledge_dataset.append([prompt, target_answer, answer_tokens, count_correct])\n",
        "            else:\n",
        "                else_dataset.append([prompt, target_answer, answer_tokens, count_correct])\n",
        "\n",
        "            examples_in_batch += 1\n",
        "\n",
        "            # Print first few examples for debugging\n",
        "            if idx < 3:\n",
        "                print(f\"\\nExample {idx}: {prompt[:50]}...\")\n",
        "                print(f\"Target: {target_answer}\")\n",
        "                print(f\"Correct count: {count_correct}/6\")\n",
        "                print(f\"Greedy: {greedy_generation}\")\n",
        "\n",
        "            # Check if we should save a batch\n",
        "            if examples_in_batch >= self.batch_size or idx == len(initial_dataset) - 1:\n",
        "                batch_num += 1\n",
        "\n",
        "                # Save current batch\n",
        "                batch_info = self._save_batch(\n",
        "                    knowledge_dataset,\n",
        "                    non_knowledge_dataset,\n",
        "                    else_dataset,\n",
        "                    batch_num,\n",
        "                    self.batch_dir\n",
        "                )\n",
        "\n",
        "                # Update totals\n",
        "                total_knowledge += len(knowledge_dataset)\n",
        "                total_non_knowledge += len(non_knowledge_dataset)\n",
        "                total_else += len(else_dataset)\n",
        "\n",
        "                # Add batch info to metadata\n",
        "                batch_metadata[\"batches\"].append({\n",
        "                    \"batch_number\": batch_num,\n",
        "                    \"examples_processed\": examples_in_batch,\n",
        "                    \"knowledge_count\": len(knowledge_dataset),\n",
        "                    \"non_knowledge_count\": len(non_knowledge_dataset),\n",
        "                    \"else_count\": len(else_dataset),\n",
        "                    \"total_processed\": idx + 1\n",
        "                })\n",
        "\n",
        "                print(f\"\\n--- Batch {batch_num} Summary ---\")\n",
        "                print(f\"Examples in batch: {examples_in_batch}\")\n",
        "                print(f\"Knowledge: {len(knowledge_dataset)}\")\n",
        "                print(f\"Non-knowledge: {len(non_knowledge_dataset)}\")\n",
        "                print(f\"Else: {len(else_dataset)}\")\n",
        "                print(f\"Total processed so far: {idx + 1}/{len(initial_dataset)}\")\n",
        "                print(f\"Running totals - K: {total_knowledge}, NK: {total_non_knowledge}, E: {total_else}\\n\")\n",
        "\n",
        "                # Clear current batch data to free memory\n",
        "                knowledge_dataset = []\n",
        "                non_knowledge_dataset = []\n",
        "                else_dataset = []\n",
        "                examples_in_batch = 0\n",
        "\n",
        "            time.sleep(0.1)  # Rate limiting\n",
        "\n",
        "        # Save metadata\n",
        "        metadata_path = os.path.join(path_to_save, f\"{self.model_name.replace('/', '_')}_{self.dataset_name}_metadata.json\")\n",
        "        with open(metadata_path, \"w\") as f:\n",
        "            json.dump(batch_metadata, f, indent=2)\n",
        "        print(f\"\\nSaved batch metadata to {metadata_path}\")\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\n=== Final Results ===\")\n",
        "        print(f\"Total batches created: {batch_num}\")\n",
        "        print(f\"Total Knowledge: {total_knowledge}\")\n",
        "        print(f\"Total Non-knowledge: {total_non_knowledge}\")\n",
        "        print(f\"Total Else: {total_else}\")\n",
        "        print(f\"Total examples processed: {total_knowledge + total_non_knowledge + total_else}\")\n",
        "\n",
        "        # Ask if user wants to consolidate batches\n",
        "        consolidate = input(\"\\nDo you want to consolidate all batches into single files? (y/n): \")\n",
        "        if consolidate.lower() == 'y':\n",
        "            self._consolidate_batches(batch_num, self.batch_dir, path_to_save)\n",
        "\n",
        "\n",
        "    def _save_batch(self, knowledge_dataset, non_knowledge_dataset, else_dataset, batch_num, batch_dir):\n",
        "        \"\"\"Save a single batch of datasets\"\"\"\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        batch_info = {}\n",
        "\n",
        "        for name, data in {\n",
        "            \"knowledge\": knowledge_dataset,\n",
        "            \"non_knowledge\": non_knowledge_dataset,\n",
        "            \"else\": else_dataset\n",
        "        }.items():\n",
        "            filename = f\"{model_name_safe}_{dataset_name_safe}_{name}_batch_{batch_num:04d}.json\"\n",
        "            path = os.path.join(batch_dir, filename)\n",
        "\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "\n",
        "            batch_info[name] = {\n",
        "                \"filename\": filename,\n",
        "                \"count\": len(data),\n",
        "                \"path\": path\n",
        "            }\n",
        "\n",
        "            print(f\"Saved {name} batch {batch_num} to {path} ({len(data)} examples)\")\n",
        "\n",
        "        return batch_info\n",
        "\n",
        "\n",
        "    def _consolidate_batches(self, total_batches: int, batch_dir: str, final_dir: str):\n",
        "        \"\"\"Consolidate all batch files into single files for each category\"\"\"\n",
        "        print(\"\\nConsolidating batches...\")\n",
        "\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        for category in [\"knowledge\", \"non_knowledge\", \"else\"]:\n",
        "            consolidated_data = []\n",
        "\n",
        "            # Read all batch files for this category\n",
        "            for batch_num in range(1, total_batches + 1):\n",
        "                batch_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_batch_{batch_num:04d}.json\"\n",
        "                batch_path = os.path.join(batch_dir, batch_filename)\n",
        "\n",
        "                if os.path.exists(batch_path):\n",
        "                    with open(batch_path, \"r\") as f:\n",
        "                        batch_data = json.load(f)\n",
        "                        consolidated_data.extend(batch_data)\n",
        "                    print(f\"Loaded {len(batch_data)} examples from batch {batch_num} for {category}\")\n",
        "\n",
        "            # Save consolidated file\n",
        "            final_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_dataset_consolidated.json\"\n",
        "            final_path = os.path.join(final_dir, final_filename)\n",
        "\n",
        "            with open(final_path, \"w\") as f:\n",
        "                json.dump(consolidated_data, f, indent=2)\n",
        "\n",
        "            print(f\"Saved consolidated {category} dataset to {final_path} ({len(consolidated_data)} total examples)\")\n",
        "\n",
        "        print(\"\\nConsolidation complete!\")\n",
        "\n",
        "        # Ask if user wants to delete batch files\n",
        "        delete_batches = input(\"\\nDo you want to delete the individual batch files? (y/n): \")\n",
        "        if delete_batches.lower() == 'y':\n",
        "            import shutil\n",
        "            shutil.rmtree(batch_dir)\n",
        "            print(\"Batch files deleted.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize with batch_size parameter\n",
        "    dataset_creator = KnowledgeDatasetGPT(\n",
        "        path_to_knowledge_dataset=\"datasets/gpt4o_mini/\",\n",
        "        dataset_name=\"English\",\n",
        "        model_name=\"gpt-4o-mini\",\n",
        "        batch_size=2000  # Save every 2000 examples\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def consolidate_all_batches(batch_dir=\"/content\", final_dir=\"/content/final_consolidated\"):\n",
        "    \"\"\"\n",
        "    Combine all batches (1–4) and the already consolidated datasets (5–8)\n",
        "    into one final consolidated file per category.\n",
        "    \"\"\"\n",
        "    print(\"\\n🔄 Creating FINAL consolidated datasets...\")\n",
        "\n",
        "    model_name_safe = \"gpt-4o-mini\"\n",
        "    dataset_name_safe = \"English\"\n",
        "\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "    for category in [\"knowledge\", \"non_knowledge\", \"else\"]:\n",
        "        # Collect batch files (1–4)\n",
        "        batch_pattern = os.path.join(batch_dir, f\"{model_name_safe}_{dataset_name_safe}_{category}_batch_*.json\")\n",
        "        batch_files = sorted(glob.glob(batch_pattern))\n",
        "\n",
        "        # Add the already consolidated file (from batches 5–8)\n",
        "        old_consolidated = os.path.join(batch_dir, f\"{model_name_safe}_{dataset_name_safe}_{category}_dataset_consolidated.json\")\n",
        "\n",
        "        # Combine both\n",
        "        all_files = batch_files.copy()\n",
        "        if os.path.exists(old_consolidated):\n",
        "            all_files.append(old_consolidated)\n",
        "\n",
        "        if not all_files:\n",
        "            print(f\"⚠️ No files found for {category}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        consolidated_data = []\n",
        "        for path in all_files:\n",
        "            with open(path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                consolidated_data.extend(data)\n",
        "            print(f\"✅ Loaded {len(data)} examples from {os.path.basename(path)}\")\n",
        "\n",
        "        # Save final file\n",
        "        final_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_dataset_final_consolidated.json\"\n",
        "        final_path = os.path.join(final_dir, final_filename)\n",
        "\n",
        "        with open(final_path, \"w\") as f:\n",
        "            json.dump(consolidated_data, f, indent=2)\n",
        "\n",
        "        print(f\"📦 Saved FINAL {category} dataset to {final_path} ({len(consolidated_data)} total examples)\")\n",
        "\n",
        "    print(\"\\n🎯 Final consolidation complete!\")\n",
        "\n",
        "    # Optional: create one combined file with all categories\n",
        "    combined_data = []\n",
        "    for category in [\"knowledge\", \"non_knowledge\", \"else\"]:\n",
        "        final_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_dataset_final_consolidated.json\"\n",
        "        final_path = os.path.join(final_dir, final_filename)\n",
        "        if os.path.exists(final_path):\n",
        "            with open(final_path, \"r\") as f:\n",
        "                combined_data.extend(json.load(f))\n",
        "\n",
        "    combined_path = os.path.join(final_dir, f\"{model_name_safe}_{dataset_name_safe}_all_categories_final_consolidated.json\")\n",
        "    with open(combined_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=2)\n",
        "\n",
        "    print(f\"\\n🧩 Combined all categories into {combined_path} ({len(combined_data)} total examples)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    consolidate_all_batches()\n"
      ],
      "metadata": {
        "id": "8fqEgEa9izim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067b57b6-e5d6-4f4f-a610-7670af584614"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Creating FINAL consolidated datasets...\n",
            "✅ Loaded 1319 examples from gpt-4o-mini_English_knowledge_batch_0001.json\n",
            "✅ Loaded 1296 examples from gpt-4o-mini_English_knowledge_batch_0002.json\n",
            "✅ Loaded 1351 examples from gpt-4o-mini_English_knowledge_batch_0003.json\n",
            "✅ Loaded 1333 examples from gpt-4o-mini_English_knowledge_batch_0004.json\n",
            "✅ Loaded 4680 examples from gpt-4o-mini_English_knowledge_dataset_consolidated.json\n",
            "📦 Saved FINAL knowledge dataset to /content/final_consolidated/gpt-4o-mini_English_knowledge_dataset_final_consolidated.json (9979 total examples)\n",
            "✅ Loaded 476 examples from gpt-4o-mini_English_non_knowledge_batch_0001.json\n",
            "✅ Loaded 487 examples from gpt-4o-mini_English_non_knowledge_batch_0002.json\n",
            "✅ Loaded 446 examples from gpt-4o-mini_English_non_knowledge_batch_0003.json\n",
            "✅ Loaded 469 examples from gpt-4o-mini_English_non_knowledge_batch_0004.json\n",
            "✅ Loaded 1667 examples from gpt-4o-mini_English_non_knowledge_dataset_consolidated.json\n",
            "📦 Saved FINAL non_knowledge dataset to /content/final_consolidated/gpt-4o-mini_English_non_knowledge_dataset_final_consolidated.json (3545 total examples)\n",
            "✅ Loaded 205 examples from gpt-4o-mini_English_else_batch_0001.json\n",
            "✅ Loaded 217 examples from gpt-4o-mini_English_else_batch_0002.json\n",
            "✅ Loaded 203 examples from gpt-4o-mini_English_else_batch_0003.json\n",
            "✅ Loaded 198 examples from gpt-4o-mini_English_else_batch_0004.json\n",
            "✅ Loaded 653 examples from gpt-4o-mini_English_else_dataset_consolidated.json\n",
            "📦 Saved FINAL else dataset to /content/final_consolidated/gpt-4o-mini_English_else_dataset_final_consolidated.json (1476 total examples)\n",
            "\n",
            "🎯 Final consolidation complete!\n",
            "\n",
            "🧩 Combined all categories into /content/final_consolidated/gpt-4o-mini_English_all_categories_final_consolidated.json (15000 total examples)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiQlOiF8xaxkDv++GDMLyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}