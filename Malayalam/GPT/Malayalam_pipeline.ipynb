{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAMB6JOwzg/3mYQsHwim8I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanya-Zac/Multilingual-LLM-hallucination-test/blob/main/Malayalam_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01R17mjI21iI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur8_hOc-guBn",
        "outputId": "3ceaca47-b742-4599-e17f-bcf223ab6f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure created!\n"
          ]
        }
      ],
      "source": [
        "# Create the directory structure\n",
        "import os\n",
        "\n",
        "dirs = [\n",
        "    'Trust_me_Im_wrong',\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty',\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty/uncertainty',\n",
        "\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models',\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty/uncertainty/uncertainty_measures',\n",
        "\n",
        "]\n",
        "\n",
        "for dir_path in dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLm2KvyL253D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX5tp3s0ayI1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "dirs = [\n",
        "    'Trust_me_Im_wrong/analysis',\n",
        "    'Trust_me_Im_wrong//visualization',\n",
        "     'Trust_me_Im_wrong/reporting',\n",
        "     'Trust_me_Im_wrong/reports',\n",
        "     'Trust_me_Im_wrong/visualizations'\n",
        "]\n",
        "for dir_path in dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CaJ30v0H3CjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kff_s7yeNwNT",
        "outputId": "01bc5b20-f494-40ae-fc03-557d296ec83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created __init__.py in Trust_me_Im_wrong/analysis\n",
            "Created __init__.py in Trust_me_Im_wrong//visualization\n",
            "Created __init__.py in Trust_me_Im_wrong/reporting\n",
            "Created __init__.py in Trust_me_Im_wrong/reports\n",
            "Created __init__.py in Trust_me_Im_wrong/visualizations\n"
          ]
        }
      ],
      "source": [
        "# Create __init__.py in each directory to mark as package\n",
        "for dir_path in dirs:\n",
        "    init_path = os.path.join(dir_path, '__init__.py')\n",
        "    if not os.path.exists(init_path):\n",
        "        with open(init_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('# Init file for package\\n')\n",
        "        print(f'Created __init__.py in {dir_path}')\n",
        "    else:\n",
        "        print(f'__init__.py already exists in {dir_path}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models/base_model.py\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Text, Optional, Tuple\n",
        "\n",
        "# Full stop sequences for post-processing (all languages)\n",
        "FULL_STOP_SEQUENCES = [\n",
        "    '\\n',\n",
        "]\n",
        "\n",
        "# API-compatible stop sequences (max 4 for OpenAI API)\n",
        "API_STOP_SEQUENCES = [\n",
        "    '\\n',      # Double newline - most common separator\n",
        "    '.',       # Period - ends sentences\n",
        "]\n",
        "\n",
        "# Keep original for backward compatibility\n",
        "STOP_SEQUENCES = FULL_STOP_SEQUENCES\n",
        "\n",
        "class BaseModel(ABC):\n",
        "    \"\"\"Base model class with enhanced stop sequence handling.\"\"\"\n",
        "\n",
        "    # Class variables\n",
        "    stop_sequences: List[Text] = FULL_STOP_SEQUENCES\n",
        "    api_stop_sequences: List[Text] = API_STOP_SEQUENCES\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize with both API and full stop sequences.\"\"\"\n",
        "        self.stop_sequences = FULL_STOP_SEQUENCES\n",
        "        self.api_stop_sequences = API_STOP_SEQUENCES\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, input_data: str, temperature: float):\n",
        "        \"\"\"\n",
        "        Generate a response from the model given input_data and temperature.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_p_true(self, input_data: str):\n",
        "        \"\"\"\n",
        "        Compute probability that the answer to input_data is 'True'.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def post_process_with_stops(\n",
        "        text: str,\n",
        "        stop_sequences: Optional[List[str]] = None,\n",
        "        preserve_stop: bool = False\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Post-process text by truncating at the first occurrence of any stop sequence.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to process\n",
        "            stop_sequences: List of stop sequences (uses FULL_STOP_SEQUENCES if None)\n",
        "            preserve_stop: If True, include the stop sequence in output\n",
        "\n",
        "        Returns:\n",
        "            Truncated text\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        if stop_sequences is None:\n",
        "            stop_sequences = FULL_STOP_SEQUENCES\n",
        "\n",
        "        # Find the earliest occurrence of any stop sequence\n",
        "        earliest_pos = len(text)\n",
        "        earliest_stop = None\n",
        "\n",
        "        for stop in stop_sequences:\n",
        "            pos = text.find(stop)\n",
        "            if pos != -1 and pos < earliest_pos:\n",
        "                earliest_pos = pos\n",
        "                earliest_stop = stop\n",
        "\n",
        "        # Truncate at the earliest stop sequence\n",
        "        if earliest_pos < len(text):\n",
        "            if preserve_stop and earliest_stop:\n",
        "                return text[:earliest_pos + len(earliest_stop)]\n",
        "            else:\n",
        "                return text[:earliest_pos]\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_for_comparison(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Aggressively clean text for accurate comparison and hallucination detection.\n",
        "        Removes special characters that shouldn't be considered as tokens.\n",
        "        Modified for Malayalam language processing.\n",
        "\n",
        "        Args:\n",
        "            text: Text to clean\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase (works with Malayalam characters too)\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove common Malayalam stop words instead of Turkish ones\n",
        "        # Most common Malayalam stop words based on linguistic research\n",
        "        remove_words = [\n",
        "            'ഉം',       # and/also\n",
        "            'ആണ്',      # is\n",
        "            'ഇല്ല',     # no/not\n",
        "            'എന്ന്',    # that/which\n",
        "            'ഒരു',      # one/a\n",
        "            'ആയി',      # became/as\n",
        "            'കൂടി',     # also/too\n",
        "            'വേണ്ടി',   # for\n",
        "            'ഉണ്ട്',    # have/has\n",
        "            'ആ',        # that (demonstrative)\n",
        "            'ഈ',        # this\n",
        "            'അത്',      # it/that\n",
        "            'ഇത്',      # this\n",
        "            'പോലെ',     # like\n",
        "            'കൊണ്ട്',   # with/by\n",
        "            'നിന്ന്',   # from\n",
        "            'മുതൽ',     # from/since\n",
        "            'വരെ',      # until\n",
        "            'മാത്രം',   # only\n",
        "            'തന്നെ'     # itself/himself\n",
        "        ]\n",
        "\n",
        "        for word in remove_words:\n",
        "            # Remove word with space boundaries to avoid partial matches\n",
        "            text = text.replace(f\" {word} \", \" \")\n",
        "            # Also check at the beginning and end of text\n",
        "            if text.startswith(word + \" \"):\n",
        "                text = text[len(word)+1:]\n",
        "            if text.endswith(\" \" + word):\n",
        "                text = text[:-len(word)-1]\n",
        "\n",
        "        # Remove all punctuation and special characters\n",
        "        # Standard punctuation works for Malayalam text\n",
        "        special_chars = [\n",
        "            '!', '?', ';', ',', '.', ':', '\"', \"'\", \"-\", \"_\",\n",
        "            '(', ')', '[', ']', '{', '}', '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*'\n",
        "        ]\n",
        "        for char in special_chars:\n",
        "            text = text.replace(char, ' ')\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_api_compatible_stops() -> List[str]:\n",
        "        \"\"\"\n",
        "        Get API-compatible stop sequences (max 4).\n",
        "\n",
        "        Returns:\n",
        "            List of up to 4 stop sequences for API calls\n",
        "        \"\"\"\n",
        "        return API_STOP_SEQUENCES[:4]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_all_stops() -> List[str]:\n",
        "        \"\"\"\n",
        "        Get all stop sequences for post-processing.\n",
        "\n",
        "        Returns:\n",
        "            Complete list of stop sequences\n",
        "        \"\"\"\n",
        "        return FULL_STOP_SEQUENCES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5DTgiiv4lNm",
        "outputId": "e1351e65-c387-4b96-947b-a979dd82a826"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models/base_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\n",
        "\"\"\"\n",
        "Semantic Entropy Calculation for API Models (GPT-4o-mini)\n",
        "Uses OpenAI API with logprobs and sentence transformers for clustering\n",
        "Adapted for Malayalam datasets\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from google.colab import userdata\n",
        "import sys\n",
        "sys.path.append('/content/Trust_me_Im_wrong')\n",
        "\n",
        "# Import the Malayalam-adapted base model\n",
        "from semantic_uncertainty.uncertainty.models.base_model import (\n",
        "    API_STOP_SEQUENCES, FULL_STOP_SEQUENCES, BaseModel\n",
        ")\n",
        "\n",
        "\n",
        "class SemanticEntropyAPI:\n",
        "    def __init__(self, model_name=\"gpt-4o-mini\", dataset_path=\"datasets/\",\n",
        "                 entailment_model=\"sentence_transformer\", max_new_tokens=10):\n",
        "        \"\"\"\n",
        "        Initialize semantic entropy calculator for API models.\n",
        "        Configured for Malayalam language processing.\n",
        "        \"\"\"\n",
        "        random.seed(0)\n",
        "        self.model_name = model_name\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "        # Initialize API client\n",
        "        try:\n",
        "            api_key = userdata.get('gptapi')\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to get API key: {e}\")\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        try:\n",
        "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "        except KeyError:\n",
        "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "        # Initialize sentence transformer for semantic similarity\n",
        "        # Using multilingual model for better Malayalam support\n",
        "        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        # Clustering threshold (may need adjustment for Malayalam)\n",
        "        # Malayalam may have more semantic variation, so slightly higher threshold\n",
        "        self.clustering_threshold = 0.5\n",
        "\n",
        "        # Setup stop sequences\n",
        "        self.api_stops = API_STOP_SEQUENCES[:4]\n",
        "        self.full_stops = FULL_STOP_SEQUENCES\n",
        "\n",
        "        print(f\"Initialized SemanticEntropyAPI with {model_name}\")\n",
        "        print(f\"Clustering threshold: {self.clustering_threshold}\")\n",
        "        print(f\"Using multilingual embedding model for Malayalam support\")\n",
        "\n",
        "    def generate_answers(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        answer: str,\n",
        "        num_generations: int = 11,\n",
        "        temperature: float = 1.0,\n",
        "        compute_acc: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Generate multiple answers from API model for entropy calculation.\n",
        "        Handles Malayalam question-answer format.\n",
        "        \"\"\"\n",
        "        generations = {}\n",
        "        generations[prompt] = {\"question\": prompt}\n",
        "        full_responses = []\n",
        "        all_generation_texts = []  # Store all generated texts for output\n",
        "\n",
        "        # System message in English as requested\n",
        "        # This helps the model understand it should give concise answers\n",
        "        system_message = \"Provide direct, brief answers, not sentences.\"\n",
        "\n",
        "        print(f\"Generating {num_generations} responses...\")\n",
        "\n",
        "        for i in range(num_generations):\n",
        "            # First generation at low temperature\n",
        "            temp = 0.1 if i == 0 else temperature\n",
        "\n",
        "            # Get response with logprobs\n",
        "            response_data = self._get_api_response(prompt, temp, system_message)\n",
        "\n",
        "            # Store the text for output\n",
        "            all_generation_texts.append(response_data[\"text\"])\n",
        "\n",
        "            if i == 0:\n",
        "                # Store most likely answer\n",
        "                most_likely_answer_dict = {\n",
        "                    \"response\": response_data[\"text\"],\n",
        "                    \"token_log_likelihoods\": response_data[\"token_logprobs\"],\n",
        "                    \"embedding\": None,\n",
        "                    \"accuracy\": self._check_accuracy(response_data[\"text\"], answer) if compute_acc else 0.0,\n",
        "                    \"total_logprob\": response_data[\"total_logprob\"]\n",
        "                }\n",
        "                generations[prompt][\"most_likely_answer\"] = most_likely_answer_dict\n",
        "            else:\n",
        "                # Store high temperature responses\n",
        "                full_responses.append((\n",
        "                    response_data[\"text\"],\n",
        "                    response_data[\"token_logprobs\"],\n",
        "                    None,\n",
        "                    self._check_accuracy(response_data[\"text\"], answer) if compute_acc else 0.0,\n",
        "                    response_data[\"total_logprob\"]  # Add total logprob\n",
        "                ))\n",
        "\n",
        "        generations[prompt][\"responses\"] = full_responses\n",
        "        generations[prompt][\"reference\"] = answer\n",
        "        generations[prompt][\"all_generation_texts\"] = all_generation_texts  # Add all texts\n",
        "\n",
        "        print(f\"Generated texts: {all_generation_texts[:3]}...\")  # Show first 3\n",
        "\n",
        "        return {\n",
        "            \"accuracies\": [most_likely_answer_dict[\"accuracy\"]] if generations[prompt].get(\"most_likely_answer\") else [],\n",
        "            \"generations\": generations,\n",
        "            \"question\": prompt,\n",
        "            \"reference\": answer,\n",
        "            \"all_generation_texts\": all_generation_texts\n",
        "        }\n",
        "\n",
        "    def _get_api_response(self, prompt: str, temperature: float, system_message: str = None):\n",
        "        \"\"\"Get response from OpenAI API with logprobs.\"\"\"\n",
        "        try:\n",
        "            messages = []\n",
        "            if system_message:\n",
        "                messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=self.max_new_tokens,\n",
        "                logprobs=True,\n",
        "                top_logprobs=20,\n",
        "                stop=self.api_stops\n",
        "            )\n",
        "\n",
        "            choice = response.choices[0]\n",
        "            raw_text = choice.message.content or \"\"\n",
        "\n",
        "            # Post-process with full stop sequences\n",
        "            processed_text = BaseModel.post_process_with_stops(\n",
        "                raw_text,\n",
        "                self.full_stops,\n",
        "                preserve_stop=False\n",
        "            )\n",
        "\n",
        "            # Extract logprobs\n",
        "            total_logprob = 0\n",
        "            token_logprobs = []\n",
        "\n",
        "            if hasattr(choice, 'logprobs') and choice.logprobs and choice.logprobs.content:\n",
        "                for logprob_data in choice.logprobs.content:\n",
        "                    token_logprobs.append(logprob_data.logprob)\n",
        "                    total_logprob += logprob_data.logprob\n",
        "\n",
        "            return {\n",
        "                \"text\": processed_text,\n",
        "                \"raw_text\": raw_text,\n",
        "                \"token_logprobs\": token_logprobs,\n",
        "                \"total_logprob\": total_logprob  # Sum of all token logprobs\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"API error: {e}\")\n",
        "            return {\n",
        "                \"text\": \"\",\n",
        "                \"raw_text\": \"\",\n",
        "                \"token_logprobs\": [],\n",
        "                \"total_logprob\": 0\n",
        "            }\n",
        "\n",
        "    def _check_accuracy(self, generated: str, target: str) -> float:\n",
        "        \"\"\"\n",
        "        Check if generated text matches target.\n",
        "        Uses BaseModel.clean_for_comparison which now handles Malayalam text\n",
        "        with Malayalam stop words removal.\n",
        "        \"\"\"\n",
        "        gen_clean = BaseModel.clean_for_comparison(generated)\n",
        "        target_clean = BaseModel.clean_for_comparison(target)\n",
        "\n",
        "        if not gen_clean or not target_clean:\n",
        "            return 0.0\n",
        "\n",
        "        return 1.0 if (target_clean in gen_clean or gen_clean in target_clean) else 0.0\n",
        "\n",
        "    def compute_uncertainty_measures(\n",
        "        self,\n",
        "        model_generations,\n",
        "        compute_predictive_entropy=True,\n",
        "        strict_entailment=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Compute semantic entropy and other uncertainty measures using clustering.\n",
        "        Optimized for Malayalam text processing.\n",
        "        \"\"\"\n",
        "        result_dict = {\"semantic_ids\": [], \"debug_info\": {}}\n",
        "        entropies = defaultdict(list)\n",
        "\n",
        "        for tid in model_generations:\n",
        "            example = model_generations[tid]\n",
        "            full_responses = example.get(\"responses\", [])\n",
        "\n",
        "            # Include the most likely answer in calculations\n",
        "            if \"most_likely_answer\" in example:\n",
        "                most_likely = example[\"most_likely_answer\"]\n",
        "                full_responses.insert(0, (\n",
        "                    most_likely[\"response\"],\n",
        "                    most_likely[\"token_log_likelihoods\"],\n",
        "                    None,\n",
        "                    most_likely.get(\"accuracy\", 0),\n",
        "                    most_likely.get(\"total_logprob\", sum(most_likely[\"token_log_likelihoods\"]))\n",
        "                ))\n",
        "\n",
        "            if not full_responses:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {len(full_responses)} total generations\")\n",
        "\n",
        "            # Clean responses before clustering\n",
        "            raw_responses = [r[0] for r in full_responses]\n",
        "            responses = [BaseModel.post_process_with_stops(r, self.full_stops) for r in raw_responses]\n",
        "\n",
        "            if compute_predictive_entropy:\n",
        "                # Get TOTAL log likelihoods (not per token average)\n",
        "                total_log_liks = []\n",
        "                for r in full_responses:\n",
        "                    if len(r) >= 5:  # Has total_logprob\n",
        "                        total_log_liks.append(r[4])\n",
        "                    elif r[1]:  # Fallback to sum of token logprobs\n",
        "                        total_log_liks.append(sum(r[1]))\n",
        "                    else:\n",
        "                        total_log_liks.append(-10)  # Default low value\n",
        "\n",
        "                print(f\"Total log likelihoods: {total_log_liks[:3]}...\")\n",
        "\n",
        "                # Compute semantic IDs using clustering\n",
        "                semantic_ids = self._get_semantic_ids_clustering(responses)\n",
        "                result_dict[\"semantic_ids\"].append(semantic_ids)\n",
        "\n",
        "                print(f\"Semantic IDs: {semantic_ids}\")\n",
        "                unique_clusters = len(set(sid for sid in semantic_ids if sid >= 0))\n",
        "                print(f\"Number of semantic clusters: {unique_clusters}\")\n",
        "\n",
        "                # Store debug info\n",
        "                result_dict[\"debug_info\"][\"num_generations\"] = len(full_responses)\n",
        "                result_dict[\"debug_info\"][\"num_clusters\"] = unique_clusters\n",
        "                result_dict[\"debug_info\"][\"responses_sample\"] = responses[:3]\n",
        "\n",
        "                # Compute cluster assignment entropy\n",
        "                cluster_entropy = self._cluster_assignment_entropy(semantic_ids)\n",
        "                entropies[\"cluster_assignment_entropy\"].append(cluster_entropy)\n",
        "                print(f\"Cluster assignment entropy: {cluster_entropy:.4f}\")\n",
        "\n",
        "                # Regular entropy - using total log likelihoods\n",
        "                regular_entropy = self._predictive_entropy_corrected(total_log_liks)\n",
        "                entropies[\"regular_entropy\"].append(regular_entropy)\n",
        "                print(f\"Regular entropy: {regular_entropy:.4f}\")\n",
        "\n",
        "                # Semantic entropy - aggregate by cluster then compute entropy\n",
        "                log_likelihood_per_semantic_id = self._logsumexp_by_id(\n",
        "                    semantic_ids, total_log_liks\n",
        "                )\n",
        "                semantic_entropy = self._semantic_entropy_corrected(log_likelihood_per_semantic_id)\n",
        "                entropies[\"semantic_entropy\"].append(semantic_entropy)\n",
        "                print(f\"Semantic entropy: {semantic_entropy:.4f}\")\n",
        "\n",
        "        # Compute averages\n",
        "        avg_entropies = {k: np.mean(v) if v else 0 for k, v in entropies.items()}\n",
        "\n",
        "        return avg_entropies, result_dict\n",
        "\n",
        "    def _get_semantic_ids_clustering(self, responses: list) -> list:\n",
        "        \"\"\"Cluster responses using sentence embeddings for Malayalam text.\"\"\"\n",
        "        if not responses or len(responses) <= 1:\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        valid_responses = [r for r in responses if r and r.strip()]\n",
        "        if not valid_responses:\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        # Show what we're clustering\n",
        "        print(f\"Clustering {len(valid_responses)} responses\")\n",
        "        print(f\"Sample responses: {valid_responses[:2]}\")\n",
        "\n",
        "        try:\n",
        "            embeddings = self.embedding_model.encode(valid_responses)\n",
        "        except Exception as e:\n",
        "            print(f\"Embedding error: {e}\")\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        if len(valid_responses) == 1:\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        clustering = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            distance_threshold=self.clustering_threshold,\n",
        "            linkage='average'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            semantic_ids = clustering.fit_predict(embeddings)\n",
        "            full_ids = []\n",
        "            valid_idx = 0\n",
        "            for r in responses:\n",
        "                if r and r.strip():\n",
        "                    full_ids.append(semantic_ids[valid_idx])\n",
        "                    valid_idx += 1\n",
        "                else:\n",
        "                    full_ids.append(-1)\n",
        "            return full_ids\n",
        "        except Exception as e:\n",
        "            print(f\"Clustering error: {e}\")\n",
        "            return [0] * len(responses)\n",
        "\n",
        "    def _cluster_assignment_entropy(self, semantic_ids: list) -> float:\n",
        "        \"\"\"Calculate entropy of cluster assignments.\"\"\"\n",
        "        if not semantic_ids:\n",
        "            return 0.0\n",
        "\n",
        "        valid_ids = [sid for sid in semantic_ids if sid >= 0]\n",
        "        if not valid_ids:\n",
        "            return 0.0\n",
        "\n",
        "        unique, counts = np.unique(valid_ids, return_counts=True)\n",
        "        probs = counts / len(valid_ids)\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "        return float(entropy)\n",
        "\n",
        "    def _predictive_entropy_corrected(self, total_log_liks: list) -> float:\n",
        "        \"\"\"\n",
        "        CORRECTED: Calculate predictive entropy from log likelihoods.\n",
        "        This is the entropy over the model's output distribution.\n",
        "        H = -E[log p(y|x)] ≈ -mean(log_likelihood)\n",
        "        \"\"\"\n",
        "        if not total_log_liks:\n",
        "            return 0.0\n",
        "\n",
        "        # Average negative log likelihood\n",
        "        avg_neg_log_lik = -np.mean(total_log_liks)\n",
        "        return float(avg_neg_log_lik)\n",
        "\n",
        "    def _semantic_entropy_corrected(self, log_likelihood_per_semantic_id: list) -> float:\n",
        "        \"\"\"\n",
        "        CORRECTED: Calculate semantic entropy from aggregated log likelihoods.\n",
        "        This is entropy over semantic equivalence classes.\n",
        "        \"\"\"\n",
        "        if not log_likelihood_per_semantic_id:\n",
        "            return 0.0\n",
        "\n",
        "        # Convert log likelihoods to probabilities\n",
        "        max_log_lik = max(log_likelihood_per_semantic_id)\n",
        "        log_probs_normalized = [ll - max_log_lik for ll in log_likelihood_per_semantic_id]\n",
        "        probs = np.exp(log_probs_normalized)\n",
        "        probs = probs / (np.sum(probs) + 1e-10)\n",
        "\n",
        "        # Calculate entropy\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "        return float(entropy)\n",
        "\n",
        "    def _logsumexp_by_id(self, semantic_ids: list, total_log_liks: list) -> list:\n",
        "        \"\"\"\n",
        "        Aggregate log probabilities by semantic ID using logsumexp.\n",
        "        This properly combines probabilities of semantically equivalent responses.\n",
        "        \"\"\"\n",
        "        if not semantic_ids or not total_log_liks:\n",
        "            return []\n",
        "\n",
        "        valid_pairs = [(sid, ll) for sid, ll in zip(semantic_ids, total_log_liks)\n",
        "                      if sid >= 0 and ll is not None]\n",
        "        if not valid_pairs:\n",
        "            return []\n",
        "\n",
        "        # Group by semantic ID\n",
        "        id_to_log_liks = defaultdict(list)\n",
        "        for sid, ll in valid_pairs:\n",
        "            id_to_log_liks[sid].append(ll)\n",
        "\n",
        "        # Aggregate using logsumexp for each semantic ID\n",
        "        log_likelihood_per_id = []\n",
        "        for sid in sorted(id_to_log_liks.keys()):\n",
        "            lls = id_to_log_liks[sid]\n",
        "            if lls:\n",
        "                # Logsumexp: log(sum(exp(x_i)))\n",
        "                max_ll = max(lls)\n",
        "                sum_exp = sum(np.exp(ll - max_ll) for ll in lls)\n",
        "                aggregated_ll = max_ll + np.log(sum_exp)\n",
        "                log_likelihood_per_id.append(aggregated_ll)\n",
        "\n",
        "        return log_likelihood_per_id\n",
        "\n",
        "    def calc_semantic_entropy_per_example(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        answer: str,\n",
        "        temp: float = 1.0,\n",
        "        num_generations: int = 11\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Calculate semantic entropy for a single Malayalam example.\n",
        "\n",
        "        Args:\n",
        "            prompt: Malayalam question (ചോദ്യം)\n",
        "            answer: Malayalam answer (ഉത്തരം)\n",
        "            temp: Temperature for generation\n",
        "            num_generations: Number of responses to generate\n",
        "        \"\"\"\n",
        "        print(f\"\\nCalculating semantic entropy with {num_generations} generations (temp={temp:.1f})...\")\n",
        "        print(f\"Processing Malayalam question-answer pair\")\n",
        "\n",
        "        results = self.generate_answers(\n",
        "            prompt=prompt,\n",
        "            answer=answer,\n",
        "            num_generations=num_generations,\n",
        "            temperature=temp,\n",
        "            compute_acc=True\n",
        "        )\n",
        "\n",
        "        avg_entropies, extra_info = self.compute_uncertainty_measures(\n",
        "            results[\"generations\"],\n",
        "            compute_predictive_entropy=True,\n",
        "            strict_entailment=False\n",
        "        )\n",
        "\n",
        "        # Add all generation texts to the output\n",
        "        avg_entropies[\"all_generations\"] = results.get(\"all_generation_texts\", [])\n",
        "        avg_entropies[\"debug_info\"] = extra_info.get(\"debug_info\", {})\n",
        "\n",
        "        return avg_entropies, results[\"generations\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRnHwd1d4sx5",
        "outputId": "fde1e31a-1e3d-4395-b726-3452d6bd44b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Trust_me_Im_wrong/uncertainty_calculation_api.py\n",
        "\"\"\"\n",
        "Uncertainty Calculation for API Models - Malayalam Adaptation\n",
        "Advanced Malayalam text matching with multi-tier classification\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "import sys\n",
        "sys.path.append('/content/Trust_me_Im_wrong')\n",
        "\n",
        "from calc_semantic_entropy_api import SemanticEntropyAPI\n",
        "from semantic_uncertainty.uncertainty.models.base_model import (\n",
        "    API_STOP_SEQUENCES, FULL_STOP_SEQUENCES, BaseModel\n",
        ")\n",
        "\n",
        "\n",
        "class MalayalamTextMatcher:\n",
        "    \"\"\"Advanced Malayalam text matching with multi-tier scoring.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Malayalam stop words (based on linguistic research)\n",
        "        self.malayalam_stops = {\n",
        "            'ഉം', 'ആണ്', 'ഇല്ല', 'എന്ന്', 'ഒരു', 'ആയി', 'കൂടി', 'വേണ്ടി',\n",
        "            'ഉണ്ട്', 'ആ', 'ഈ', 'അത്', 'ഇത്', 'പോലെ', 'കൊണ്ട്', 'നിന്ന്',\n",
        "            'മുതൽ', 'വരെ', 'മാത്രം', 'തന്നെ', 'പിന്നെ', 'എന്നാൽ', 'പക്ഷേ',\n",
        "            'അല്ല', 'ഓ', 'നു', 'ക്ക്', 'യുടെ', 'യിൽ', 'ത്തെ'\n",
        "        }\n",
        "\n",
        "        # English stop words (for mixed content)\n",
        "        self.english_stops = {\n",
        "            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "            'of', 'in', 'at', 'on', 'to', 'for', 'with', 'by', 'from'\n",
        "        }\n",
        "\n",
        "        # Common Malayalam abbreviations\n",
        "        self.abbreviations = {\n",
        "            'ഡോ.': 'ഡോക്ടർ',\n",
        "            'പ്രൊഫ.': 'പ്രൊഫസർ',\n",
        "            'കി.മീ.': 'കിലോമീറ്റർ',\n",
        "            'മീ.': 'മീറ്റർ',\n",
        "            'രൂ.': 'രൂപ',\n",
        "            'എം.എൽ.എ.': 'എംഎൽഎ',\n",
        "            'എം.പി.': 'എംപി'\n",
        "        }\n",
        "\n",
        "    def malayalam_lower(self, text):\n",
        "        \"\"\"Proper Malayalam lowercase conversion.\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        # Malayalam doesn't have uppercase/lowercase distinction like English\n",
        "        # But we still need to handle mixed English-Malayalam text\n",
        "        return text.lower()\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        \"\"\"Advanced text normalization for Malayalam.\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase (handles English parts)\n",
        "        text = self.malayalam_lower(text)\n",
        "\n",
        "        # Expand abbreviations\n",
        "        for abbr, full in self.abbreviations.items():\n",
        "            text = text.replace(abbr, full)\n",
        "\n",
        "        # Remove punctuation but keep spaces\n",
        "        # Malayalam uses standard punctuation marks\n",
        "        text = re.sub(r'[.,;:!?\\'\"()\\[\\]{}\"\"]', ' ', text)\n",
        "        text = re.sub(r'[-–—]', ' ', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def get_meaningful_tokens(self, text):\n",
        "        \"\"\"Extract meaningful tokens, filtering stop words.\"\"\"\n",
        "        text = self.normalize_text(text)\n",
        "        tokens = text.split()\n",
        "\n",
        "        # Filter stop words (both Malayalam and English)\n",
        "        all_stops = self.malayalam_stops | self.english_stops\n",
        "        meaningful = [t for t in tokens if t not in all_stops and len(t) > 1]\n",
        "\n",
        "        # If no meaningful tokens, return all non-empty tokens\n",
        "        if not meaningful:\n",
        "            meaningful = [t for t in tokens if t]\n",
        "\n",
        "        return meaningful\n",
        "\n",
        "    def calculate_match_score(self, generated, target):\n",
        "        \"\"\"\n",
        "        Multi-tier matching strategy for Malayalam text.\n",
        "        Returns (score, match_type)\n",
        "        \"\"\"\n",
        "        if not generated or not target:\n",
        "            return 0.0, \"no_match\"\n",
        "\n",
        "        # Normalize both texts\n",
        "        gen_norm = self.normalize_text(generated)\n",
        "        target_norm = self.normalize_text(target)\n",
        "\n",
        "        # 1. Exact match after normalization\n",
        "        if gen_norm == target_norm:\n",
        "            return 1.0, \"exact\"\n",
        "\n",
        "        # 2. Contains check (for answers within longer text)\n",
        "        if target_norm in gen_norm:\n",
        "            return 0.95, \"contains\"\n",
        "\n",
        "        # 3. Token overlap check\n",
        "        gen_tokens = set(self.get_meaningful_tokens(generated))\n",
        "        target_tokens = set(self.get_meaningful_tokens(target))\n",
        "\n",
        "        if target_tokens and gen_tokens:\n",
        "            # Check if all target tokens appear in generation\n",
        "            if target_tokens.issubset(gen_tokens):\n",
        "                return 0.9, \"token_subset\"\n",
        "\n",
        "            # Calculate overlap ratio\n",
        "            overlap = len(target_tokens & gen_tokens)\n",
        "            target_size = len(target_tokens)\n",
        "\n",
        "            if target_size > 0:\n",
        "                overlap_ratio = overlap / target_size\n",
        "\n",
        "                if overlap_ratio >= 0.8:\n",
        "                    return 0.85, \"high_token_overlap\"\n",
        "                elif overlap_ratio >= 0.6:\n",
        "                    return 0.7, \"medium_token_overlap\"\n",
        "                elif overlap_ratio >= 0.4:\n",
        "                    return 0.5, \"low_token_overlap\"\n",
        "\n",
        "        # 4. Fuzzy matching for typos/variations\n",
        "        similarity = SequenceMatcher(None, gen_norm, target_norm).ratio()\n",
        "\n",
        "        if similarity >= 0.8:\n",
        "            return similarity * 0.9, \"fuzzy_high\"\n",
        "        elif similarity >= 0.6:\n",
        "            return similarity * 0.7, \"fuzzy_medium\"\n",
        "        elif similarity >= 0.4:\n",
        "            return similarity * 0.5, \"fuzzy_low\"\n",
        "\n",
        "        # 5. Partial credit for compound answers\n",
        "        # Check if any significant part matches\n",
        "        if len(target_tokens) > 1:\n",
        "            for token in target_tokens:\n",
        "                if len(token) >= 4 and token in gen_norm:\n",
        "                    return 0.4, \"partial_match\"\n",
        "\n",
        "        # 6. Check for common variations (e.g., different spellings)\n",
        "        if self._check_common_variations(gen_norm, target_norm):\n",
        "            return 0.6, \"variation_match\"\n",
        "\n",
        "        return 0.0, \"no_match\"\n",
        "\n",
        "    def _check_common_variations(self, gen, target):\n",
        "        \"\"\"Check for common spelling variations in Malayalam/English names.\"\"\"\n",
        "        variations = [\n",
        "            ('സമുദ്രം', 'കടൽ'),  # ocean/sea\n",
        "            ('നദി', 'പുഴ',),\n",
        "            ('മീഥെയ്ൻ','മീഥെയിൻ','മീത്തെയ്ൻ','മീത്തെയിൻ','മീത്തേൻ','മീഥേൻ','മേതേൻ'),# river variations\n",
        "            ('പർവതം', 'മല'),  # mountain variations\n",
        "            ('തടാകം', 'കായൽ'),  # lake variations\n",
        "            ('രാജ്യം', 'ദേശം'),  # country/nation\n",
        "        ]\n",
        "\n",
        "        for v1, v2 in variations:\n",
        "            if (v1 in gen and v2 in target) or (v2 in gen and v1 in target):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "class UncertaintyCalculationAPI:\n",
        "    def __init__(self, model_name=\"gpt-4o-mini\", dataset_path=\"/content/\",\n",
        "                 method_k_positive=\"bad_shots\", dataset_name=\"mal_500\"):\n",
        "        \"\"\"Initialize uncertainty calculation for Malayalam datasets.\"\"\"\n",
        "        random.seed(0)\n",
        "\n",
        "        # API Setup\n",
        "        try:\n",
        "            api_key = userdata.get('gptapi')\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: API key setup issue: {e}\")\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.dataset_name = dataset_name\n",
        "        self.method_k_positive = method_k_positive\n",
        "\n",
        "        # Initialize Malayalam text matcher\n",
        "        self.matcher = MalayalamTextMatcher()\n",
        "\n",
        "        # Setup stop sequences\n",
        "        self.api_stops = API_STOP_SEQUENCES[:4]\n",
        "        self.full_stops = FULL_STOP_SEQUENCES\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        try:\n",
        "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "        except KeyError:\n",
        "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "        # Load datasets\n",
        "        self.data_path_know = self._load_dataset(\n",
        "            os.path.join(dataset_path, \"knowledge.json\")\n",
        "        )\n",
        "        self.data_path_do_not_know = self._load_dataset(\n",
        "            os.path.join(dataset_path, \"nonknowledge.json\")\n",
        "        )\n",
        "\n",
        "        # Initialize semantic entropy calculator\n",
        "        self.semantic_entropy = SemanticEntropyAPI(\n",
        "            model_name=model_name,\n",
        "            dataset_path=dataset_path,\n",
        "            max_new_tokens=10\n",
        "        )\n",
        "\n",
        "        # Setup results directory\n",
        "        os.makedirs(f\"results/{model_name}/{dataset_name}/{method_k_positive}/\", exist_ok=True)\n",
        "        self.path_results = f\"results/{model_name}/{dataset_name}/{method_k_positive}/\"\n",
        "\n",
        "        # Malayalam few-shot examples\n",
        "        self.list_good_shot = [\n",
        "            \"ചോദ്യം: ഫ്രാൻസിന്റെ തലസ്ഥാനം എന്താണ്?\\nഉത്തരം: പാരീസ്\\n\",\n",
        "            \"ചോദ്യം: എത്ര ഭൂഖണ്ഡങ്ങളുണ്ട്?\\nഉത്തരം: ഏഴ്\\n\",\n",
        "            \"ചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\n\",\n",
        "            \"ചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\n\",\n",
        "            \"ചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\n\",\n",
        "        ]\n",
        "\n",
        "        print(f\"✓ Initialized Malayalam UncertaintyCalculationAPI\")\n",
        "        print(f\"  Model: {model_name}\")\n",
        "        print(f\"  Using advanced Malayalam text matching\")\n",
        "\n",
        "    def _load_dataset(self, data_path, sample_size=20000):\n",
        "        \"\"\"Load dataset from JSON file.\"\"\"\n",
        "        if not os.path.exists(data_path):\n",
        "            print(f\"Warning: Dataset not found at {data_path}\")\n",
        "            return []\n",
        "\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        processed_data = []\n",
        "        for item in data:\n",
        "            if isinstance(item, list):\n",
        "                if len(item) >= 2:\n",
        "                    question_text = item[0]\n",
        "                    # Handle Malayalam format\n",
        "                    if \"ഉത്തരം:\" in question_text:\n",
        "                        question_text = question_text.rsplit(\"ഉത്തരം:\", 1)[0].strip()\n",
        "                    processed_data.append({\n",
        "                        \"prompt\": question_text,\n",
        "                        \"target_answer\": item[1],\n",
        "                        \"ids\": item[2] if len(item) > 2 else [],\n",
        "                        \"score\": item[3] if len(item) > 3 else 0\n",
        "                    })\n",
        "            elif isinstance(item, dict):\n",
        "                processed_data.append(item)\n",
        "\n",
        "        if len(processed_data) > sample_size:\n",
        "            processed_data = random.sample(processed_data, sample_size)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def get_prompt(self, method, example):\n",
        "        \"\"\"Create Malayalam prompt with appropriate format.\"\"\"\n",
        "        idx = random.randint(0, len(self.list_good_shot) - 1)\n",
        "        single_shot = self.list_good_shot[idx]\n",
        "\n",
        "        if method == \"alice\":\n",
        "            # Malayalam context for Alice-Bob scenario\n",
        "            context = \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\n\"\n",
        "        elif method == \"child\":\n",
        "            # Malayalam context for child scenario\n",
        "            context = \"ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\n\"\n",
        "        else:\n",
        "            context = \"\"\n",
        "\n",
        "        if isinstance(example, dict):\n",
        "            question = example.get(\"prompt\", example.get(\"question\", \"\"))\n",
        "        elif isinstance(example, list) and len(example) > 0:\n",
        "            question = example[0]\n",
        "        else:\n",
        "            question = str(example)\n",
        "\n",
        "        question = question.strip()\n",
        "        if not question.startswith(\"ചോദ്യം:\"):\n",
        "            question = f\"ചോദ്യം: {question}\"\n",
        "\n",
        "        if not question.endswith(\"ഉത്തരം:\"):\n",
        "            if question.endswith(\"?\"):\n",
        "                question = question + \"\\nഉത്തരം:\"\n",
        "            else:\n",
        "                question = question + \"?\\nഉത്തരം:\"\n",
        "\n",
        "        return context + single_shot + question\n",
        "\n",
        "    def calculate_probabilities_uncertainty(self, data, with_knowledge=True):\n",
        "        \"\"\"Main processing pipeline with Malayalam text matching.\"\"\"\n",
        "\n",
        "        # Step 1: Generate all responses\n",
        "        print(\"Step 1: Generating responses...\")\n",
        "        all_responses = self._generate_all_responses(data)\n",
        "\n",
        "        # Step 2: Classify with advanced matching (2 categories only)\n",
        "        print(\"Step 2: Classifying with Malayalam text matching...\")\n",
        "        classifications = self._classify_responses_simple(all_responses)\n",
        "\n",
        "        # Separate by classification\n",
        "        factuality_responses = classifications['factuality']\n",
        "        hallucinated_responses = classifications['hallucination']\n",
        "\n",
        "        print(f\"  Found {len(factuality_responses)} factual, {len(hallucinated_responses)} hallucinations\")\n",
        "\n",
        "        # Step 3: Extract probabilities with all generations\n",
        "        print(\"Step 3: Extracting probabilities with semantic entropy...\")\n",
        "        factuality_stats = self._extract_probabilities_with_match_info(factuality_responses, \"FACTUALITY\")\n",
        "        hallucination_stats = self._extract_probabilities_with_match_info(hallucinated_responses, \"HALLUCINATION\")\n",
        "\n",
        "        # Save results\n",
        "        self._save_stats_with_analysis(hallucination_stats, factuality_stats, classifications['match_stats'])\n",
        "\n",
        "        # Print analysis\n",
        "        self._print_match_analysis(classifications['match_stats'])\n",
        "\n",
        "        return factuality_stats, hallucination_stats\n",
        "\n",
        "    def _generate_all_responses(self, data):\n",
        "        \"\"\"Generate responses with logprobs.\"\"\"\n",
        "        all_responses = []\n",
        "\n",
        "        for i, example in enumerate(tqdm(data, desc=\"Generating responses\")):\n",
        "            prompt = self.get_prompt(self.method_k_positive, example)\n",
        "            if not prompt:\n",
        "                continue\n",
        "\n",
        "            if isinstance(example, dict):\n",
        "                answer = example.get(\"target_answer\", example.get(\"answer\", \"\"))\n",
        "            elif isinstance(example, list) and len(example) > 1:\n",
        "                answer = example[1]\n",
        "            else:\n",
        "                answer = \"\"\n",
        "\n",
        "            try:\n",
        "                messages = [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You must provide ONLY the direct answer, not sentences. \"\n",
        "                            \"Use minimum words possible. Never explain. \"\n",
        "                             \"Malayalam is primarily spoken in Kerala, India\"\n",
        "                             \"Default to Kerala-specific cultural, geographical contexts\"\n",
        "                            \"Never say 'ഉത്തരം അതാണ്' or similar phrases. \"\n",
        "                            \"Examples: 'പാരീസ്', 'ടോം ഹാർഡി', 'പസിഫിക് ഓഷൻ'\"\n",
        "                        )\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=messages,\n",
        "                    temperature=0.01,\n",
        "                    max_tokens=10,\n",
        "                    logprobs=True,\n",
        "                    top_logprobs=20,\n",
        "                    stop=self.api_stops\n",
        "                )\n",
        "\n",
        "                full_response = response.choices[0].message.content.strip()\n",
        "                logprobs_data = response.choices[0].logprobs.content if response.choices[0].logprobs else []\n",
        "\n",
        "                all_responses.append({\n",
        "                    'prompt': prompt,\n",
        "                    'full_response': full_response,\n",
        "                    'logprobs': logprobs_data,\n",
        "                    'true_answer': answer,\n",
        "                    'example_id': i\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating response {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return all_responses\n",
        "\n",
        "    def _classify_responses_simple(self, all_responses):\n",
        "        \"\"\"Simple classification: factuality vs hallucination.\"\"\"\n",
        "        factuality = []\n",
        "        hallucination = []\n",
        "        match_stats = {\n",
        "            'exact': 0, 'contains': 0, 'token_subset': 0,\n",
        "            'high_token_overlap': 0, 'medium_token_overlap': 0,\n",
        "            'low_token_overlap': 0, 'fuzzy_high': 0, 'fuzzy_medium': 0,\n",
        "            'fuzzy_low': 0, 'partial_match': 0, 'variation_match': 0,\n",
        "            'no_match': 0\n",
        "        }\n",
        "\n",
        "        for resp in all_responses:\n",
        "            score, match_type = self.matcher.calculate_match_score(\n",
        "                resp['full_response'],\n",
        "                resp['true_answer']\n",
        "            )\n",
        "\n",
        "            # Store match info\n",
        "            resp['match_score'] = score\n",
        "            resp['match_type'] = match_type\n",
        "            match_stats[match_type] += 1\n",
        "\n",
        "            # Simple 2-way classification\n",
        "            if score >= 0.3:  # Factuality threshold (includes exact and partial matches)\n",
        "                factuality.append(resp)\n",
        "            else:\n",
        "                hallucination.append(resp)\n",
        "\n",
        "        return {\n",
        "            'factuality': factuality,\n",
        "            'hallucination': hallucination,\n",
        "            'match_stats': match_stats\n",
        "        }\n",
        "\n",
        "    def _extract_probabilities_with_match_info(self, responses, classification):\n",
        "        \"\"\"Extract probabilities with match information.\"\"\"\n",
        "        stats = []\n",
        "\n",
        "        for resp in tqdm(responses, desc=f\"Processing {classification}\"):\n",
        "            full_answer = resp['full_response']\n",
        "\n",
        "            if resp['logprobs'] and len(resp['logprobs']) > 0:\n",
        "                first_token_data = resp['logprobs'][0]\n",
        "\n",
        "                try:\n",
        "                    answer_token_ids = self.tokenizer.encode(full_answer)\n",
        "                except:\n",
        "                    answer_token_ids = []\n",
        "\n",
        "                first_token_prob = float(np.exp(first_token_data.logprob))\n",
        "                word_alternatives = self._get_word_alternatives(first_token_data)\n",
        "\n",
        "                prob_diff = 0\n",
        "                if len(word_alternatives) >= 2:\n",
        "                    prob_diff = word_alternatives[0]['prob'] - word_alternatives[1]['prob']\n",
        "\n",
        "                # Calculate semantic entropy\n",
        "                semantic_result, generation_details = self.semantic_entropy.calc_semantic_entropy_per_example(\n",
        "                    resp['prompt'],\n",
        "                    resp['true_answer'],\n",
        "                    temp=1.0,\n",
        "                    num_generations=11\n",
        "                )\n",
        "\n",
        "                all_generations = semantic_result.get('all_generations', [])\n",
        "                debug_info = semantic_result.get('debug_info', {})\n",
        "\n",
        "                stats.append({\n",
        "                    \"prompt\": resp['prompt'],\n",
        "                    \"full_llm_output\": full_answer,\n",
        "                    \"true_answer\": resp['true_answer'],\n",
        "                    \"classification\": classification,\n",
        "                    \"match_score\": resp.get('match_score', 0),\n",
        "                    \"match_type\": resp.get('match_type', 'unknown'),\n",
        "                    \"answer_text\": full_answer,\n",
        "                    \"answer_token_ids\": answer_token_ids,\n",
        "                    \"first_token_probability\": first_token_prob,\n",
        "                    \"top_word_alternatives\": word_alternatives[:2],\n",
        "                    \"prob_diff_top2\": float(prob_diff),\n",
        "                    \"semantic_entropy\": float(semantic_result.get('semantic_entropy', 0)),\n",
        "                    \"regular_entropy\": float(semantic_result.get('regular_entropy', 0)),\n",
        "                    \"cluster_assignment_entropy\": float(semantic_result.get('cluster_assignment_entropy', 0)),\n",
        "                    \"all_generations\": all_generations,\n",
        "                    \"num_generations\": len(all_generations),\n",
        "                    \"num_semantic_clusters\": debug_info.get('num_clusters', 1)\n",
        "                })\n",
        "            else:\n",
        "                stats.append({\n",
        "                    \"prompt\": resp['prompt'],\n",
        "                    \"full_llm_output\": full_answer,\n",
        "                    \"true_answer\": resp['true_answer'],\n",
        "                    \"classification\": classification,\n",
        "                    \"match_score\": resp.get('match_score', 0),\n",
        "                    \"match_type\": resp.get('match_type', 'unknown'),\n",
        "                    \"answer_text\": full_answer,\n",
        "                    \"answer_token_ids\": [],\n",
        "                    \"first_token_probability\": 0.5,\n",
        "                    \"top_word_alternatives\": [],\n",
        "                    \"prob_diff_top2\": 0,\n",
        "                    \"semantic_entropy\": 0,\n",
        "                    \"regular_entropy\": 0,\n",
        "                    \"cluster_assignment_entropy\": 0,\n",
        "                    \"all_generations\": [],\n",
        "                    \"num_generations\": 0,\n",
        "                    \"num_semantic_clusters\": 1\n",
        "                })\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def _get_word_alternatives(self, token_data):\n",
        "        \"\"\"Get word alternatives from token data.\"\"\"\n",
        "        word_alternatives = []\n",
        "\n",
        "        if token_data.top_logprobs:\n",
        "            all_alternatives = []\n",
        "            for alt in token_data.top_logprobs:\n",
        "                token = alt.token.strip()\n",
        "                prob = float(np.exp(alt.logprob))\n",
        "\n",
        "                if not token:\n",
        "                    is_complete = False\n",
        "                else:\n",
        "                    # Check for Malayalam or English complete words\n",
        "                    is_complete = (\n",
        "                        len(token) >= 4 or\n",
        "                        token[0].isupper() or\n",
        "                        (' ' not in token and len(token) >= 3)\n",
        "                    )\n",
        "\n",
        "                all_alternatives.append({\n",
        "                    'token': token,\n",
        "                    'prob': prob,\n",
        "                    'is_complete': is_complete\n",
        "                })\n",
        "\n",
        "            all_alternatives.sort(key=lambda x: (x['is_complete'], x['prob']), reverse=True)\n",
        "\n",
        "            for alt in all_alternatives[:5]:\n",
        "                word_alternatives.append({\n",
        "                    'token': alt['token'],\n",
        "                    'prob': alt['prob']\n",
        "                })\n",
        "                if len(word_alternatives) >= 2:\n",
        "                    break\n",
        "\n",
        "        return word_alternatives\n",
        "\n",
        "    def _save_stats_with_analysis(self, hallucination_stats, factuality_stats, match_stats):\n",
        "        \"\"\"Save statistics with match analysis.\"\"\"\n",
        "        try:\n",
        "            with open(f\"{self.path_results}/hallucination.json\", \"w\", encoding='utf-8') as f:\n",
        "                json.dump(hallucination_stats, f, ensure_ascii=False, indent=2)\n",
        "            with open(f\"{self.path_results}/factuality.json\", \"w\", encoding='utf-8') as f:\n",
        "                json.dump(factuality_stats, f, ensure_ascii=False, indent=2)\n",
        "            with open(f\"{self.path_results}/match_analysis.json\", \"w\", encoding='utf-8') as f:\n",
        "                json.dump(match_stats, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"📊 Stats saved with Malayalam text match analysis\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error saving stats: {e}\")\n",
        "\n",
        "    def _print_match_analysis(self, match_stats):\n",
        "        \"\"\"Print analysis of matching patterns.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MALAYALAM TEXT MATCHING ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        total = sum(match_stats.values())\n",
        "        if total > 0:\n",
        "            print(\"\\nMatch Type Distribution:\")\n",
        "            for match_type, count in sorted(match_stats.items(), key=lambda x: -x[1]):\n",
        "                if count > 0:\n",
        "                    percentage = (count / total) * 100\n",
        "                    print(f\"  {match_type:20} : {count:4d} ({percentage:5.1f}%)\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def _print_enhanced_sample_output(self, factuality_stats, hallucination_stats):\n",
        "        \"\"\"Print sample output with Malayalam match information.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"SAMPLE OUTPUT WITH MALAYALAM MATCHING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        if factuality_stats and len(factuality_stats) > 0:\n",
        "            sample = factuality_stats[0]\n",
        "            print(\"\\n📍 FACTUALITY EXAMPLE:\")\n",
        "            print(f\"Generated: {sample['answer_text']}\")\n",
        "            print(f\"Target: {sample['true_answer']}\")\n",
        "            print(f\"Classification: {sample['classification']}\")\n",
        "            print(f\"Match Score: {sample['match_score']:.3f}\")\n",
        "            print(f\"Match Type: {sample['match_type']}\")\n",
        "            print(f\"Semantic Entropy: {sample['semantic_entropy']:.4f}\")\n",
        "            print(f\"Number of generations: {sample['num_generations']}\")\n",
        "            if sample['all_generations'] and len(sample['all_generations']) > 0:\n",
        "                print(f\"Sample generations: {sample['all_generations'][:3]}\")\n",
        "\n",
        "        if hallucination_stats and len(hallucination_stats) > 0:\n",
        "            sample = hallucination_stats[0]\n",
        "            print(\"\\n📍 HALLUCINATION EXAMPLE:\")\n",
        "            print(f\"Generated: {sample['answer_text']}\")\n",
        "            print(f\"Target: {sample['true_answer']}\")\n",
        "            print(f\"Classification: {sample['classification']}\")\n",
        "            print(f\"Match Score: {sample['match_score']:.3f}\")\n",
        "            print(f\"Match Type: {sample['match_type']}\")\n",
        "            print(f\"Semantic Entropy: {sample['semantic_entropy']:.4f}\")\n",
        "            print(f\"Number of generations: {sample['num_generations']}\")\n",
        "            if sample['all_generations'] and len(sample['all_generations']) > 0:\n",
        "                print(f\"Sample generations: {sample['all_generations'][:3]}\")\n",
        "\n",
        "        print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCd4Yhks7PvB",
        "outputId": "f551e41a-7fe0-4c4d-a819-a04be29c294e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Trust_me_Im_wrong/uncertainty_calculation_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models/base_model.py"
      ],
      "metadata": {
        "id": "LYc23FJI_ec_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Trust_me_Im_wrong/calc_semantic_entropy_api.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5zs4EQ-_hQd",
        "outputId": "93ea840a-b6db-46a6-e72c-987bcaf20cd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 13:55:30.526416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763387730.555611   16532 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763387730.564087   16532 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763387730.598772   16532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763387730.598835   16532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763387730.598841   16532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763387730.598846   16532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Trust_me_Im_wrong/uncertainty_calculation_api.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAHgv_Eh_o0P",
        "outputId": "a38658dc-baa7-417c-ce4d-18db1332093e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 13:55:53.995695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763387754.024578   16633 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763387754.037798   16633 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763387754.063561   16633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763387754.063615   16633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763387754.063619   16633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763387754.063623   16633 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Trust_me_Im_wrong.uncertainty_calculation_api import UncertaintyCalculationAPI\n",
        "\n",
        "uncertainty_api = UncertaintyCalculationAPI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    dataset_path=\"/content/\",\n",
        "    dataset_name=\"mal_500\",\n",
        "    method_k_positive=\"alice\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ9CDR4qBe13",
        "outputId": "e8a0dca2-8171-4496-8b1d-b5ef4d5db9e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized SemanticEntropyAPI with gpt-4o-mini\n",
            "Clustering threshold: 0.5\n",
            "Using multilingual embedding model for Malayalam support\n",
            "✓ Initialized Malayalam UncertaintyCalculationAPI\n",
            "  Model: gpt-4o-mini\n",
            "  Using advanced Malayalam text matching\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uncertainty_api.calculate_probabilities_uncertainty(uncertainty_api.data_path_know)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "ShUNjGMEBkDk",
        "outputId": "4f173304-4e34-43e2-aa79-97c2f0a8176a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Generating responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating responses: 100%|██████████| 7/7 [00:06<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Classifying with Malayalam text matching...\n",
            "  Found 5 factual, 2 hallucinations\n",
            "Step 3: Extracting probabilities with semantic entropy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:   0%|          | 0/5 [00:08<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3568763075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muncertainty_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_probabilities_uncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncertainty_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path_know\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Trust_me_Im_wrong/uncertainty_calculation_api.py\u001b[0m in \u001b[0;36mcalculate_probabilities_uncertainty\u001b[0;34m(self, data, with_knowledge)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# Step 3: Extract probabilities with all generations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 3: Extracting probabilities with semantic entropy...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mfactuality_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_probabilities_with_match_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactuality_responses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FACTUALITY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mhallucination_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_probabilities_with_match_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhallucinated_responses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HALLUCINATION\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Trust_me_Im_wrong/uncertainty_calculation_api.py\u001b[0m in \u001b[0;36m_extract_probabilities_with_match_info\u001b[0;34m(self, responses, classification)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mprob_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_alternatives\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                     \u001b[0mprob_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_alternatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prob'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mword_alternatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;31m# Calculate semantic entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\u001b[0m in \u001b[0;36mcalc_semantic_entropy_per_example\u001b[0;34m(self, prompt, answer, temp, num_generations)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing Malayalam question-answer pair\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         results = self.generate_answers(\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0manswer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\u001b[0m in \u001b[0;36mgenerate_answers\u001b[0;34m(self, prompt, answer, num_generations, temperature, compute_acc)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Get response with logprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Store the text for output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\u001b[0m in \u001b[0;36m_get_api_response\u001b[0;34m(self, prompt, temperature, system_message)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQupCX8x9jQz",
        "outputId": "e39fe48e-0fba-4106-8037-c0406eca2041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized SemanticEntropyAPI with gpt-4o-mini\n",
            "Clustering threshold: 0.5\n",
            "Using multilingual embedding model for Malayalam support\n",
            "✓ Initialized Malayalam UncertaintyCalculationAPI\n",
            "  Model: gpt-4o-mini\n",
            "  Using advanced Malayalam text matching\n",
            "Step 1: Generating responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating responses: 100%|██████████| 7/7 [00:04<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Classifying with Malayalam text matching...\n",
            "  Found 5 factual, 2 hallucinations\n",
            "Step 3: Extracting probabilities with semantic entropy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  20%|██        | 1/5 [00:17<01:09, 17.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['മിഷിഗൺ', 'മിഷിഗൺ', 'മിഷിഗൻ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-0.8633527043275535, -0.8136682210024446, -2.297202075365931]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['മിഷിഗൺ', 'മിഷിഗൺ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(4), np.int64(0), np.int64(3), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(0)]\n",
            "Number of semantic clusters: 5\n",
            "Cluster assignment entropy: 1.1596\n",
            "Regular entropy: 911.0561\n",
            "Semantic entropy: 0.2468\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n",
            "Generated texts: ['സ്റ്റുക്കാ', 'സ്റ്റುക്കാ', 'സ്റ്റുക്ക ഏൺ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-1.7320584803819656, -3.9068780913949013, -10001.901096194983]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['സ്റ്റുക്കാ', 'സ്റ്റುക്കാ']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  40%|████      | 2/5 [00:29<00:42, 14.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(6), np.int64(7), np.int64(0), np.int64(0), np.int64(4), np.int64(3), np.int64(5), np.int64(1), np.int64(2)]\n",
            "Number of semantic clusters: 8\n",
            "Cluster assignment entropy: 1.8938\n",
            "Regular entropy: 3640.5446\n",
            "Semantic entropy: 0.5966\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  60%|██████    | 3/5 [00:46<00:31, 15.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ബാർബി', 'ബാർബി ഡോളുകള്\\u200d', 'ബാർബി രാജ്കുമാരി']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-0.32524702697992325, -7.031356435269117, -10002.80022486113]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ബാർബി', 'ബാർബി ഡോളുകള്\\u200d']\n",
            "Semantic IDs: [np.int64(0), np.int64(4), np.int64(3), np.int64(2), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 5\n",
            "Cluster assignment entropy: 1.1596\n",
            "Regular entropy: 911.3438\n",
            "Semantic entropy: 0.3376\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  80%|████████  | 4/5 [01:08<00:18, 18.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['പസഫിക് മഹാസാഗരം', 'ശാന്ത് മഹാസാഗരം', 'ക്രമീകരങ്ങൾ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-2.4578398689627647, -8.962611388415098, -10006.38985824585]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['പസഫിക് മഹാസാഗരം', 'ശാന്ത് മഹാസാഗരം']\n",
            "Semantic IDs: [np.int64(0), np.int64(4), np.int64(5), np.int64(1), np.int64(6), np.int64(0), np.int64(7), np.int64(3), np.int64(2), np.int64(3), np.int64(1)]\n",
            "Number of semantic clusters: 8\n",
            "Cluster assignment entropy: 2.0198\n",
            "Regular entropy: 1823.7086\n",
            "Semantic entropy: 1.1238\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing FACTUALITY: 100%|██████████| 5/5 [01:35<00:00, 19.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ജോൺ', 'ജോൺ', 'ജോൺ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-0.21121689677238464, -0.1478146556764841, -0.19734289310872555]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ജോൺ', 'ജോൺ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(0)]\n",
            "Number of semantic clusters: 3\n",
            "Cluster assignment entropy: 0.6002\n",
            "Regular entropy: 0.8466\n",
            "Semantic entropy: 0.0427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing HALLUCINATION:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing HALLUCINATION:  50%|█████     | 1/2 [00:29<00:29, 29.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ഹൈഡ്രജൻ', 'ഹൈഡ്രജൻ', 'ഹൈഡ്രജൻ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-0.7077273337636143, -0.9283873825334013, -0.9455034122802317]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ഹൈഡ്രജൻ', 'ഹൈഡ്രജൻ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(3), np.int64(0), np.int64(1), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 4\n",
            "Cluster assignment entropy: 0.8856\n",
            "Regular entropy: 1.8490\n",
            "Semantic entropy: 0.1967\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Processing Malayalam question-answer pair\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing HALLUCINATION: 100%|██████████| 2/2 [00:44<00:00, 22.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['\"സ്ക്കൈഫാൾ\"', '\"സോംകസ്\"', 'ജിയോ പ്ളഗ് ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-4.288041769352276, -20004.466959528625, -49996.83490908146]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['\"സ്ക്കൈഫാൾ\"', '\"സോംകസ്\"']\n",
            "Semantic IDs: [np.int64(7), np.int64(9), np.int64(8), np.int64(10), np.int64(3), np.int64(4), np.int64(6), np.int64(5), np.int64(2), np.int64(1), np.int64(0)]\n",
            "Number of semantic clusters: 11\n",
            "Cluster assignment entropy: 2.3979\n",
            "Regular entropy: 10008.6581\n",
            "Semantic entropy: 0.6278\n",
            "📊 Stats saved with Malayalam text match analysis\n",
            "\n",
            "================================================================================\n",
            "MALAYALAM TEXT MATCHING ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Match Type Distribution:\n",
            "  exact                :    2 ( 28.6%)\n",
            "  contains             :    2 ( 28.6%)\n",
            "  no_match             :    2 ( 28.6%)\n",
            "  low_token_overlap    :    1 ( 14.3%)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'prompt': \"ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: ഹെൻറി ഫോർഡ്, മാജിക് ജോൺസൺ, ബെറി ഗോർഡി എന്നിവരെല്ലാം ഏത് യുഎസ് സംസ്ഥാനത്താണ് ജനിച്ചത്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'മിഷിഗൺ',\n",
              "   'true_answer': 'മിഷിഗൺ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'മിഷിഗൺ',\n",
              "   'answer_token_ids': [4719, 56008, 146894, 40546],\n",
              "   'first_token_probability': 0.9431643412684418,\n",
              "   'top_word_alternatives': [{'token': 'Michigan',\n",
              "     'prob': 0.006354992242513312},\n",
              "    {'token': 'മാണ', 'prob': 0.0011043320625785257}],\n",
              "   'prob_diff_top2': 0.0052506601799347866,\n",
              "   'semantic_entropy': 0.24684465616728155,\n",
              "   'regular_entropy': 911.0560790133154,\n",
              "   'cluster_assignment_entropy': 1.1595888138086259,\n",
              "   'all_generations': ['മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൻ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മICHിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിച്ചിഗൻ',\n",
              "    'മిచിഗൻ',\n",
              "    'മിഷിഗൺ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 5},\n",
              "  {'prompt': 'ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: ഫ്രാൻസിന്റെ തലസ്ഥാനം എന്താണ്?\\nഉത്തരം: പാരീസ്\\nചോദ്യം: രണ്ടാം ലോക മഹായുദ്ധത്തിലെ ജർമൻ സ്ഫോടക ബോംബായുധമായ ജങ്കേഴ്സ് ജു 87ന്റെ കൂടുതൽ അറിയപ്പെടുന്ന പേര് എന്തായിരുന്നു?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'സ്റ്റുക്കാ',\n",
              "   'true_answer': 'സ്റ്റുക്ക',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.95,\n",
              "   'match_type': 'contains',\n",
              "   'answer_text': 'സ്റ്റുക്കാ',\n",
              "   'answer_token_ids': [37290, 33907, 1674],\n",
              "   'first_token_probability': 0.9336160334979097,\n",
              "   'top_word_alternatives': [{'token': 'സ്റ്റ', 'prob': 0.9336160334979097},\n",
              "    {'token': 'സ്ത', 'prob': 0.011752493793686834}],\n",
              "   'prob_diff_top2': 0.9218635397042229,\n",
              "   'semantic_entropy': 0.5966240616393698,\n",
              "   'regular_entropy': 3640.5445700441064,\n",
              "   'cluster_assignment_entropy': 1.8937882315911376,\n",
              "   'all_generations': ['സ്റ്റുക്കാ',\n",
              "    'സ്റ്റುക്കാ',\n",
              "    'സ്റ്റുക്ക ഏൺ',\n",
              "    'സ്റ്റുക്ക മാഡെയർ',\n",
              "    'സ്റ്റുക്കാ',\n",
              "    'ഷറ്റുക്കാ',\n",
              "    'സ്റ്റುക്ക എന്ന പേരിൽ അറിയപ്പെടുന്നു',\n",
              "    'സ്റ്റുക്കა',\n",
              "    'സ്റ്റുക്കা',\n",
              "    'സ്റ്റൂക്കാ',\n",
              "    'സ്റ്റുക്കാസ്'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 8},\n",
              "  {'prompt': 'ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\nചോദ്യം: 1959 - ൽ യുവതികൾക്ക് ഏതു കളിപ്പാട്ടമാണ് റൂത്ത് ഹാൻഡ്ലർ അവതരിപ്പിച്ചത്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ബാർബി',\n",
              "   'true_answer': 'ബാർബി ഡോൾ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.5,\n",
              "   'match_type': 'low_token_overlap',\n",
              "   'answer_text': 'ബാർബി',\n",
              "   'answer_token_ids': [17133, 27620, 148020],\n",
              "   'first_token_probability': 0.9618808731596313,\n",
              "   'top_word_alternatives': [{'token': 'ബ്ര', 'prob': 0.015547357156552224},\n",
              "    {'token': 'بار', 'prob': 0.003930987499294697}],\n",
              "   'prob_diff_top2': 0.011616369657257528,\n",
              "   'semantic_entropy': 0.3375533735765703,\n",
              "   'regular_entropy': 911.34380823069,\n",
              "   'cluster_assignment_entropy': 1.1595888138086259,\n",
              "   'all_generations': ['ബാർബി',\n",
              "    'ബാർബി ഡോളുകള്\\u200d',\n",
              "    'ബാർബി രാജ്കുമാരി',\n",
              "    'ബാർബി ഡോൾ',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി ഡാൾ',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 5},\n",
              "  {'prompt': \"ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: ലോക കാലാവസ്ഥയെ ബാധിക്കുന്ന ചൂടുള്ള ഒഴുക്ക് എൽ നിനോ എവിടെയാണ്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'പസഫിക് മഹാസാഗരം',\n",
              "   'true_answer': 'പസഫിക്',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.95,\n",
              "   'match_type': 'contains',\n",
              "   'answer_text': 'പസഫിക് മഹാസാഗരം',\n",
              "   'answer_token_ids': [6263, 4851, 27162, 85998, 130421, 22042, 32190, 38101],\n",
              "   'first_token_probability': 0.7482364575233225,\n",
              "   'top_word_alternatives': [{'token': 'പ്ര', 'prob': 0.11474578055534654},\n",
              "    {'token': 'പ്പ', 'prob': 0.004449176743066173}],\n",
              "   'prob_diff_top2': 0.11029660381228036,\n",
              "   'semantic_entropy': 1.1238225471696675,\n",
              "   'regular_entropy': 1823.708580844951,\n",
              "   'cluster_assignment_entropy': 2.019814991692946,\n",
              "   'all_generations': ['പസഫിക് മഹാസാഗരം',\n",
              "    'ശാന്ത് മഹാസാഗരം',\n",
              "    'ക്രമീകരങ്ങൾ',\n",
              "    'പസിഫിക് സമുദ്രം',\n",
              "    'പസഫിക് സമുദ്രമധ്യേ',\n",
              "    'പസിഫിക് മഹാസാഗരം',\n",
              "    'താണ്ടിയിൽ',\n",
              "    'പസഫിക് മഹാസാഗരത്തിൽ',\n",
              "    'പസഫിക് മഹാസ്agar',\n",
              "    'പസഫിക് മഹാസാഗരത്തിൽ',\n",
              "    'പസഫിക് സമുദ്രം'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 8},\n",
              "  {'prompt': \"ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: സാങ്കൽപ്പിക കഥാപാത്രമായ ഷെർലക് ഹോംസിന്റെ സ്നേഹിതനും സഹായിയും ആയ ഡോക്ടർ വാട്ട്സണ്റെ ആദ്യനാമം എന്താണ്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'ജോൺ',\n",
              "   'true_answer': 'ജോൺ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'ജോൺ',\n",
              "   'answer_token_ids': [23597, 4384, 40546],\n",
              "   'first_token_probability': 0.9704315633928982,\n",
              "   'top_word_alternatives': [{'token': 'John', 'prob': 0.009513775149246792},\n",
              "    {'token': 'جون', 'prob': 0.0027257422165084767}],\n",
              "   'prob_diff_top2': 0.006788032932738316,\n",
              "   'semantic_entropy': 0.04267051124843188,\n",
              "   'regular_entropy': 0.8466377200728114,\n",
              "   'cluster_assignment_entropy': 0.6001660728596456,\n",
              "   'all_generations': ['ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജಾನ್',\n",
              "    'ജോൻ',\n",
              "    'ജോൺ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 3}],\n",
              " [{'prompt': \"ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം ഏതാണ്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'ഹൈഡ്രജൻ',\n",
              "   'true_answer': 'മീഥെയ്ൻ',\n",
              "   'classification': 'HALLUCINATION',\n",
              "   'match_score': 0.0,\n",
              "   'match_type': 'no_match',\n",
              "   'answer_text': 'ഹൈഡ്രജൻ',\n",
              "   'answer_token_ids': [17648, 14863, 15830, 4657, 23597, 11548],\n",
              "   'first_token_probability': 0.7805544185890214,\n",
              "   'top_word_alternatives': [{'token': 'കാര', 'prob': 0.0021924168501223037},\n",
              "    {'token': 'പ്ര', 'prob': 0.0020595850281372664}],\n",
              "   'prob_diff_top2': 0.00013283182198503725,\n",
              "   'semantic_entropy': 0.19665743546106995,\n",
              "   'regular_entropy': 1.8489937406583605,\n",
              "   'cluster_assignment_entropy': 0.8855741511766717,\n",
              "   'all_generations': ['ഹൈഡ്രജൻ',\n",
              "    'ഹൈഡ്രജൻ',\n",
              "    'ഹൈഡ്രജൻ',\n",
              "    'ഹീലിയം',\n",
              "    'ഹൈഡ്രജൻ',\n",
              "    'അർഗൺ',\n",
              "    'ഹൈഡ്രജൻ',\n",
              "    'കാർബൺ',\n",
              "    'ഹൈഡ്രജൻ',\n",
              "    'ഹൈഡ്രജൻ',\n",
              "    'ഹൈഡ്രജൻ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 4},\n",
              "  {'prompt': 'ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\nചോദ്യം: പോളും ലിൻഡ മക്കാർട്ട്\\u200cനിയും ചേർന്ന് രചിച്ച ബോണ്ട് സിനിമയിലെ തീം സോങ്ങ് ഏതാണ്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'സിൽവർ ഫിംഗർ',\n",
              "   'true_answer': 'ലൈവ് ആൻഡ് ലെറ്റ് ഡൈ',\n",
              "   'classification': 'HALLUCINATION',\n",
              "   'match_score': 0.0,\n",
              "   'match_type': 'no_match',\n",
              "   'answer_text': 'സിൽവർ ഫിംഗർ',\n",
              "   'answer_token_ids': [4851, 10751, 98980, 26509, 50252, 12067, 7208],\n",
              "   'first_token_probability': 0.23707533767102307,\n",
              "   'top_word_alternatives': [{'token': 'Live', 'prob': 0.022051441437915778},\n",
              "    {'token': 'സ്ക', 'prob': 0.01946032876648627}],\n",
              "   'prob_diff_top2': 0.002591112671429509,\n",
              "   'semantic_entropy': 0.6277735830267138,\n",
              "   'regular_entropy': 10008.65806442076,\n",
              "   'cluster_assignment_entropy': 2.3978952716983706,\n",
              "   'all_generations': ['\"സ്ക്കൈഫാൾ\"',\n",
              "    '\"സോംകസ്\"',\n",
              "    'ജിയോ പ്ളഗ് ',\n",
              "    '\"സെവൻറിൻ\"',\n",
              "    '\"എൻഡിങ്\" (Live and Let',\n",
              "    '\"സкот്ലാൻഡ് അയോസ് അഗ്',\n",
              "    'നൊട്ടി സോംഗ്',\n",
              "    '\"സkyfall\"',\n",
              "    'സ്മൈർഡ്',\n",
              "    'പ്രേക്ഷകരുടെ നാട്',\n",
              "    '\"ഇന്ടർനാഷണൽ മണ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 11}])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from Trust_me_Im_wrong.uncertainty_calculation_api import UncertaintyCalculationAPI\n",
        "\n",
        "uncertainty_api = UncertaintyCalculationAPI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    dataset_path=\"/content/\",\n",
        "    dataset_name=\"mal_500\",\n",
        "    method_k_positive=\"child\"\n",
        ")\n",
        "uncertainty_api.calculate_probabilities_uncertainty(uncertainty_api.data_path_know)"
      ]
    }
  ]
}