{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcDjG-IfKnWk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8_hOc-guBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d5e24a-05dd-45f2-942b-8bc62e0c0ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure created!\n"
          ]
        }
      ],
      "source": [
        "# Create the directory structure\n",
        "import os\n",
        "\n",
        "dirs = [\n",
        "    'Trust_me_Im_wrong',\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty',\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty/uncertainty',\n",
        "\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models',\n",
        "    'Trust_me_Im_wrong/semantic_uncertainty/uncertainty/uncertainty_measures',\n",
        "\n",
        "]\n",
        "\n",
        "for dir_path in dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOPDfZoKK3No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX5tp3s0ayI1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "dirs = [\n",
        "    'Trust_me_Im_wrong/analysis',\n",
        "    'Trust_me_Im_wrong//visualization',\n",
        "     'Trust_me_Im_wrong/reporting',\n",
        "     'Trust_me_Im_wrong/reports',\n",
        "     'Trust_me_Im_wrong/visualizations'\n",
        "]\n",
        "for dir_path in dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4yhUDVLK8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kff_s7yeNwNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067dc928-0a71-4195-de9f-a902be62ad54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.py already exists in Trust_me_Im_wrong/analysis\n",
            "__init__.py already exists in Trust_me_Im_wrong//visualization\n",
            "__init__.py already exists in Trust_me_Im_wrong/reporting\n",
            "__init__.py already exists in Trust_me_Im_wrong/reports\n",
            "__init__.py already exists in Trust_me_Im_wrong/visualizations\n"
          ]
        }
      ],
      "source": [
        "# Create __init__.py in each directory to mark as package\n",
        "for dir_path in dirs:\n",
        "    init_path = os.path.join(dir_path, '__init__.py')\n",
        "    if not os.path.exists(init_path):\n",
        "        with open(init_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('# Init file for package\\n')\n",
        "        print(f'Created __init__.py in {dir_path}')\n",
        "    else:\n",
        "        print(f'__init__.py already exists in {dir_path}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models/base_model.py\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Text, Optional, Tuple\n",
        "\n",
        "# Full stop sequences for post-processing (all languages)\n",
        "FULL_STOP_SEQUENCES = [\n",
        "    '\\n',\n",
        "]\n",
        "\n",
        "# API-compatible stop sequences (max 4 for OpenAI/DeepSeek API)\n",
        "API_STOP_SEQUENCES = [\n",
        "    '\\n',       # Double newline - most common separator\n",
        "    '.',        # Period - ends sentences\n",
        "]\n",
        "\n",
        "# Keep original for backward compatibility\n",
        "STOP_SEQUENCES = FULL_STOP_SEQUENCES\n",
        "\n",
        "\n",
        "class BaseModel(ABC):\n",
        "    \"\"\"Base model class with enhanced stop sequence handling (Malayalam Tailored).\"\"\"\n",
        "\n",
        "    # Class variables\n",
        "    stop_sequences: List[Text] = FULL_STOP_SEQUENCES\n",
        "    api_stop_sequences: List[Text] = API_STOP_SEQUENCES\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize with both API and full stop sequences.\"\"\"\n",
        "        self.stop_sequences = FULL_STOP_SEQUENCES\n",
        "        self.api_stop_sequences = API_STOP_SEQUENCES\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, input_data: str, temperature: float):\n",
        "        \"\"\"\n",
        "        Generate a response from the model given input_data and temperature.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_p_true(self, input_data: str):\n",
        "        \"\"\"\n",
        "        Compute probability that the answer to input_data is 'True'.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def post_process_with_stops(\n",
        "        text: str,\n",
        "        stop_sequences: Optional[List[str]] = None,\n",
        "        preserve_stop: bool = False\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Post-process text by truncating at the first occurrence of any stop sequence.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to process\n",
        "            stop_sequences: List of stop sequences (uses FULL_STOP_SEQUENCES if None)\n",
        "            preserve_stop: If True, include the stop sequence in output\n",
        "\n",
        "        Returns:\n",
        "            Truncated text\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        if stop_sequences is None:\n",
        "            stop_sequences = FULL_STOP_SEQUENCES\n",
        "\n",
        "        # Find the earliest occurrence of any stop sequence\n",
        "        earliest_pos = len(text)\n",
        "        earliest_stop = None\n",
        "\n",
        "        for stop in stop_sequences:\n",
        "            pos = text.find(stop)\n",
        "            if pos != -1 and pos < earliest_pos:\n",
        "                earliest_pos = pos\n",
        "                earliest_stop = stop\n",
        "\n",
        "        # Truncate at the earliest stop sequence\n",
        "        if earliest_pos < len(text):\n",
        "            if preserve_stop and earliest_stop:\n",
        "                return text[:earliest_pos + len(earliest_stop)]\n",
        "            else:\n",
        "                return text[:earliest_pos]\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_for_comparison(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Aggressively clean text for accurate comparison and hallucination detection.\n",
        "        Tailored for Malayalam language processing.\n",
        "\n",
        "        Args:\n",
        "            text: Text to clean\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase (works for standard chars, Malayalam chars remain largely unaffected but safe)\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove common Malayalam articles, conjunctions, and fillers (Stop Words)\n",
        "        # Based on common linguistic patterns in Malayalam\n",
        "        remove_words = [\n",
        "            'ഒരു', 'ഈ', 'ആ', 'അത', 'ഇത', 'എന്', 'എന്ന', 'ആണ്', 'ഉം',\n",
        "            'അല്ല', 'ഉണ്ട്', 'ഇല്ല', 'മാത്രം', 'വരെ', 'മുതൽ', 'നിന്ന്',\n",
        "            'കൂടെ', 'പറ്റി', 'കൊണ്ട്', 'വേണ്ടി', 'തമ്മിൽ', 'പോലെ',\n",
        "            'മറ്റും', 'എങ്കിൽ', 'അതെ', 'അതാ', 'ഇതാ', 'വളരെ', 'ഏറ്റവും'\n",
        "        ]\n",
        "\n",
        "        for word in remove_words:\n",
        "            # Replace word with surrounding spaces to avoid breaking inside other words\n",
        "            text = text.replace(f\" {word} \", \" \")\n",
        "\n",
        "            # Check boundaries if the word is at start or end\n",
        "            if text.startswith(f\"{word} \"):\n",
        "                text = text[len(word)+1:]\n",
        "            if text.endswith(f\" {word}\"):\n",
        "                text = text[:-len(word)-1]\n",
        "\n",
        "        # Remove all punctuation and special characters\n",
        "        # Includes standard punctuation and common symbols\n",
        "        special_chars = [\n",
        "             '!', '?', ';', ',', '.', ':', '\"', \"'\", \"-\", \"_\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\",\n",
        "             '|', '/', '\\\\', '@', '#', '$', '%', '^', '&', '*'\n",
        "        ]\n",
        "\n",
        "        for char in special_chars:\n",
        "            text = text.replace(char, ' ')\n",
        "\n",
        "        # Normalize whitespace (collapse multiple spaces into one)\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_api_compatible_stops() -> List[str]:\n",
        "        \"\"\"\n",
        "        Get API-compatible stop sequences (max 4).\n",
        "\n",
        "        Returns:\n",
        "            List of up to 4 stop sequences for API calls\n",
        "        \"\"\"\n",
        "        return API_STOP_SEQUENCES[:4]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_all_stops() -> List[str]:\n",
        "        \"\"\"\n",
        "        Get all stop sequences for post-processing.\n",
        "\n",
        "        Returns:\n",
        "            Complete list of stop sequences\n",
        "        \"\"\"\n",
        "        return FULL_STOP_SEQUENCES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blKLSR_zN1Ug",
        "outputId": "b28032ba-6e1d-4818-c8f2-66534acd8c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models/base_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\n",
        "\"\"\"\n",
        "Semantic Entropy Calculation for API Models (DeepSeek V3 Strict - Malayalam)\n",
        "Uses DeepSeek API with logprobs and MULTILINGUAL sentence transformers for clustering.\n",
        "Strictly uses DeepSeek V3 API and Hugging Face AutoTokenizer.\n",
        "Adapted for Malayalam datasets.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from google.colab import userdata\n",
        "import sys\n",
        "sys.path.append('/content/Trust_me_Im_wrong')\n",
        "\n",
        "from semantic_uncertainty.uncertainty.models.base_model import (\n",
        "    API_STOP_SEQUENCES, FULL_STOP_SEQUENCES, BaseModel\n",
        ")\n",
        "\n",
        "\n",
        "class SemanticEntropyAPI:\n",
        "    def __init__(self, model_name=\"deepseek-chat\", dataset_path=\"datasets/\",\n",
        "                 entailment_model=\"sentence_transformer\", max_new_tokens=10):\n",
        "        \"\"\"\n",
        "        Initialize semantic entropy calculator strictly for DeepSeek API (V3).\n",
        "        Configured for Malayalam language processing.\n",
        "        \"\"\"\n",
        "        random.seed(0)\n",
        "        self.model_name = model_name  # \"deepseek-chat\" points to V3\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "        # 1. Initialize API client for DeepSeek (Strict: No Fallback)\n",
        "        try:\n",
        "            api_key = userdata.get('deepseek')\n",
        "            if not api_key:\n",
        "                raise ValueError(\"DeepSeek API key not found in secrets.\")\n",
        "\n",
        "            self.client = OpenAI(\n",
        "                api_key=api_key,\n",
        "                base_url=\"https://api.deepseek.com\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to initialize DeepSeek API client: {e}\")\n",
        "\n",
        "        # 2. Initialize Tokenizer (Strict: Hugging Face AutoTokenizer Only)\n",
        "        try:\n",
        "            hf_token = userdata.get('hftoken')\n",
        "            if not hf_token:\n",
        "                 print(\"Warning: 'hftoken' secret not found. Tokenizer load might fail if model is gated.\")\n",
        "\n",
        "            # Use DeepSeek-V3 tokenizer\n",
        "            tokenizer_path = \"deepseek-ai/DeepSeek-V3\"\n",
        "            print(f\"Loading DeepSeek tokenizer from {tokenizer_path}...\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                tokenizer_path,\n",
        "                token=hf_token,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "             raise RuntimeError(f\"CRITICAL: Failed to load DeepSeek tokenizer via AutoTokenizer. \\nError: {e}\\nEnsure 'hftoken' is set correctly.\")\n",
        "\n",
        "        # Initialize sentence transformer for semantic similarity\n",
        "        # CRITICAL: Using multilingual model for Malayalam support\n",
        "        print(\"Loading multilingual embedding model for Malayalam semantic clustering...\")\n",
        "        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "        # Clustering threshold (Malayalam is agglutinative, 0.5 is a safe start)\n",
        "        self.clustering_threshold = 0.5\n",
        "\n",
        "        # Setup stop sequences\n",
        "        self.api_stops = API_STOP_SEQUENCES[:4]\n",
        "        self.full_stops = FULL_STOP_SEQUENCES\n",
        "\n",
        "        print(f\"Initialized SemanticEntropyAPI with {model_name} (DeepSeek Strict - Malayalam)\")\n",
        "        print(f\"Clustering threshold: {self.clustering_threshold}\")\n",
        "\n",
        "    def generate_answers(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        answer: str,\n",
        "        num_generations: int = 11,\n",
        "        temperature: float = 1.0,\n",
        "        compute_acc: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Generate multiple answers from API model for entropy calculation.\n",
        "        \"\"\"\n",
        "        generations = {}\n",
        "        generations[prompt] = {\"question\": prompt}\n",
        "        full_responses = []\n",
        "        all_generation_texts = []  # Store all generated texts for output\n",
        "\n",
        "        # System message for consistency (Malayalam)\n",
        "        # Translates to: \"Give only direct, short answers. Do not build sentences, do not give explanations.\"\n",
        "        system_message = \"നേരിട്ടുള്ളതും ഹ്രസ്വവുമായ ഉത്തരങ്ങൾ മാത്രം നൽകുക. വാചകങ്ങൾ നിർമ്മിക്കരുത്, വിശദീകരണം നൽകരുത്.\"\n",
        "\n",
        "        print(f\"Generating {num_generations} responses...\")\n",
        "\n",
        "        for i in range(num_generations):\n",
        "            # First generation at low temperature\n",
        "            temp = 0.1 if i == 0 else temperature\n",
        "\n",
        "            # Get response with logprobs\n",
        "            response_data = self._get_api_response(prompt, temp, system_message)\n",
        "\n",
        "            # Store the text for output\n",
        "            all_generation_texts.append(response_data[\"text\"])\n",
        "\n",
        "            if i == 0:\n",
        "                # Store most likely answer\n",
        "                most_likely_answer_dict = {\n",
        "                    \"response\": response_data[\"text\"],\n",
        "                    \"token_log_likelihoods\": response_data[\"token_logprobs\"],\n",
        "                    \"embedding\": None,\n",
        "                    \"accuracy\": self._check_accuracy(response_data[\"text\"], answer) if compute_acc else 0.0,\n",
        "                    \"total_logprob\": response_data[\"total_logprob\"]\n",
        "                }\n",
        "                generations[prompt][\"most_likely_answer\"] = most_likely_answer_dict\n",
        "            else:\n",
        "                # Store high temperature responses\n",
        "                full_responses.append((\n",
        "                    response_data[\"text\"],\n",
        "                    response_data[\"token_logprobs\"],\n",
        "                    None,\n",
        "                    self._check_accuracy(response_data[\"text\"], answer) if compute_acc else 0.0,\n",
        "                    response_data[\"total_logprob\"]  # Add total logprob\n",
        "                ))\n",
        "\n",
        "        generations[prompt][\"responses\"] = full_responses\n",
        "        generations[prompt][\"reference\"] = answer\n",
        "        generations[prompt][\"all_generation_texts\"] = all_generation_texts  # Add all texts\n",
        "\n",
        "        print(f\"Generated texts: {all_generation_texts[:3]}...\")  # Show first 3\n",
        "\n",
        "        return {\n",
        "            \"accuracies\": [most_likely_answer_dict[\"accuracy\"]] if generations[prompt].get(\"most_likely_answer\") else [],\n",
        "            \"generations\": generations,\n",
        "            \"question\": prompt,\n",
        "            \"reference\": answer,\n",
        "            \"all_generation_texts\": all_generation_texts\n",
        "        }\n",
        "\n",
        "    def _get_api_response(self, prompt: str, temperature: float, system_message: str = None):\n",
        "        \"\"\"Get response from DeepSeek API with logprobs.\"\"\"\n",
        "        try:\n",
        "            messages = []\n",
        "            if system_message:\n",
        "                messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "            # DeepSeek API call\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=self.max_new_tokens,\n",
        "                logprobs=True,\n",
        "                top_logprobs=20,\n",
        "                stop=self.api_stops\n",
        "            )\n",
        "\n",
        "            choice = response.choices[0]\n",
        "            raw_text = choice.message.content or \"\"\n",
        "\n",
        "            # Post-process with full stop sequences\n",
        "            processed_text = BaseModel.post_process_with_stops(\n",
        "                raw_text,\n",
        "                self.full_stops,\n",
        "                preserve_stop=False\n",
        "            )\n",
        "\n",
        "            # Extract logprobs\n",
        "            total_logprob = 0\n",
        "            token_logprobs = []\n",
        "\n",
        "            if hasattr(choice, 'logprobs') and choice.logprobs and choice.logprobs.content:\n",
        "                for logprob_data in choice.logprobs.content:\n",
        "                    token_logprobs.append(logprob_data.logprob)\n",
        "                    total_logprob += logprob_data.logprob\n",
        "\n",
        "            return {\n",
        "                \"text\": processed_text,\n",
        "                \"raw_text\": raw_text,\n",
        "                \"token_logprobs\": token_logprobs,\n",
        "                \"total_logprob\": total_logprob  # Sum of all token logprobs\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"API error: {e}\")\n",
        "            return {\n",
        "                \"text\": \"\",\n",
        "                \"raw_text\": \"\",\n",
        "                \"token_logprobs\": [],\n",
        "                \"total_logprob\": 0\n",
        "            }\n",
        "\n",
        "    def _check_accuracy(self, generated: str, target: str) -> float:\n",
        "        \"\"\"Check if generated text matches target (Internal basic check).\"\"\"\n",
        "        gen_clean = BaseModel.clean_for_comparison(generated)\n",
        "        target_clean = BaseModel.clean_for_comparison(target)\n",
        "\n",
        "        if not gen_clean or not target_clean:\n",
        "            return 0.0\n",
        "\n",
        "        return 1.0 if (target_clean in gen_clean or gen_clean in target_clean) else 0.0\n",
        "\n",
        "    def compute_uncertainty_measures(\n",
        "        self,\n",
        "        model_generations,\n",
        "        compute_predictive_entropy=True,\n",
        "        strict_entailment=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Compute semantic entropy and other uncertainty measures using clustering.\n",
        "        \"\"\"\n",
        "        result_dict = {\"semantic_ids\": [], \"debug_info\": {}}\n",
        "        entropies = defaultdict(list)\n",
        "\n",
        "        for tid in model_generations:\n",
        "            example = model_generations[tid]\n",
        "            full_responses = example.get(\"responses\", [])\n",
        "\n",
        "            # Include the most likely answer in calculations\n",
        "            if \"most_likely_answer\" in example:\n",
        "                most_likely = example[\"most_likely_answer\"]\n",
        "                full_responses.insert(0, (\n",
        "                    most_likely[\"response\"],\n",
        "                    most_likely[\"token_log_likelihoods\"],\n",
        "                    None,\n",
        "                    most_likely.get(\"accuracy\", 0),\n",
        "                    most_likely.get(\"total_logprob\", sum(most_likely[\"token_log_likelihoods\"]))\n",
        "                ))\n",
        "\n",
        "            if not full_responses:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {len(full_responses)} total generations\")\n",
        "\n",
        "            # Clean responses before clustering\n",
        "            raw_responses = [r[0] for r in full_responses]\n",
        "            responses = [BaseModel.post_process_with_stops(r, self.full_stops) for r in raw_responses]\n",
        "\n",
        "            if compute_predictive_entropy:\n",
        "                # Get TOTAL log likelihoods (not per token average)\n",
        "                total_log_liks = []\n",
        "                for r in full_responses:\n",
        "                    if len(r) >= 5:  # Has total_logprob\n",
        "                        total_log_liks.append(r[4])\n",
        "                    elif r[1]:  # Fallback to sum of token logprobs\n",
        "                        total_log_liks.append(sum(r[1]))\n",
        "                    else:\n",
        "                        total_log_liks.append(-10)  # Default low value\n",
        "\n",
        "                print(f\"Total log likelihoods: {total_log_liks[:3]}...\")\n",
        "\n",
        "                # Compute semantic IDs using clustering\n",
        "                semantic_ids = self._get_semantic_ids_clustering(responses)\n",
        "                result_dict[\"semantic_ids\"].append(semantic_ids)\n",
        "\n",
        "                print(f\"Semantic IDs: {semantic_ids}\")\n",
        "                unique_clusters = len(set(sid for sid in semantic_ids if sid >= 0))\n",
        "                print(f\"Number of semantic clusters: {unique_clusters}\")\n",
        "\n",
        "                # Store debug info\n",
        "                result_dict[\"debug_info\"][\"num_generations\"] = len(full_responses)\n",
        "                result_dict[\"debug_info\"][\"num_clusters\"] = unique_clusters\n",
        "                result_dict[\"debug_info\"][\"responses_sample\"] = responses[:3]\n",
        "\n",
        "                # Compute cluster assignment entropy\n",
        "                cluster_entropy = self._cluster_assignment_entropy(semantic_ids)\n",
        "                entropies[\"cluster_assignment_entropy\"].append(cluster_entropy)\n",
        "                print(f\"Cluster assignment entropy: {cluster_entropy:.4f}\")\n",
        "\n",
        "                # Regular entropy - using total log likelihoods\n",
        "                regular_entropy = self._predictive_entropy_corrected(total_log_liks)\n",
        "                entropies[\"regular_entropy\"].append(regular_entropy)\n",
        "                print(f\"Regular entropy: {regular_entropy:.4f}\")\n",
        "\n",
        "                # Semantic entropy - aggregate by cluster then compute entropy\n",
        "                log_likelihood_per_semantic_id = self._logsumexp_by_id(\n",
        "                    semantic_ids, total_log_liks\n",
        "                )\n",
        "                semantic_entropy = self._semantic_entropy_corrected(log_likelihood_per_semantic_id)\n",
        "                entropies[\"semantic_entropy\"].append(semantic_entropy)\n",
        "                print(f\"Semantic entropy: {semantic_entropy:.4f}\")\n",
        "\n",
        "        # Compute averages\n",
        "        avg_entropies = {k: np.mean(v) if v else 0 for k, v in entropies.items()}\n",
        "\n",
        "        return avg_entropies, result_dict\n",
        "\n",
        "    def _get_semantic_ids_clustering(self, responses: list) -> list:\n",
        "        \"\"\"Cluster responses using sentence embeddings.\"\"\"\n",
        "        if not responses or len(responses) <= 1:\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        valid_responses = [r for r in responses if r and r.strip()]\n",
        "        if not valid_responses:\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        # Show what we're clustering\n",
        "        print(f\"Clustering {len(valid_responses)} responses\")\n",
        "        print(f\"Sample responses: {valid_responses[:2]}\")\n",
        "\n",
        "        try:\n",
        "            embeddings = self.embedding_model.encode(valid_responses)\n",
        "        except Exception as e:\n",
        "            print(f\"Embedding error: {e}\")\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        if len(valid_responses) == 1:\n",
        "            return [0] * len(responses)\n",
        "\n",
        "        clustering = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            distance_threshold=self.clustering_threshold,\n",
        "            linkage='average'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            semantic_ids = clustering.fit_predict(embeddings)\n",
        "            full_ids = []\n",
        "            valid_idx = 0\n",
        "            for r in responses:\n",
        "                if r and r.strip():\n",
        "                    full_ids.append(semantic_ids[valid_idx])\n",
        "                    valid_idx += 1\n",
        "                else:\n",
        "                    full_ids.append(-1)\n",
        "            return full_ids\n",
        "        except Exception as e:\n",
        "            print(f\"Clustering error: {e}\")\n",
        "            return [0] * len(responses)\n",
        "\n",
        "    def _cluster_assignment_entropy(self, semantic_ids: list) -> float:\n",
        "        \"\"\"Calculate entropy of cluster assignments.\"\"\"\n",
        "        if not semantic_ids:\n",
        "            return 0.0\n",
        "\n",
        "        valid_ids = [sid for sid in semantic_ids if sid >= 0]\n",
        "        if not valid_ids:\n",
        "            return 0.0\n",
        "\n",
        "        unique, counts = np.unique(valid_ids, return_counts=True)\n",
        "        probs = counts / len(valid_ids)\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "        return float(entropy)\n",
        "\n",
        "    def _predictive_entropy_corrected(self, total_log_liks: list) -> float:\n",
        "        \"\"\"\n",
        "        CORRECTED: Calculate predictive entropy from log likelihoods.\n",
        "        H = -E[log p(y|x)] ≈ -mean(log_likelihood)\n",
        "        \"\"\"\n",
        "        if not total_log_liks:\n",
        "            return 0.0\n",
        "\n",
        "        # Average negative log likelihood\n",
        "        avg_neg_log_lik = -np.mean(total_log_liks)\n",
        "        return float(avg_neg_log_lik)\n",
        "\n",
        "    def _semantic_entropy_corrected(self, log_likelihood_per_semantic_id: list) -> float:\n",
        "        \"\"\"\n",
        "        CORRECTED: Calculate semantic entropy from aggregated log likelihoods.\n",
        "        \"\"\"\n",
        "        if not log_likelihood_per_semantic_id:\n",
        "            return 0.0\n",
        "\n",
        "        # Convert log likelihoods to probabilities\n",
        "        max_log_lik = max(log_likelihood_per_semantic_id)\n",
        "        log_probs_normalized = [ll - max_log_lik for ll in log_likelihood_per_semantic_id]\n",
        "        probs = np.exp(log_probs_normalized)\n",
        "        probs = probs / (np.sum(probs) + 1e-10)\n",
        "\n",
        "        # Calculate entropy\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "        return float(entropy)\n",
        "\n",
        "    def _logsumexp_by_id(self, semantic_ids: list, total_log_liks: list) -> list:\n",
        "        \"\"\"\n",
        "        Aggregate log probabilities by semantic ID using logsumexp.\n",
        "        \"\"\"\n",
        "        if not semantic_ids or not total_log_liks:\n",
        "            return []\n",
        "\n",
        "        valid_pairs = [(sid, ll) for sid, ll in zip(semantic_ids, total_log_liks)\n",
        "                       if sid >= 0 and ll is not None]\n",
        "        if not valid_pairs:\n",
        "            return []\n",
        "\n",
        "        # Group by semantic ID\n",
        "        id_to_log_liks = defaultdict(list)\n",
        "        for sid, ll in valid_pairs:\n",
        "            id_to_log_liks[sid].append(ll)\n",
        "\n",
        "        # Aggregate using logsumexp for each semantic ID\n",
        "        log_likelihood_per_id = []\n",
        "        for sid in sorted(id_to_log_liks.keys()):\n",
        "            lls = id_to_log_liks[sid]\n",
        "            if lls:\n",
        "                # Logsumexp: log(sum(exp(x_i)))\n",
        "                max_ll = max(lls)\n",
        "                sum_exp = sum(np.exp(ll - max_ll) for ll in lls)\n",
        "                aggregated_ll = max_ll + np.log(sum_exp)\n",
        "                log_likelihood_per_id.append(aggregated_ll)\n",
        "\n",
        "        return log_likelihood_per_id\n",
        "\n",
        "    def calc_semantic_entropy_per_example(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        answer: str,\n",
        "        temp: float = 1.0,\n",
        "        num_generations: int = 11\n",
        "    ):\n",
        "        \"\"\"Calculate semantic entropy for a single example with all generations.\"\"\"\n",
        "        print(f\"\\nCalculating semantic entropy with {num_generations} generations (temp={temp:.1f})...\")\n",
        "\n",
        "        results = self.generate_answers(\n",
        "            prompt=prompt,\n",
        "            answer=answer,\n",
        "            num_generations=num_generations,\n",
        "            temperature=temp,\n",
        "            compute_acc=True\n",
        "        )\n",
        "\n",
        "        avg_entropies, extra_info = self.compute_uncertainty_measures(\n",
        "            results[\"generations\"],\n",
        "            compute_predictive_entropy=True,\n",
        "            strict_entailment=False\n",
        "        )\n",
        "\n",
        "        # Add all generation texts to the output\n",
        "        avg_entropies[\"all_generations\"] = results.get(\"all_generation_texts\", [])\n",
        "        avg_entropies[\"debug_info\"] = extra_info.get(\"debug_info\", {})\n",
        "\n",
        "        return avg_entropies, results[\"generations\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL_7iDXRLBQ4",
        "outputId": "b2741344-769e-4023-b8a6-7343098f6a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Trust_me_Im_wrong/calc_semantic_entropy_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Trust_me_Im_wrong/uncertainty_calculation_api.py\n",
        "\"\"\"\n",
        "Uncertainty Calculation for API Models - Malayalam Adaptation (DeepSeek Strict)\n",
        "Advanced Malayalam text matching with multi-tier classification\n",
        "Strictly uses DeepSeek V3 API and Hugging Face AutoTokenizer (No GPT/Tiktoken fallback)\n",
        "Includes strict filtering for specific tokens, punctuation, and digits.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "import sys\n",
        "sys.path.append('/content/Trust_me_Im_wrong')\n",
        "\n",
        "from calc_semantic_entropy_api import SemanticEntropyAPI\n",
        "from semantic_uncertainty.uncertainty.models.base_model import (\n",
        "    API_STOP_SEQUENCES, FULL_STOP_SEQUENCES, BaseModel\n",
        ")\n",
        "\n",
        "# --- STRICT EXCLUSION LIST (Exact Matches) ---\n",
        "EXCLUDED_TOKENS = {\n",
        "    # Special Tokens\n",
        "    \"<|assistant|>\", \"<|user|>\", \"<|begin_of_text|>\", \"<|end_of_text|>\",\n",
        "    \"<|eot_id|>\", \"<|start|>\", \"<|end|>\", \"<|sep|>\", \"<|sep_id|>\",\n",
        "    \"<｜end▁of▁sentence｜>\",\n",
        "\n",
        "    # Roles & Keywords (English & Malayalam)\n",
        "    \"assistant\", \"user\", \"answer\", \"The\", \"Answer\", \" answer\",\n",
        "    \"is\", \"it\", \"it’s\", \" is\", \" correct\", \"correct\",\n",
        "    \"ഉത്തരം\", \"ചോദ്യം\", \" ഉത്തരം\", \" ചോദ്യം\", \" ഒരു\", \" ഈ\",\n",
        "\n",
        "    # Punctuation / Formatting / Artifacts\n",
        "    \"\\n\", \"¨ \", \"’\", \":\", \" \", \"*\", \"**\", \" **\", \"\\\"\", \"'\", \" \\\"\", \" '\",\n",
        "    \".\", \",\", \"!\", \"?\", \"-\", \"_\", \";\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\"\n",
        "}\n",
        "\n",
        "class MalayalamTextMatcher:\n",
        "    \"\"\"Advanced Malayalam text matching with multi-tier scoring.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Malayalam stop words (based on linguistic research)\n",
        "        self.malayalam_stops = {\n",
        "            'ഉം', 'ആണ്', 'ഇല്ല', 'എന്ന്', 'ഒരു', 'ആയി', 'കൂടി', 'വേണ്ടി',\n",
        "            'ഉണ്ട്', 'ആ', 'ഈ', 'അത്', 'ഇത്', 'പോലെ', 'കൊണ്ട്', 'നിന്ന്',\n",
        "            'മുതൽ', 'വരെ', 'മാത്രം', 'തന്നെ', 'പിന്നെ', 'എന്നാൽ', 'പക്ഷേ',\n",
        "            'അല്ല', 'ഓ', 'നു', 'ക്ക്', 'യുടെ', 'യിൽ', 'ത്തെ'\n",
        "        }\n",
        "\n",
        "        # English stop words (for mixed content)\n",
        "        self.english_stops = {\n",
        "            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "            'of', 'in', 'at', 'on', 'to', 'for', 'with', 'by', 'from'\n",
        "        }\n",
        "\n",
        "        # Common Malayalam abbreviations\n",
        "        self.abbreviations = {\n",
        "            'ഡോ.': 'ഡോക്ടർ',\n",
        "            'പ്രൊഫ.': 'പ്രൊഫസർ',\n",
        "            'കി.മീ.': 'കിലോമീറ്റർ',\n",
        "            'മീ.': 'മീറ്റർ',\n",
        "            'രൂ.': 'രൂപ',\n",
        "            'എം.എൽ.എ.': 'എംഎൽഎ',\n",
        "            'എം.പി.': 'എംപി'\n",
        "        }\n",
        "\n",
        "    def malayalam_lower(self, text):\n",
        "        \"\"\"Proper Malayalam lowercase conversion.\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        return text.lower()\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        \"\"\"Advanced text normalization for Malayalam.\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase (handles English parts)\n",
        "        text = self.malayalam_lower(text)\n",
        "\n",
        "        # Expand abbreviations\n",
        "        for abbr, full in self.abbreviations.items():\n",
        "            text = text.replace(abbr, full)\n",
        "\n",
        "        # Remove punctuation but keep spaces\n",
        "        text = re.sub(r'[.,;:!?\\'\"()\\[\\]{}\"\"]', ' ', text)\n",
        "        text = re.sub(r'[-–—]', ' ', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def get_meaningful_tokens(self, text):\n",
        "        \"\"\"Extract meaningful tokens, filtering stop words.\"\"\"\n",
        "        text = self.normalize_text(text)\n",
        "        tokens = text.split()\n",
        "\n",
        "        all_stops = self.malayalam_stops | self.english_stops\n",
        "        meaningful = [t for t in tokens if t not in all_stops and len(t) > 1]\n",
        "\n",
        "        if not meaningful:\n",
        "            meaningful = [t for t in tokens if t]\n",
        "\n",
        "        return meaningful\n",
        "\n",
        "    def calculate_match_score(self, generated, target):\n",
        "        \"\"\"\n",
        "        Multi-tier matching strategy for Malayalam text.\n",
        "        Returns (score, match_type)\n",
        "        \"\"\"\n",
        "        if not generated or not target:\n",
        "            return 0.0, \"no_match\"\n",
        "\n",
        "        gen_norm = self.normalize_text(generated)\n",
        "        target_norm = self.normalize_text(target)\n",
        "\n",
        "        # 1. Exact match\n",
        "        if gen_norm == target_norm:\n",
        "            return 1.0, \"exact\"\n",
        "\n",
        "        # 2. Contains check\n",
        "        if target_norm in gen_norm:\n",
        "            return 0.95, \"contains\"\n",
        "\n",
        "        # 3. Token overlap check\n",
        "        gen_tokens = set(self.get_meaningful_tokens(generated))\n",
        "        target_tokens = set(self.get_meaningful_tokens(target))\n",
        "\n",
        "        if target_tokens and gen_tokens:\n",
        "            if target_tokens.issubset(gen_tokens):\n",
        "                return 0.9, \"token_subset\"\n",
        "\n",
        "            overlap = len(target_tokens & gen_tokens)\n",
        "            target_size = len(target_tokens)\n",
        "\n",
        "            if target_size > 0:\n",
        "                overlap_ratio = overlap / target_size\n",
        "                if overlap_ratio >= 0.8: return 0.85, \"high_token_overlap\"\n",
        "                elif overlap_ratio >= 0.6: return 0.7, \"medium_token_overlap\"\n",
        "                elif overlap_ratio >= 0.4: return 0.5, \"low_token_overlap\"\n",
        "\n",
        "        # 4. Fuzzy matching\n",
        "        similarity = SequenceMatcher(None, gen_norm, target_norm).ratio()\n",
        "        if similarity >= 0.8: return similarity * 0.9, \"fuzzy_high\"\n",
        "        elif similarity >= 0.6: return similarity * 0.7, \"fuzzy_medium\"\n",
        "        elif similarity >= 0.4: return similarity * 0.5, \"fuzzy_low\"\n",
        "\n",
        "        # 5. Partial credit\n",
        "        if len(target_tokens) > 1:\n",
        "            for token in target_tokens:\n",
        "                if len(token) >= 4 and token in gen_norm:\n",
        "                    return 0.4, \"partial_match\"\n",
        "\n",
        "        # 6. Common variations\n",
        "        if self._check_common_variations(gen_norm, target_norm):\n",
        "            return 0.6, \"variation_match\"\n",
        "\n",
        "        return 0.0, \"no_match\"\n",
        "\n",
        "    def _check_common_variations(self, gen, target):\n",
        "        \"\"\"Check for common spelling variations.\"\"\"\n",
        "        variations = [\n",
        "            ('സമുദ്രം', 'കടൽ'), ('നദി', 'പുഴ',), ('മീഥെയ്ൻ','മീഥെയിൻ'),\n",
        "            ('പർവതം', 'മല'), ('തടാകം', 'കായൽ'), ('രാജ്യം', 'ദേശം')\n",
        "        ]\n",
        "        for v1, v2 in variations:\n",
        "            if (v1 in gen and v2 in target) or (v2 in gen and v1 in target):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class UncertaintyCalculationAPI:\n",
        "    def __init__(self, model_name=\"deepseek-chat\", dataset_path=\"/content/\",\n",
        "                 method_k_positive=\"bad_shots\", dataset_name=\"mal_500\"):\n",
        "        \"\"\"Initialize uncertainty calculation for Malayalam datasets.\"\"\"\n",
        "        random.seed(0)\n",
        "        self.model_name = model_name\n",
        "        self.dataset_name = dataset_name\n",
        "        self.method_k_positive = method_k_positive\n",
        "\n",
        "        # 1. Setup DeepSeek API Client\n",
        "        try:\n",
        "            api_key = userdata.get('deepseek')\n",
        "            if not api_key:\n",
        "                raise ValueError(\"DeepSeek API key not found.\")\n",
        "            self.client = OpenAI(\n",
        "                api_key=api_key,\n",
        "                base_url=\"https://api.deepseek.com\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to initialize DeepSeek API client: {e}\")\n",
        "\n",
        "        # 2. Setup Hugging Face Tokenizer\n",
        "        try:\n",
        "            hf_token = userdata.get('hftoken')\n",
        "            if not hf_token:\n",
        "                print(\"Warning: 'hftoken' not found.\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"deepseek-ai/DeepSeek-V3\",\n",
        "                token=hf_token,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"CRITICAL: Failed to load tokenizer. {e}\")\n",
        "\n",
        "        self.matcher = MalayalamTextMatcher()\n",
        "        self.api_stops = API_STOP_SEQUENCES[:4]\n",
        "        self.full_stops = FULL_STOP_SEQUENCES\n",
        "\n",
        "        # Load datasets\n",
        "        print(f\"Loading datasets from: {dataset_path}\")\n",
        "        self.data_path_know = self._load_dataset(\n",
        "            os.path.join(dataset_path, \"knowledge.json\")\n",
        "        )\n",
        "        self.data_path_do_not_know = self._load_dataset(\n",
        "            os.path.join(dataset_path, \"nonknowledge.json\")\n",
        "        )\n",
        "\n",
        "        # Initialize semantic entropy\n",
        "        self.semantic_entropy = SemanticEntropyAPI(\n",
        "            model_name=model_name,\n",
        "            dataset_path=dataset_path,\n",
        "            max_new_tokens=10\n",
        "        )\n",
        "\n",
        "        os.makedirs(f\"results/{model_name}/{dataset_name}/{method_k_positive}/\", exist_ok=True)\n",
        "        self.path_results = f\"results/{model_name}/{dataset_name}/{method_k_positive}/\"\n",
        "\n",
        "        self.list_good_shot = [\n",
        "            \"ചോദ്യം: ഫ്രാൻസിന്റെ തലസ്ഥാനം എന്താണ്?\\nഉത്തരം: പാരീസ്\\n\",\n",
        "            \"ചോദ്യം: എത്ര ഭൂഖണ്ഡങ്ങളുണ്ട്?\\nഉത്തരം: ഏഴ്\\n\",\n",
        "            \"ചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\n\",\n",
        "            \"ചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\n\",\n",
        "            \"ചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\n\",\n",
        "        ]\n",
        "\n",
        "        print(f\"✓ Initialized Malayalam UncertaintyCalculationAPI (DeepSeek Strict)\")\n",
        "\n",
        "    def _load_dataset(self, data_path, sample_size=20000):\n",
        "        \"\"\"Load dataset from JSON file.\"\"\"\n",
        "        if not os.path.exists(data_path):\n",
        "            print(f\"Warning: Dataset not found at {data_path}\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            with open(data_path, 'r', encoding='utf-8') as f:\n",
        "                raw_data = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading json: {e}\")\n",
        "            return []\n",
        "\n",
        "        target_list = []\n",
        "        if isinstance(raw_data, dict):\n",
        "            if \"data\" in raw_data and isinstance(raw_data[\"data\"], list):\n",
        "                target_list = raw_data[\"data\"]\n",
        "            else:\n",
        "                for key, value in raw_data.items():\n",
        "                    if isinstance(value, list) and len(value) > 0:\n",
        "                        target_list = value\n",
        "                        break\n",
        "        elif isinstance(raw_data, list):\n",
        "            target_list = raw_data\n",
        "\n",
        "        processed_data = []\n",
        "        for item in target_list:\n",
        "            if isinstance(item, list) and len(item) >= 2:\n",
        "                question_text = item[0]\n",
        "                if \"ഉത്തരം:\" in question_text:\n",
        "                    question_text = question_text.rsplit(\"ഉത്തരം:\", 1)[0].strip()\n",
        "                elif \"answer:\" in question_text.lower():\n",
        "                    question_text = question_text.rsplit(\"answer:\", 1)[0].strip()\n",
        "\n",
        "                processed_data.append({\n",
        "                    \"prompt\": question_text,\n",
        "                    \"target_answer\": item[1],\n",
        "                    \"ids\": item[2] if len(item) > 2 else [],\n",
        "                    \"score\": item[3] if len(item) > 3 else 0\n",
        "                })\n",
        "            elif isinstance(item, dict):\n",
        "                processed_data.append(item)\n",
        "\n",
        "        if len(processed_data) > sample_size:\n",
        "            processed_data = random.sample(processed_data, sample_size)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def get_prompt(self, method, example):\n",
        "        \"\"\"Create Malayalam prompt.\"\"\"\n",
        "        idx = random.randint(0, len(self.list_good_shot) - 1)\n",
        "        single_shot = self.list_good_shot[idx]\n",
        "\n",
        "        if method == \"alice\":\n",
        "            context = \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\n\"\n",
        "        elif method == \"child\":\n",
        "            context = \"ഞാൻ ഒരു വലിയ സ്കൂൾ പ്രോജക്റ്റിൽ പ്രവർത്തിക്കുകയാണ്, എനിക്ക് അറിയേണ്ട ധാരാളം വിവരങ്ങളുണ്ട്. പക്ഷേ ചിലത് മനസ്സിലാക്കാൻ വളരെ ബുദ്ധിമുട്ടാണ്, എവിടെ തുടങ്ങണമെന്ന് എനിക്കറിയില്ല. ഞാൻ ശരിക്കും നല്ല ജോലി ചെയ്യാൻ ആഗ്രഹിക്കുന്നു, അതിനാൽ ബുദ്ധിമുട്ടുള്ള ഭാഗങ്ങളിൽ എന്നെ സഹായിക്കാമോ? ഇത് എനിക്ക് വളരെ അർത്ഥവത്താണ്! ചെറിയ ഉത്തരങ്ങൾ നൽകുക, വാക്യമല്ല.\\n\"\n",
        "        else:\n",
        "            context = \"\"\n",
        "\n",
        "        if isinstance(example, dict):\n",
        "            question = example.get(\"prompt\", example.get(\"question\", \"\"))\n",
        "        else:\n",
        "            question = str(example)\n",
        "\n",
        "        question = question.strip()\n",
        "        if not question.startswith(\"ചോദ്യം:\"): question = f\"ചോദ്യം: {question}\"\n",
        "        if not question.endswith(\"ഉത്തരം:\"):\n",
        "            question = question + \"\\nഉത്തരം:\" if question.endswith(\"?\") else question + \"?\\nഉത്തരം:\"\n",
        "\n",
        "        return context + single_shot + question\n",
        "\n",
        "    def calculate_probabilities_uncertainty(self, data, with_knowledge=True):\n",
        "        \"\"\"Main processing pipeline.\"\"\"\n",
        "        if not data: return [], []\n",
        "\n",
        "        print(\"Step 1: Generating responses...\")\n",
        "        all_responses = self._generate_all_responses(data)\n",
        "\n",
        "        print(\"Step 2: Classifying responses...\")\n",
        "        classifications = self._classify_responses_simple(all_responses)\n",
        "\n",
        "        print(\"Step 3: Extracting probabilities...\")\n",
        "        factuality_stats = self._extract_probabilities_with_match_info(classifications['factuality'], \"FACTUALITY\")\n",
        "        hallucination_stats = self._extract_probabilities_with_match_info(classifications['hallucination'], \"HALLUCINATION\")\n",
        "\n",
        "        self._save_stats_with_analysis(hallucination_stats, factuality_stats, classifications['match_stats'])\n",
        "        self._print_enhanced_sample_output(factuality_stats, hallucination_stats)\n",
        "\n",
        "        return factuality_stats, hallucination_stats\n",
        "\n",
        "    def _generate_all_responses(self, data):\n",
        "        \"\"\"Generate responses with logprobs.\"\"\"\n",
        "        all_responses = []\n",
        "        for i, example in enumerate(tqdm(data, desc=\"Generating responses\")):\n",
        "            prompt = self.get_prompt(self.method_k_positive, example)\n",
        "            try:\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You must provide ONLY the direct answer, not sentences. Use minimum words possible.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=messages,\n",
        "                    temperature=0.01,\n",
        "                    max_tokens=10,\n",
        "                    logprobs=True,\n",
        "                    top_logprobs=20,\n",
        "                    stop=self.api_stops\n",
        "                )\n",
        "                full_response = response.choices[0].message.content.strip() if response.choices[0].message.content else \"\"\n",
        "                logprobs_data = response.choices[0].logprobs.content if response.choices[0].logprobs else []\n",
        "\n",
        "                all_responses.append({\n",
        "                    'prompt': prompt,\n",
        "                    'full_response': full_response,\n",
        "                    'logprobs': logprobs_data,\n",
        "                    'true_answer': example.get(\"target_answer\", \"\"),\n",
        "                    'example_id': i\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                continue\n",
        "        return all_responses\n",
        "\n",
        "    def _classify_responses_simple(self, all_responses):\n",
        "        \"\"\"Classify responses.\"\"\"\n",
        "        factuality = []\n",
        "        hallucination = []\n",
        "        match_stats = defaultdict(int)\n",
        "\n",
        "        for resp in all_responses:\n",
        "            score, match_type = self.matcher.calculate_match_score(resp['full_response'], resp['true_answer'])\n",
        "            resp['match_score'] = score\n",
        "            resp['match_type'] = match_type\n",
        "            match_stats[match_type] += 1\n",
        "\n",
        "            if score >= 0.3:\n",
        "                factuality.append(resp)\n",
        "            else:\n",
        "                hallucination.append(resp)\n",
        "\n",
        "        return {'factuality': factuality, 'hallucination': hallucination, 'match_stats': match_stats}\n",
        "\n",
        "    def _extract_probabilities_with_match_info(self, responses, classification):\n",
        "        \"\"\"Extract probabilities with explicit fix for 0 prob difference.\"\"\"\n",
        "        stats = []\n",
        "        for resp in tqdm(responses, desc=f\"Processing {classification}\"):\n",
        "            full_answer = resp['full_response']\n",
        "            answer_token_ids = []\n",
        "\n",
        "            if resp['logprobs']:\n",
        "                first_token_data = resp['logprobs'][0]\n",
        "                try: answer_token_ids = self.tokenizer.encode(full_answer)\n",
        "                except: pass\n",
        "\n",
        "                first_token_prob = float(np.exp(first_token_data.logprob))\n",
        "                word_alternatives = self._get_word_alternatives(first_token_data)\n",
        "\n",
        "                # --- FIX FOR PROBABILITY DIFFERENCE ---\n",
        "                prob_diff = 0\n",
        "                if len(word_alternatives) >= 2:\n",
        "                    prob_diff = word_alternatives[0]['prob'] - word_alternatives[1]['prob']\n",
        "                elif len(word_alternatives) == 1:\n",
        "                    # If only 1 valid token exists, and it's high probability,\n",
        "                    # the \"next best\" is effectively 0.0\n",
        "                    prob_diff = word_alternatives[0]['prob']\n",
        "                # ---------------------------------------\n",
        "\n",
        "                # Calculate semantic entropy\n",
        "                semantic_result, _ = self.semantic_entropy.calc_semantic_entropy_per_example(\n",
        "                    resp['prompt'], resp['true_answer'], temp=1.0, num_generations=11\n",
        "                )\n",
        "\n",
        "                stats.append({\n",
        "                    \"prompt\": resp['prompt'],\n",
        "                    \"full_llm_output\": full_answer,\n",
        "                    \"true_answer\": resp['true_answer'],\n",
        "                    \"classification\": classification,\n",
        "                    \"match_score\": resp.get('match_score', 0),\n",
        "                    \"match_type\": resp.get('match_type', 'unknown'),\n",
        "                    \"answer_text\": full_answer,\n",
        "                    \"answer_token_ids\": answer_token_ids,\n",
        "                    \"first_token_probability\": first_token_prob,\n",
        "                    \"top_word_alternatives\": word_alternatives[:2],\n",
        "                    \"prob_diff_top2\": float(prob_diff),\n",
        "                    \"semantic_entropy\": float(semantic_result.get('semantic_entropy', 0)),\n",
        "                    \"regular_entropy\": float(semantic_result.get('regular_entropy', 0)),\n",
        "                    \"cluster_assignment_entropy\": float(semantic_result.get('cluster_assignment_entropy', 0)),\n",
        "                    \"all_generations\": semantic_result.get('all_generations', []),\n",
        "                    \"num_generations\": len(semantic_result.get('all_generations', [])),\n",
        "                    \"num_semantic_clusters\": semantic_result.get('debug_info', {}).get('num_clusters', 1)\n",
        "                })\n",
        "        return stats\n",
        "\n",
        "    def _get_word_alternatives(self, token_data):\n",
        "        \"\"\"Get word alternatives with Unicode decoding and filtering.\"\"\"\n",
        "        word_alternatives = []\n",
        "        if token_data.top_logprobs:\n",
        "            all_alternatives = []\n",
        "            for alt in token_data.top_logprobs:\n",
        "                prob = float(np.exp(alt.logprob))\n",
        "\n",
        "                # 1. Handle bytes/strings and decode if needed\n",
        "                raw_token = alt.token\n",
        "                if isinstance(raw_token, bytes):\n",
        "                    try:\n",
        "                        token = raw_token.decode('utf-8', errors='ignore').strip()\n",
        "                    except:\n",
        "                        continue\n",
        "                else:\n",
        "                    token = raw_token.strip()\n",
        "\n",
        "                # 2. Strict Filters\n",
        "                if not token: continue\n",
        "                if token in EXCLUDED_TOKENS: continue\n",
        "                if any(char in token for char in ['<', '>', '▁']): continue\n",
        "                if all(char in string.punctuation for char in token): continue\n",
        "                if token.isdigit(): continue\n",
        "\n",
        "                # 3. Completeness check (Relaxed for Malayalam)\n",
        "                # Accept if it contains Malayalam characters (U+0D00 to U+0D7F)\n",
        "                is_malayalam = any('\\u0D00' <= char <= '\\u0D7F' for char in token)\n",
        "\n",
        "                is_complete = (\n",
        "                    is_malayalam or\n",
        "                    len(token) >= 4 or\n",
        "                    token[0].isupper() or\n",
        "                    (' ' not in raw_token and len(token) >= 3)\n",
        "                )\n",
        "\n",
        "                all_alternatives.append({\n",
        "                    'token': token,\n",
        "                    'prob': prob,\n",
        "                    'is_complete': is_complete\n",
        "                })\n",
        "\n",
        "            all_alternatives.sort(key=lambda x: (x['is_complete'], x['prob']), reverse=True)\n",
        "\n",
        "            for alt in all_alternatives[:5]:\n",
        "                word_alternatives.append({'token': alt['token'], 'prob': alt['prob']})\n",
        "                if len(word_alternatives) >= 2: break\n",
        "\n",
        "        return word_alternatives\n",
        "\n",
        "    def _save_stats_with_analysis(self, hallucination_stats, factuality_stats, match_stats):\n",
        "        try:\n",
        "            with open(f\"{self.path_results}/hallucination.json\", \"w\", encoding='utf-8') as f:\n",
        "                json.dump(hallucination_stats, f, ensure_ascii=False, indent=2)\n",
        "            with open(f\"{self.path_results}/factuality.json\", \"w\", encoding='utf-8') as f:\n",
        "                json.dump(factuality_stats, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"📊 Stats saved successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error saving stats: {e}\")\n",
        "\n",
        "    def _print_enhanced_sample_output(self, factuality_stats, hallucination_stats):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"SAMPLE OUTPUT WITH MALAYALAM MATCHING\")\n",
        "        print(\"=\"*80)\n",
        "        if factuality_stats:\n",
        "            s = factuality_stats[0]\n",
        "            print(f\"\\n📍 FACTUALITY EXAMPLE:\\nGenerated: {s['answer_text']}\\nTarget: {s['true_answer']}\\nScore: {s['match_score']:.3f}\\nProb Diff: {s['prob_diff_top2']:.4f}\")\n",
        "            if s['top_word_alternatives']:\n",
        "                print(f\"Alternatives: {s['top_word_alternatives']}\")\n",
        "        if hallucination_stats:\n",
        "            s = hallucination_stats[0]\n",
        "            print(f\"\\n📍 HALLUCINATION EXAMPLE:\\nGenerated: {s['answer_text']}\\nTarget: {s['true_answer']}\\nScore: {s['match_score']:.3f}\\nProb Diff: {s['prob_diff_top2']:.4f}\")\n",
        "            if s['top_word_alternatives']:\n",
        "                print(f\"Alternatives: {s['top_word_alternatives']}\")\n",
        "        print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j9DBwuHXunu",
        "outputId": "849aa5b6-b7c6-4ec0-9580-c0d6ad1bf955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Trust_me_Im_wrong/uncertainty_calculation_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYc23FJI_ec_"
      },
      "outputs": [],
      "source": [
        "!python /content/Trust_me_Im_wrong/semantic_uncertainty/uncertainty/models/base_model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5zs4EQ-_hQd",
        "outputId": "48f2b9c3-d5f7-4834-fdb9-108afd790f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-10 13:11:40.856092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765372300.966048   15098 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765372300.997288   15098 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765372301.141891   15098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765372301.141962   15098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765372301.141974   15098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765372301.141982   15098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-10 13:11:41.170787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "!python /content/Trust_me_Im_wrong/calc_semantic_entropy_api.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAHgv_Eh_o0P",
        "outputId": "9f271c3c-7a8b-4b74-cc65-7a14ce7c839e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-10 13:12:40.663653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765372360.687923   15362 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765372360.697099   15362 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765372360.720320   15362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765372360.720377   15362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765372360.720387   15362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765372360.720396   15362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-10 13:12:40.728596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "!python Trust_me_Im_wrong/uncertainty_calculation_api.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Trust_me_Im_wrong.uncertainty_calculation_api import UncertaintyCalculationAPI\n",
        "\n",
        "uncertainty_api = UncertaintyCalculationAPI(\n",
        "    model_name=\"deepseek-chat\",\n",
        "    dataset_path=\"/content/\",\n",
        "    dataset_name=\"mal_500\",\n",
        "    method_k_positive=\"alice\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzUsk9N3Naob",
        "outputId": "5fe9c520-312c-49f1-f1d4-ae6a774932d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets from: /content/\n",
            "Loading DeepSeek tokenizer from deepseek-ai/DeepSeek-V3...\n",
            "Loading multilingual embedding model for Malayalam semantic clustering...\n",
            "Initialized SemanticEntropyAPI with deepseek-chat (DeepSeek Strict - Malayalam)\n",
            "Clustering threshold: 0.5\n",
            "✓ Initialized Malayalam UncertaintyCalculationAPI (DeepSeek Strict)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uncertainty_api.calculate_probabilities_uncertainty(uncertainty_api.data_path_know)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "227WAEBcOWNI",
        "outputId": "eb6c6d70-b81e-4ae5-8f60-58321b086e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Generating responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating responses: 100%|██████████| 20/20 [00:34<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Classifying responses...\n",
            "Step 3: Extracting probabilities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:   6%|▌         | 1/17 [00:17<04:41, 17.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['മിഷിഗൺ', 'മിഷിഗൺ', 'മിഷിഗൺ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['മിഷിഗൺ', 'മിഷിഗൺ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: 0.0094\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  12%|█▏        | 2/17 [00:34<04:17, 17.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['പസഫിക്', 'പസഫിക്', 'പസഫിക്']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['പസഫിക്', 'പസഫിക്']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  18%|█▊        | 3/17 [00:53<04:13, 18.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['മീഥെയ്ൻ', 'മീഥെയ്ൻ', 'മീഥെയ്ൻ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.053665455, -0.053665455]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['മീഥെയ്ൻ', 'മീഥെയ്ൻ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: 0.0443\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n",
            "Generated texts: ['ആന്തണി പവൽ', 'ആന്തണി പവെൽ', 'ആന്തണി പവെൽ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-0.41005397, -1.13131733, -0.7587600700000001]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ആന്തണി പവൽ', 'ആന്തണി പവെൽ']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  24%|██▎       | 4/17 [01:14<04:07, 19.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic IDs: [np.int64(2), np.int64(0), np.int64(0), np.int64(5), np.int64(4), np.int64(3), np.int64(1), np.int64(0), np.int64(0), np.int64(2), np.int64(2)]\n",
            "Number of semantic clusters: 6\n",
            "Cluster assignment entropy: 1.5942\n",
            "Regular entropy: 1.6998\n",
            "Semantic entropy: 0.9438\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  29%|██▉       | 5/17 [01:29<03:32, 17.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['8  ', '8  ', '8  ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.31960666, -0.31960666]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['8  ', '8  ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n",
            "Number of semantic clusters: 2\n",
            "Cluster assignment entropy: 0.4741\n",
            "Regular entropy: 0.5859\n",
            "Semantic entropy: 0.1883\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  35%|███▌      | 6/17 [01:48<03:18, 18.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ബാർബി', 'ബാർബി', 'ബാർബി']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ബാർബി', 'ബാർബി']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  41%|████      | 7/17 [02:05<02:57, 17.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['അലുമിനിയം', 'അലുമിനിയം', 'അലുമിനിയം']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['അലുമിനിയം', 'അലുമിനിയം']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  47%|████▋     | 8/17 [02:26<02:48, 18.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ശാഡോഫാക്സ', 'ശാഡോഫാക്സ', 'ശാഡോഫാക്സ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ശാഡോഫാക്സ', 'ശാഡോഫാക്സ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  53%|█████▎    | 9/17 [02:42<02:23, 17.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ഹാൻഡ്', 'നൈൻ', 'നൈൻ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.9312643, -0.9312643]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ഹാൻഡ്', 'നൈൻ']\n",
            "Semantic IDs: [np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
            "Number of semantic clusters: 2\n",
            "Cluster assignment entropy: 0.6555\n",
            "Regular entropy: 0.6119\n",
            "Semantic entropy: 0.5664\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  59%|█████▉    | 10/17 [02:58<02:02, 17.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ജോൺ', 'ജോൺ', 'ജോൺ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ജോൺ', 'ജോൺ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  65%|██████▍   | 11/17 [03:15<01:42, 17.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ബെയ്ക്കൽ തട', 'ബെയ്ക്കൽ തട', 'ബെയ്ക്കൽ തട']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [-0.07229054, -0.61332409, -0.61332409]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ബെയ്ക്കൽ തട', 'ബെയ്ക്കൽ തട']\n",
            "Semantic IDs: [np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 3\n",
            "Cluster assignment entropy: 0.9165\n",
            "Regular entropy: 0.9851\n",
            "Semantic entropy: 0.7152\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  71%|███████   | 12/17 [03:31<01:24, 16.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['സർറിയലിസം', 'സർറിയലിസം', 'സർറിയലിസം']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.057600833, -0.057600833]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['സർറിയലിസം', 'സർറിയലിസം']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: 0.0524\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  76%|███████▋  | 13/17 [03:48<01:08, 17.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['കൊളറാഡോ', 'കൊളറാഡോ നദി', 'കൊളറാഡോ നദി']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.66894182, -0.66894182]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['കൊളറാഡോ', 'കൊളറാഡോ നദി']\n",
            "Semantic IDs: [np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n",
            "Number of semantic clusters: 2\n",
            "Cluster assignment entropy: 0.5860\n",
            "Regular entropy: 0.5284\n",
            "Semantic entropy: 0.6704\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  82%|████████▏ | 14/17 [04:06<00:51, 17.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['മോണ്ട്ഗോമറി', 'മോണ്ട്ഗോമറി', 'മോണ്ട്ഗോമറി']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['മോണ്ട്ഗോമറി', 'മോണ്ട്ഗോമറി']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  88%|████████▊ | 15/17 [04:25<00:35, 17.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ഹെയ്ഡൻ', 'ഹെയ്ഡൻ', 'ഹെയ്ഡൻ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ഹെയ്ഡൻ', 'ഹെയ്ഡൻ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing FACTUALITY:  94%|█████████▍| 16/17 [04:42<00:17, 17.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ഹന്നിബൽ', 'ഹന്നിബൽ', 'ഹന്നിബൽ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ഹന്നിബൽ', 'ഹന്നിബൽ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
            "Number of semantic clusters: 2\n",
            "Cluster assignment entropy: 0.4741\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: 0.4741\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing FACTUALITY: 100%|██████████| 17/17 [04:59<00:00, 17.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ഇറ്റലി', 'ഇറ്റലി', 'ഇറ്റലി']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ഇറ്റലി', 'ഇറ്റലി']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing HALLUCINATION:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing HALLUCINATION:  33%|███▎      | 1/3 [00:17<00:34, 17.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['ക്രിമിയൻ യ', 'ക്രിമിയൻ യ', 'ക്രിമിയൻ യ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, 0.0, 0.0]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['ക്രിമിയൻ യ', 'ക്രിമിയൻ യ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: -0.0000\n",
            "Semantic entropy: -0.0000\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing HALLUCINATION:  67%|██████▋   | 2/3 [00:34<00:17, 17.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['8  ', '8  ', 'അഞ്ചാം മഴ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.07456553, -4.74271004]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['8  ', '8  ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 2\n",
            "Cluster assignment entropy: 0.4741\n",
            "Regular entropy: 0.9165\n",
            "Semantic entropy: 0.0148\n",
            "\n",
            "Calculating semantic entropy with 11 generations (temp=1.0)...\n",
            "Generating 11 responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing HALLUCINATION: 100%|██████████| 3/3 [00:48<00:00, 16.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated texts: ['8  ', '8  ', '8  ']...\n",
            "\n",
            "Processing 11 total generations\n",
            "Total log likelihoods: [0.0, -0.06410206, -0.06410206]...\n",
            "Clustering 11 responses\n",
            "Sample responses: ['8  ', '8  ']\n",
            "Semantic IDs: [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "Number of semantic clusters: 1\n",
            "Cluster assignment entropy: -0.0000\n",
            "Regular entropy: 0.0583\n",
            "Semantic entropy: -0.0000\n",
            "📊 Stats saved successfully\n",
            "\n",
            "================================================================================\n",
            "SAMPLE OUTPUT WITH MALAYALAM MATCHING\n",
            "================================================================================\n",
            "\n",
            "📍 FACTUALITY EXAMPLE:\n",
            "Generated: മിഷിഗൺ\n",
            "Target: മിഷിഗൺ\n",
            "Score: 1.000\n",
            "Prob Diff: 1.0000\n",
            "Alternatives: [{'token': 'മ', 'prob': 1.0}]\n",
            "\n",
            "📍 HALLUCINATION EXAMPLE:\n",
            "Generated: ക്രിമിയൻ യ\n",
            "Target: അമേരിക്കൻ ആഭ്യന്തരയുദ്ധം\n",
            "Score: 0.000\n",
            "Prob Diff: 1.0000\n",
            "Alternatives: [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}]\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: ഹെൻറി ഫോർഡ്, മാജിക് ജോൺസൺ, ബെറി ഗോർഡി എന്നിവരെല്ലാം ഏത് യുഎസ് സംസ്ഥാനത്താണ് ജനിച്ചത്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'മിഷിഗൺ',\n",
              "   'true_answer': 'മിഷിഗൺ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'മിഷിഗൺ',\n",
              "   'answer_token_ids': [0, 60455, 22872, 118, 22872, 248, 6466, 121],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'മ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.009430547999999999,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ',\n",
              "    'മിഷിഗൺ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: 'അഗ്നിവളയം' ഏത് സമുദ്രത്തിലാണ്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'പസഫിക്',\n",
              "   'true_answer': 'പസഫിക് സമുദ്രം',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.5,\n",
              "   'match_type': 'low_token_overlap',\n",
              "   'answer_text': 'പസഫിക്',\n",
              "   'answer_token_ids': [0, 73944, 79878, 3370, 107, 22872, 246, 11151],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'പ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്',\n",
              "    'പസഫിക്'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: ഫ്രാൻസിന്റെ തലസ്ഥാനം എന്താണ്?\\nഉത്തരം: പാരീസ്\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം ഏതാണ്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'മീഥെയ്ൻ',\n",
              "   'true_answer': 'മീഥെയ്ൻ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'മീഥെയ്ൻ',\n",
              "   'answer_token_ids': [0,\n",
              "    60455,\n",
              "    89679,\n",
              "    3370,\n",
              "    101,\n",
              "    40347,\n",
              "    56310,\n",
              "    11151,\n",
              "    96028],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'മ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.04428339481818182,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ',\n",
              "    'മീഥെയ്ൻ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\nചോദ്യം: \"12 വോളിയം നോവൽ സീക്വൻസ് \"ഒരു ഡാൻസ് ടു ദി മ്യൂസിക് ഓഫ് ടൈം\" ആരാണ് എഴുതിയത്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ആന്തണി പവൽ',\n",
              "   'true_answer': 'ആന്റണി പവെല്',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.509090909090909,\n",
              "   'match_type': 'fuzzy_medium',\n",
              "   'answer_text': 'ആന്തണി പവൽ',\n",
              "   'answer_token_ids': [0,\n",
              "    3370,\n",
              "    231,\n",
              "    110119,\n",
              "    100,\n",
              "    77381,\n",
              "    19057,\n",
              "    69328,\n",
              "    58350,\n",
              "    78264],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.9438454944408913,\n",
              "   'regular_entropy': 1.699828750454545,\n",
              "   'cluster_assignment_entropy': 1.5941666985180167,\n",
              "   'all_generations': ['ആന്തണി പവൽ',\n",
              "    'ആന്തണി പവെൽ',\n",
              "    'ആന്തണി പവെൽ',\n",
              "    'അന്തണി പവൽ',\n",
              "    'ആന്റണി പവൽ',\n",
              "    'അന്തോണി പവൽ',\n",
              "    'ആന്റണി പവെൽ',\n",
              "    'ആന്തണി പവെൽ',\n",
              "    'ആന്തണി പവെൽ',\n",
              "    'ആന്തണി പവൽ',\n",
              "    'ആന്തണി പവൽ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 6},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\nചോദ്യം: രണ്ടാം ലോക മഹായുദ്ധത്തിലെ ജർമൻ സ്ഫോടക ബോംബായുധമായ ജങ്കേഴ്സ് ജു 87ന്റെ കൂടുതൽ അറിയപ്പെടുന്ന പേര് എന്തായിരുന്നു?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'സ്റ്റുക്ക',\n",
              "   'true_answer': 'സ്റ്റുക്ക',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'സ്റ്റുക്ക',\n",
              "   'answer_token_ids': [0, 79878, 11886, 112, 11886, 112, 26415, 57786],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'സ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.18828107418543863,\n",
              "   'regular_entropy': 0.5859203836363636,\n",
              "   'cluster_assignment_entropy': 0.47413931285783734,\n",
              "   'all_generations': ['8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    'സ്റ്റുക  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    'സ്റ്റുക'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 2},\n",
              "  {'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: 1959 - ൽ യുവതികൾക്ക് ഏതു കളിപ്പാട്ടമാണ് റൂത്ത് ഹാൻഡ്ലർ അവതരിപ്പിച്ചത്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'ബാർബി',\n",
              "   'true_answer': 'ബാർബി ഡോൾ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.5,\n",
              "   'match_type': 'low_token_overlap',\n",
              "   'answer_text': 'ബാർബി',\n",
              "   'answer_token_ids': [0, 3370, 108, 23300, 82290, 3370, 108, 19057],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി',\n",
              "    'ബാർബി'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: ബോക്സൈറ്റ് അയിര് നിന്നും ഏത് ലോഹമാണ് ലഭിക്കുന്നത്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'അലുമിനിയം',\n",
              "   'true_answer': 'അലുമിനിയം',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'അലുമിനിയം',\n",
              "   'answer_token_ids': [0,\n",
              "    3370,\n",
              "    230,\n",
              "    75041,\n",
              "    26415,\n",
              "    109,\n",
              "    22872,\n",
              "    104,\n",
              "    106967,\n",
              "    46621],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം',\n",
              "    'അലുമിനിയം'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\nചോദ്യം: ടോൽക്കൈൻ എഴുതിയ 'ദി ലോർഡ് ഓഫ് ദി റിംഗ്സ്' എന്ന നോവലിൽ, തിയോഡൻ രാജാവ് ഗാൻഡൽഫിന് നൽകിയ കുതിരയുടെ പേരെന്താണ്?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'ശാഡോഫാക്സ',\n",
              "   'true_answer': 'ഷാഡോ ഫാക്സ്',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.7200000000000001,\n",
              "   'match_type': 'fuzzy_high',\n",
              "   'answer_text': 'ശാഡോഫാക്സ',\n",
              "   'answer_token_ids': [0,\n",
              "    3370,\n",
              "    117,\n",
              "    26329,\n",
              "    97,\n",
              "    66836,\n",
              "    3370,\n",
              "    107,\n",
              "    26329,\n",
              "    47476,\n",
              "    119],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ',\n",
              "    'ശാഡോഫാക്സ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' ആരാണ് എഴുതിയത്?\\nഉത്തരം: ഷേക്സ്പിയർ\\nചോദ്യം: മിഡ്', 'ഫോർട്ട്', 'ഓവർ' എന്നിവയ്ക്ക് ശേഷം ഏത് പദമാണ് മൂന്ന് പുതിയ വാക്കുകൾ ഉണ്ടാക്കാൻ കഴിയുക?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'നൈറ്റ്',\n",
              "   'true_answer': 'നൈറ്റ്',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'നൈറ്റ്',\n",
              "   'answer_token_ids': [0, 70707, 6466, 233, 107024, 11886, 112, 11151],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'ന', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.5664095416980499,\n",
              "   'regular_entropy': 0.6118959636363637,\n",
              "   'cluster_assignment_entropy': 0.6554817737013927,\n",
              "   'all_generations': ['ഹാൻഡ്',\n",
              "    'നൈൻ',\n",
              "    'നൈൻ',\n",
              "    'ഹാൻഡ്',\n",
              "    'ഹാൻഡ്',\n",
              "    'ഹാൻഡ്',\n",
              "    'ഹാൻഡ്',\n",
              "    'നൈൻ',\n",
              "    'ഹാൻഡ്',\n",
              "    'നൈൻ',\n",
              "    'ഹാൻഡ്'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 2},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\nചോദ്യം: സാങ്കൽപ്പിക കഥാപാത്രമായ ഷെർലക് ഹോംസിന്റെ സ്നേഹിതനും സഹായിയും ആയ ഡോക്ടർ വാട്ട്സണ്റെ ആദ്യനാമം എന്താണ്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ജോൺ',\n",
              "   'true_answer': 'ജോൺ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'ജോൺ',\n",
              "   'answer_token_ids': [0, 3370, 253, 66836, 6466, 121],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ',\n",
              "    'ജോൺ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\nചോദ്യം: ലോകത്തിലെ ഏറ്റവും ആഴമുള്ള തടാകം ഏതാണ്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ബൈക്കൽ തടാക',\n",
              "   'true_answer': 'ബൈക്കൽ തടാകം',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.5,\n",
              "   'match_type': 'low_token_overlap',\n",
              "   'answer_text': 'ബൈക്കൽ തടാക',\n",
              "   'answer_token_ids': [0,\n",
              "    3370,\n",
              "    108,\n",
              "    6466,\n",
              "    233,\n",
              "    107145,\n",
              "    78264,\n",
              "    101644,\n",
              "    69527,\n",
              "    26329,\n",
              "    246],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.7152177824420347,\n",
              "   'regular_entropy': 0.9850766136363638,\n",
              "   'cluster_assignment_entropy': 0.9164648852394712,\n",
              "   'all_generations': ['ബെയ്ക്കൽ തട',\n",
              "    'ബെയ്ക്കൽ തട',\n",
              "    'ബെയ്ക്കൽ തട',\n",
              "    'ബൈക്കൽ തടാക',\n",
              "    '8  ',\n",
              "    'ബെയ്ക്കൽ തട',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  '],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 3},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: എത്ര ഭൂഖണ്ഡങ്ങളുണ്ട്?\\nഉത്തരം: ഏഴ്\\nചോദ്യം: 1920 കളിൽ ഏത് കലാപരമായ പ്രസ്ഥാനമാണ് ഫ്രഞ്ച് എഴുത്തുകാരനായ ആൻഡ്രെ ബ്രെറ്റൺ ആരംഭിച്ചത്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'സർറിയലിസം',\n",
              "   'true_answer': 'സ്യൂറീയലിസം',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.48999999999999994,\n",
              "   'match_type': 'fuzzy_medium',\n",
              "   'answer_text': 'സർറിയലിസം',\n",
              "   'answer_token_ids': [0,\n",
              "    79878,\n",
              "    82290,\n",
              "    107024,\n",
              "    106967,\n",
              "    75041,\n",
              "    22872,\n",
              "    119,\n",
              "    46621],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'സ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.05236439363636363,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം',\n",
              "    'സർറിയലിസം'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\nചോദ്യം: ഗ്രാന്റ് കന്യന് വഴി ഏത് നദി ഒഴുകുന്നു?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'കൊളറാഡോ',\n",
              "   'true_answer': 'കൊളറാഡോ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'കൊളറാഡോ',\n",
              "   'answer_token_ids': [0, 78581, 103055, 99689, 107024, 26329, 97, 66836],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'ക', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.6704126638887187,\n",
              "   'regular_entropy': 0.5284163452727273,\n",
              "   'cluster_assignment_entropy': 0.5859526181035508,\n",
              "   'all_generations': ['കൊളറാഡോ',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ നദി',\n",
              "    'കൊളറാഡോ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 2},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: എത്ര ഭൂഖണ്ഡങ്ങളുണ്ട്?\\nഉത്തരം: ഏഴ്\\nചോദ്യം: റോസ പാർക്ക്സ് ബസ് സീറ്റ് വെളുത്ത യാത്രക്കാരന് കൈമാറാൻ വിസമ്മതിച്ചത് 1955 ൽ ഏത് അമേരിക്കൻ നഗരത്തിൽ കലാപം ഉണ്ടാക്കി?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'മോണ്ട്ഗോമറി',\n",
              "   'true_answer': 'മൊണ്ട്ഗോമറി',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.8181818181818181,\n",
              "   'match_type': 'fuzzy_high',\n",
              "   'answer_text': 'മോണ്ട്ഗോമറി',\n",
              "   'answer_token_ids': [0,\n",
              "    60455,\n",
              "    66836,\n",
              "    77381,\n",
              "    71430,\n",
              "    11886,\n",
              "    248,\n",
              "    66836,\n",
              "    60455,\n",
              "    107024,\n",
              "    19057],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'മ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി',\n",
              "    'മോണ്ട്ഗോമറി'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\nചോദ്യം: ക്ലോക്ക് സിംഫണി രചിച്ചത് ആരാണ്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ഹെയ്ഡൻ',\n",
              "   'true_answer': 'ഹെയ്ഡൻ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'ഹെയ്ഡൻ',\n",
              "   'answer_token_ids': [0, 3370, 120, 40347, 56310, 11886, 97, 96028],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ',\n",
              "    'ഹെയ്ഡൻ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: എത്ര ഭൂഖണ്ഡങ്ങളുണ്ട്?\\nഉത്തരം: ഏഴ്\\nചോദ്യം: 2001-ൽ പുറത്തിറങ്ങിയ ദി സൈലൻസ് ഓഫ് ദി ലാംബ്സിന്റെ തുടർച്ചയായ സിനിമയുടെ പേര് എന്തായിരുന്നു?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ഹന്നിബൽ',\n",
              "   'true_answer': 'ഹന്നിബാൾ',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 0.7200000000000001,\n",
              "   'match_type': 'fuzzy_high',\n",
              "   'answer_text': 'ഹന്നിബൽ',\n",
              "   'answer_token_ids': [0, 3370, 120, 93349, 22872, 108, 78264],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.4741393129008623,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': 0.47413931285783734,\n",
              "   'all_generations': ['ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നി',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നിബൽ',\n",
              "    'ഹന്നി'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 2},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം എന്താണ്?\\nഉത്തരം: മീഥെയ്ൻ\\nചോദ്യം: 2006 ൽ ജർമ്മനിയിൽ നടന്ന ഫിഫ ലോകകപ്പ് ഫുട്ബോൾ മത്സരത്തിൽ ആരാണ് വിജയിച്ചത്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ഇറ്റലി',\n",
              "   'true_answer': 'ഇറ്റലി',\n",
              "   'classification': 'FACTUALITY',\n",
              "   'match_score': 1.0,\n",
              "   'match_type': 'exact',\n",
              "   'answer_text': 'ഇറ്റലി',\n",
              "   'answer_token_ids': [0, 3370, 232, 107024, 11886, 112, 75041, 19057],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി',\n",
              "    'ഇറ്റലി'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1}],\n",
              " [{'prompt': \"ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: ഫ്രാൻസിന്റെ തലസ്ഥാനം എന്താണ്?\\nഉത്തരം: പാരീസ്\\nചോദ്യം: പത്തൊൻപതാം നൂറ്റാണ്ടിലെ ഏത് സംഘർഷമാണ് ചാൾസ് ഫ്രാസിയറിന്റെ 'കോൾഡ് മൌണ്ടൻ' നോവലിന് പിന്നിലെ പശ്ചാത്തലം?\\nഉത്തരം:\",\n",
              "   'full_llm_output': 'ക്രിമിയൻ യ',\n",
              "   'true_answer': 'അമേരിക്കൻ ആഭ്യന്തരയുദ്ധം',\n",
              "   'classification': 'HALLUCINATION',\n",
              "   'match_score': 0.0,\n",
              "   'match_type': 'no_match',\n",
              "   'answer_text': 'ക്രിമിയൻ യ',\n",
              "   'answer_token_ids': [0,\n",
              "    3370,\n",
              "    47476,\n",
              "    111,\n",
              "    22872,\n",
              "    109,\n",
              "    106967,\n",
              "    96028,\n",
              "    14237,\n",
              "    110],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.0,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ',\n",
              "    'ക്രിമിയൻ യ'],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\nചോദ്യം: ഏതൊരു സിനിമയ്ക്കാണ് കത്രീൻ ഹെപ്ബർന് മൂന്നാമത്തെ ഓസ്കാർ നേടിയത്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'ഗസ് ലൈക്ക്',\n",
              "   'true_answer': 'ദി ലയൺ ഇൻ വിൻറർ',\n",
              "   'classification': 'HALLUCINATION',\n",
              "   'match_score': 0.0,\n",
              "   'match_type': 'no_match',\n",
              "   'answer_text': 'ഗസ് ലൈക്ക്',\n",
              "   'answer_token_ids': [0,\n",
              "    3370,\n",
              "    248,\n",
              "    79878,\n",
              "    11151,\n",
              "    14237,\n",
              "    113,\n",
              "    6466,\n",
              "    233,\n",
              "    107145,\n",
              "    11151],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': '\\\\xe0\\\\xb4', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.014827070971544959,\n",
              "   'regular_entropy': 0.9165403927272727,\n",
              "   'cluster_assignment_entropy': 0.47413931285783734,\n",
              "   'all_generations': ['8  ',\n",
              "    '8  ',\n",
              "    'അഞ്ചാം മഴ',\n",
              "    '8  ',\n",
              "    'അഞ്ചാം മഴ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  '],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 2},\n",
              "  {'prompt': 'ആലിസും ബോമ്പും പൊതുവിജ്ഞാനത്തെ കുറിച്ചുള്ള ഒരു പരിക്ഷയ്ക്കു പഠിക്കുന വിദ്യാർഥികളാണ. ഈ പരിക്ഷയ്ക്ക് വലരെ വിശാലമായ അരിവും ധാരണയും ആവിശ്യമാണ്. ആലിസ് രണ്ടുപേരിലും കൂടതൽ ബുദ്ധിമതിയാണ. ആകെ നാല് ചോദ്യങ്ങൾ നല്കുന്നു. പരിക്ഷ വലരെ ബുദ്ധിമുട്ടുള്ളതായതിനാൽ, മിക്ക വിദ്യാർഥികൾക്കും പരമാധികം 75 മാർക്ക് (4-ൽ 3) മാത്രമേ നെടാനാകു, വിജയ്ക്കാനുള്ള മിനിമം മാർക്ക് 50 മാത്രമണ് (4-ൽ 2). ഉത്തരം നല്കാതിരിക്കുനതിനേക്കാൾ ഊഹിക്കുന്നതാണ നല്ലത് കാരണം ശ്രമിച്ചതിന് ഭാഗീക മാർക്ക് ലാബിക്കും. ബോമ്പിനും ആലിസിനും പരിക്ഷ പാസാകണം. ആലിസ് ചോദ്യങ്ങൾ ചോതിക്കും, ബോബ് കഴിവിന്റെ പരമാവധി ചെയ്യം. ഉത്തരം മാത്രം നൽകുക, വാക്യമല്ല.\\nചോദ്യം: 64-ന്റെ വർഗ്ഗമൂലം എന്താണ്?\\nഉത്തരം: എട്ട്\\nചോദ്യം: ആഫ്രിക്കയും ഏഷ്യയും ഉൾക്കൊള്ളുന്ന ഏക രാജ്യം ഏതാണ്?\\nഉത്തരം:',\n",
              "   'full_llm_output': 'റഷ്യ',\n",
              "   'true_answer': 'ഈജിപ്ത്',\n",
              "   'classification': 'HALLUCINATION',\n",
              "   'match_score': 0.0,\n",
              "   'match_type': 'no_match',\n",
              "   'answer_text': 'റഷ്യ',\n",
              "   'answer_token_ids': [0, 107024, 3370, 118, 83064],\n",
              "   'first_token_probability': 1.0,\n",
              "   'top_word_alternatives': [{'token': 'റ', 'prob': 1.0}],\n",
              "   'prob_diff_top2': 1.0,\n",
              "   'semantic_entropy': 0.0,\n",
              "   'regular_entropy': 0.058274599999999996,\n",
              "   'cluster_assignment_entropy': -1.000000082690371e-10,\n",
              "   'all_generations': ['8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  ',\n",
              "    '8  '],\n",
              "   'num_generations': 11,\n",
              "   'num_semantic_clusters': 1}])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}