{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB+cUF1Ah9o5jYPTck8YbB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanya-Zac/Multilingual-LLM-hallucination-test/blob/main/knowledge_dataset_creation_malayalam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Simple Malayalam CSV to JSON Converter\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def simple_csv_to_json(csv_file='mal_2000.csv', json_file='mal_dataset.json', max_rows=15000):\n",
        "    \"\"\"\n",
        "    Simplest possible implementation for Malayalam dataset\n",
        "    \"\"\"\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
        "\n",
        "    # Keep first 15000 rows (or whatever is in the file)\n",
        "    df = df.iloc[:max_rows]\n",
        "\n",
        "    # Convert to JSON (array of objects format)\n",
        "    df.to_json(json_file, orient='records', indent=2, force_ascii=False)\n",
        "\n",
        "    print(f\"✓ Converted {len(df)} rows from {csv_file} to {json_file}\")\n",
        "\n",
        "def oneliner_csv_to_json(csv_file='mal_2000.csv', json_file='mal_dataset.json'):\n",
        "    \"\"\"Ultra-compact version\"\"\"\n",
        "    pd.read_csv(csv_file, encoding='utf-8').iloc[:15000].to_json(\n",
        "        json_file, orient='records', indent=2, force_ascii=False\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Convert Malayalam dataset\n",
        "    simple_csv_to_json('mal_2000.csv', 'mal_dataset.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysFQdGwdh4yb",
        "outputId": "eb44bca8-8555-49d7-cc90-1b64361a0ec4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Converted 15000 rows from mal_2000.csv to mal_dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "kN3LJNZUH0o0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RofO1vUbG54_"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrfmsBaPdozq",
        "outputId": "54c51a5c-73c3-42a2-b836-7c0a9f3036c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rapidfuzz import fuzz\n",
        "import random\n",
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "\n",
        "class KnowledgeDataset:\n",
        "    def __init__(self, model_name=\"gpt-4o-mini\", dataset_path=\"mal_500.json\", save_dir=\"./\", dataset_name=\"mal_dataset\"):\n",
        "        # Get API key from Colab secrets\n",
        "        api_key = userdata.get('gptapi')\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.dataset_path = dataset_path\n",
        "        self.save_dir = save_dir\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        # Initialize tokenizer for the model\n",
        "        # For gpt-4o-mini, we use cl100k_base encoding\n",
        "        self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "\n",
        "        # Load dataset\n",
        "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        self.initial_dataset = []\n",
        "        for item in raw_data:\n",
        "            if isinstance(item, str):\n",
        "                try:\n",
        "                    q, a = item.split(\"\\nഉത്തരം:\")\n",
        "                    q = q.replace(\"ചോദ്യം:\", \"\").strip()\n",
        "                    a = a.strip()\n",
        "                    if q and a and q.lower() != \"nan\" and a.lower() != \"nan\":\n",
        "                        self.initial_dataset.append({\"ചോദ്യം\": q, \"ഉത്തരം\": a})\n",
        "                except ValueError:\n",
        "                    # Silent skip for malformed items\n",
        "                    pass\n",
        "            elif isinstance(item, dict):\n",
        "                q = item.get(\"ചോദ്യം\", \"\").strip()\n",
        "                a = item.get(\"ഉത്തരം\", \"\").strip()\n",
        "                if q and a and q.lower() != \"nan\" and a.lower() != \"nan\":\n",
        "                    self.initial_dataset.append({\"ചോദ്യം\": q, \"ഉത്തരം\": a})\n",
        "\n",
        "        # Pre-built Malayalam few-shot examples\n",
        "        self.list_good_shot = [\n",
        "            \"ചോദ്യം: ഫ്രാൻസിന്റെ തലസ്ഥാനം ഏതാണ്?\\nഉത്തരം: പാരീസ്\\n\",\n",
        "            \"ചോദ്യം: എത്ര ഭൂഖണ്ഡങ്ങളുണ്ട്?\\nഉത്തരം: 7\\n\",\n",
        "            \"ചോദ്യം: പ്രകൃതിവാതകത്തിന്റെ പ്രധാന ഘടകം ഏതാണ്?\\nഉത്തരം:മീഥേൻ\\n\",\n",
        "            \"ചോദ്യം: 1999 ൽ ഓസ്ട്രേലിയയിലെ മെൽബണിൽ ആരംഭിച്ച 'മോവെംബർ' എന്ന വാർഷിക ചാരിറ്റബിൾ സംരംഭം പുരുഷന്മാരുടെ ആരോഗ്യ പ്രശ്നങ്ങൾക്ക് (പ്രത്യേകിച്ച് പ്രോസ്റ്റേറ്റ്, ടെസ്റ്റിക്കിൾ കാൻസർ) വേണ്ടിയുള്ള ബോധവൽക്കരണവും ധാരണയും ഉയർത്തുന്നു. ഇതിന്റെ ഭാഗമായി പുരുഷന്മാർ എന്താണ് ധരിക്കുന്നത്?\\nഉത്തരം:മീശകൾ\\n\",\n",
        "            \"ചോദ്യം: 'ദി എറിക് ബർത്തലോമെവ്' എന്ന വെതർസ്പൂൺസ് പബ് ഏത് പട്ടണത്തിലാണ്?\\nഉത്തരം:മോറെകാംബെ\\n\",\n",
        "            \"ചോദ്യം: 'റോമിയോ ആൻഡ് ജൂലിയറ്റ്' എഴുതിയത് ആരാണ്?\\nഉത്തരം: വില്യം ഷേക്സ്പിയർ\\n\",\n",
        "            \"ചോദ്യം: 64 ന്റെ വർഗ്ഗമൂല്യം എന്താണ്?\\nഉത്തരം: 8\\n\",\n",
        "            \"ചോദ്യം: ഏത് മൂലകത്തിനാണ് 'H' എന്ന രാസചിഹ്നം ഉള്ളത്?\\nഉത്തരം: ഹൈഡ്രജൻ\\n\",\n",
        "            \"ചോദ്യം: അമേരിക്കൻ ഐക്യനാടുകളുടെ ആദ്യ പ്രസിഡന്റ് ആരായിരുന്നു?\\nഉത്തരം: ജോർജ്ജ് വാഷിംഗ്ടൺ\\n\",\n",
        "            \"ചോദ്യം: കോശത്തിന്റെ പവർഹൗസ് എന്താണ്?\\nഉത്തരം: മൈറ്റോകോൺഡ്രിയ\\n\",\n",
        "            \"ചോദ്യം: ഗ്രനാഡ ടെലിവിഷനുവേണ്ടി ജെറി ആൻഡേഴ്‌സൺ നിർമ്മിച്ച മൂന്നാമത്തെ പപ്പറ്റ് ടെലിവിഷൻ ഷോ എന്തായിരുന്നു 'ഫോർ ___ ഫാൾസ്'?\\nഉത്തരം:ഫെതർ\\n\",\n",
        "            \"ചോദ്യം: മാർഗരിറ്റ് ഹെൻറി എഴുതിയ 'കിംഗ് ഓഫ് ദി വിൻഡ്' എന്ന കുട്ടികളുടെ പുസ്തകത്തിൽ, സ്വർണ്ണ നിറത്തിലുള്ള അറേബ്യൻ കുതിരയുടെ പേരെന്താണ്?\\nഉത്തരം:ഷാം\\n\",\n",
        "            \"ചോദ്യം: പോളും ലിൻഡ മക്കാർട്ട്‌നിയും ചേർന്ന് രചിച്ച ബോണ്ട് സിനിമയിലെ തീം സോങ്ങ് ഏതാണ്?\\nഉത്തരം:ലൈവ് ആൻഡ് ലെറ്റ് ഡൈ\\n\",\n",
        "            \"ചോദ്യം: മോണാലിസ ആരാണ് വരച്ചത്?\\nഉത്തരം: ലിയോനാർഡോ ഡാവിഞ്ചി\\n\"\n",
        "        ]\n",
        "\n",
        "    # ------------------------------\n",
        "    # Get token IDs for a text\n",
        "    # ------------------------------\n",
        "    def get_token_ids(self, text):\n",
        "        \"\"\"Convert text to token IDs using the model's tokenizer\"\"\"\n",
        "        return self.tokenizer.encode(text)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Batch generation (temperature sampling)\n",
        "    # ------------------------------\n",
        "    def batch_generation_with_temperature(self, prompt, n=5, max_tokens=50, temperature=0.5):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            n=n,\n",
        "            max_completion_tokens=max_tokens\n",
        "        )\n",
        "        return [choice.message.content.strip() for choice in response.choices]\n",
        "\n",
        "    # ------------------------------\n",
        "    # Greedy generation (temperature=0)\n",
        "    # ------------------------------\n",
        "    def greedy_generation(self, prompt, max_tokens=50):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0,\n",
        "            max_completion_tokens=max_tokens\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    # ------------------------------\n",
        "    # Malayalam-aware fuzzy matching\n",
        "    # ------------------------------\n",
        "    @staticmethod\n",
        "    def partial_match(a: str, gen: str, threshold: float = 65) -> bool:\n",
        "        a_norm  = unicodedata.normalize(\"NFC\", a).strip()\n",
        "        gen_norm = unicodedata.normalize(\"NFC\", gen).strip()\n",
        "        if a_norm and a_norm in gen_norm:\n",
        "            return True\n",
        "        a_tokens  = set(re.findall(r\"\\w+\", a_norm))\n",
        "        gen_tokens = set(re.findall(r\"\\w+\", gen_norm))\n",
        "        if a_tokens and a_tokens.issubset(gen_tokens):\n",
        "            return True\n",
        "        return fuzz.partial_ratio(a_norm, gen_norm) >= threshold\n",
        "\n",
        "    # ------------------------------\n",
        "    # Core dataset creation\n",
        "    # ------------------------------\n",
        "    def create_knowledge_dataset(self, save_every=100):\n",
        "        knowledge_dataset = []\n",
        "        non_knowledge_dataset = []\n",
        "        else_dataset = []\n",
        "\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "        # Create progress bar with custom formatting\n",
        "        pbar = tqdm(\n",
        "            self.initial_dataset,\n",
        "            desc=\"Processing dataset\",\n",
        "            total=len(self.initial_dataset),\n",
        "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
        "        )\n",
        "\n",
        "        for index, point in enumerate(pbar, 1):\n",
        "            q = point[\"ചോദ്യം\"]\n",
        "            a = point[\"ഉത്തരം\"]\n",
        "            if not q or not a:\n",
        "                continue\n",
        "\n",
        "            # Create prompt in the format you specified\n",
        "            prompt = f\"ചോദ്യം: {q}\\nഉത്തരം:\"\n",
        "\n",
        "            # Get token IDs for the answer\n",
        "            token_ids = self.get_token_ids(a)\n",
        "\n",
        "            # pick 3 random few-shot examples\n",
        "            idx = random.sample(range(len(self.list_good_shot)), 3)\n",
        "            good_shots = \"\".join([self.list_good_shot[i] for i in idx])\n",
        "\n",
        "            # generate 6 outputs (5 temp + 1 greedy)\n",
        "            temp_generation = self.batch_generation_with_temperature(good_shots + prompt, temperature=0.5)\n",
        "            greedy_gen = self.greedy_generation(good_shots + prompt)\n",
        "            temp_generation.append(greedy_gen)\n",
        "\n",
        "            # classify using Malayalam fuzzy matching\n",
        "            count_know = sum([1 for temp in temp_generation if self.partial_match(a, temp)])\n",
        "\n",
        "            # Store in the format: [prompt, answer, token_ids_list, count]\n",
        "            if count_know == 6:\n",
        "                knowledge_dataset.append([prompt, a, token_ids, count_know])\n",
        "            elif count_know == 0:\n",
        "                non_knowledge_dataset.append([prompt, a, token_ids, count_know])\n",
        "            else:\n",
        "                else_dataset.append([prompt, a, token_ids, count_know])\n",
        "\n",
        "            # Update progress bar description with current counts\n",
        "            pbar.set_description(\n",
        "                f\"K:{len(knowledge_dataset)} NK:{len(non_knowledge_dataset)} E:{len(else_dataset)}\"\n",
        "            )\n",
        "\n",
        "            # periodic saving (without disrupting progress bar)\n",
        "            if index % save_every == 0:\n",
        "                self.save_data(knowledge_dataset, os.path.join(self.save_dir, f\"{self.model_name}_{self.dataset_name}_knowledge.json\"))\n",
        "                self.save_data(non_knowledge_dataset, os.path.join(self.save_dir, f\"{self.model_name}_{self.dataset_name}_non_knowledge.json\"))\n",
        "                self.save_data(else_dataset, os.path.join(self.save_dir, f\"{self.model_name}_{self.dataset_name}_else.json\"))\n",
        "                # Add a postfix to show last save point\n",
        "                pbar.set_postfix({\"last_save\": f\"idx_{index}\"})\n",
        "\n",
        "        # Close progress bar\n",
        "        pbar.close()\n",
        "\n",
        "        # final save\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"FINAL RESULTS:\")\n",
        "        print(f\"  Knowledge dataset: {len(knowledge_dataset)} items\")\n",
        "        print(f\"  Non-knowledge dataset: {len(non_knowledge_dataset)} items\")\n",
        "        print(f\"  Else dataset: {len(else_dataset)} items\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.save_data(knowledge_dataset, os.path.join(self.save_dir, f\"{self.model_name}_{self.dataset_name}_knowledge.json\"))\n",
        "        self.save_data(non_knowledge_dataset, os.path.join(self.save_dir, f\"{self.model_name}_{self.dataset_name}_non_knowledge.json\"))\n",
        "        self.save_data(else_dataset, os.path.join(self.save_dir, f\"{self.model_name}_{self.dataset_name}_else.json\"))\n",
        "\n",
        "    # ------------------------------\n",
        "    # Save helper\n",
        "    # ------------------------------\n",
        "    def save_data(self, data, path):\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # First install tiktoken if not already installed\n",
        "    # !pip install tiktoken\n",
        "\n",
        "    # Create and run the dataset processor\n",
        "    processor = KnowledgeDataset(\n",
        "        model_name=\"gpt-4o-mini\",\n",
        "        dataset_path=\"mal_dataset.json\",\n",
        "        save_dir=\"./output/\",\n",
        "        dataset_name=\"mal_dataset\"\n",
        "    )\n",
        "\n",
        "    processor.create_knowledge_dataset(save_every=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWLseXUvt2o3",
        "outputId": "ee9ee9ca-93da-4050-d80a-7b8e7e3c39dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "K:3188 NK:7910 E:3902: 100%|██████████| 15000/15000 [8:13:13<00:00,  1.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "FINAL RESULTS:\n",
            "  Knowledge dataset: 3188 items\n",
            "  Non-knowledge dataset: 7910 items\n",
            "  Else dataset: 3902 items\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}