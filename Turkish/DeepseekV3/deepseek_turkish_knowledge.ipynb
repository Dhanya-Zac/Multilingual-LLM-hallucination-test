{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIWhimVJofPw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNbpJcdFx-jP",
        "outputId": "5dc9e3d6-93d8-4fe4-d87f-7dbedc32efea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 15000 rows from turkish.csv to turkish_dataset.json\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple Usage Example for CSV to JSON Converter\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Simple, direct approach\n",
        "def simple_csv_to_json(csv_file, json_file, max_rows=15000):\n",
        "    \"\"\"\n",
        "    Simplest possible implementation\n",
        "    \"\"\"\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Keep first 15000 rows (delete everything from row 15001 onwards)\n",
        "    df = df.iloc[:max_rows]\n",
        "\n",
        "    # Convert to JSON (array of objects format)\n",
        "    df.to_json(json_file, orient='records', indent=2)\n",
        "\n",
        "    print(f\"Converted {len(df)} rows from {csv_file} to {json_file}\")\n",
        "\n",
        "\n",
        "\n",
        "def oneliner_csv_to_json(csv_file, json_file):\n",
        "    \"\"\"Ultra-compact version\"\"\"\n",
        "    pd.read_csv(csv_file).iloc[:15000].to_json(json_file, orient='records', indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Simple usage\n",
        "    simple_csv_to_json('turkish.csv', 'turkish_dataset.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "237b39308a694f13a2080b83c8bfc693",
            "d37298ab14bc4b968cbbc240704e35d0",
            "80b0127198774af8bddbaaec653e0677",
            "79c07308c3e04f62977c5bac6bac3ddc",
            "f4c537aba6ac49349d86c3dce8114e18",
            "e4a511170a624dafb2a8f6a73d95c372",
            "18d3f8133e6f4da28f46fc9e51dbfa39",
            "f306fa9a80cd4b2d9bdd9ddb3babcfb0",
            "d0e965067422467ab3554535086b2dd4",
            "eccdf828247643529b046c64edb4d4a9",
            "5d55feaee35a4484a0857652125875a9",
            "09bfad5df938409b8cfb19d0cbac71c1",
            "fc60daa4541d423f8630ef03cbd1cfff",
            "8eb246a7e41d426c9fe5ec0cfc067a8d",
            "8e8ba9ddf4574576bdd4fa3b1acf06fa",
            "e2f17efc82b14cfc8eb274e1a93a373c",
            "7aee3a31ffda4ef0a50c1608fff4770e",
            "d815bdb548364fe1ae67b2f6da44d924",
            "29ac1e68fb6f402aa40edf97e703c4c5",
            "21258d11c33e4acda3eb7d942123db2f",
            "77b237734314458a813fdf163c54eca2",
            "27422c114358429fb29f43d93fc0bfc2"
          ]
        },
        "id": "lKdL9nlm42mC",
        "outputId": "dcfb9128-1eb0-4052-82fa-2a21209d8959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Turkish Knowledge Dataset Creator with DeepSeek V3\n",
            "============================================================\n",
            "\n",
            "Configuration:\n",
            "  path_to_knowledge_dataset: datasets/deepseek/turkish/\n",
            "  dataset_name: turkish\n",
            "  model_name: deepseek-chat\n",
            "  batch_size: 2000\n",
            "  tokenizer_model: deepseek-ai/DeepSeek-V3\n",
            "\n",
            "Initializing Turkish Knowledge Dataset Creator with DeepSeek V3...\n",
            "Model: deepseek-chat\n",
            "Tokenizer: deepseek-ai/DeepSeek-V3\n",
            "Batch size: 2000\n",
            "\n",
            "Loading DeepSeek V3 tokenizer from deepseek-ai/DeepSeek-V3...\n",
            "This may take a moment on first run as it downloads the tokenizer files...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "237b39308a694f13a2080b83c8bfc693",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09bfad5df938409b8cfb19d0cbac71c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Tokenizer loaded successfully!\n",
            "  Vocabulary size: 128000\n",
            "  Model max length: 131072\n",
            "  Turkish test: 'Merhaba d√ºnya! ƒ∞stanbul'dan selamlar.' -> 15 tokens\n",
            "\n",
            "‚úì Turkish text matcher initialized with multi-tier matching strategy\n",
            "Please upload your Turkish dataset file (JSON or CSV)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9fb66b41-881b-4fe9-b7a4-719885752c6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9fb66b41-881b-4fe9-b7a4-719885752c6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving turkish_dataset.json to turkish_dataset.json\n",
            "Uploaded file: turkish_dataset.json\n",
            "Loaded 15000 records from turkish_dataset.json\n",
            "Example 0: soru: Henry Ford, Magic Johnson ve Berry Gordy han... -> Michigan (2 tokens)\n",
            "Example 1: soru: Ring of Fire' hangi okyanusta bulunuyor?\n",
            "cev... -> Pasifik Okyanusu (6 tokens)\n",
            "Example 2: soru: Doƒüal gazƒ±n ana bile≈üeni nedir?\n",
            "cevap:... -> Metan (2 tokens)\n",
            "Example 3: soru: \"Zamanƒ±n M√ºziƒüine Dans\" adlƒ± 12 ciltlik roma... -> Anthony Powell (2 tokens)\n",
            "Example 4: soru: 1999 yƒ±lƒ±nda Avustralya‚Äônƒ±n Melbourne kentin... -> Bƒ±yƒ±klar (5 tokens)\n",
            "\n",
            "Tokenization Statistics (DeepSeek V3):\n",
            "  Average tokens per answer: 3.24\n",
            "  Min tokens: 1\n",
            "  Max tokens: 20\n",
            "  Median tokens: 3.00\n",
            "\n",
            "============================================================\n",
            "Processing 15000 examples in batches of 2000...\n",
            "Using advanced Turkish-aware string matching...\n",
            "Using DeepSeek model: deepseek-chat\n",
            "Using tokenizer: DeepSeek V3 (vocab size: 128000)\n",
            "============================================================\n",
            "\n",
            "‚ö†Ô∏è  Resuming from batch 8 (index 14000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:   0%|          | 1/1000 [00:09<2:34:32,  9.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 0: soru: Sherlock Holmes hangi enstr√ºmanƒ± √ßalardƒ±?\n",
            "cevap:...\n",
            "Target answer: 'Keman' (2 tokens)\n",
            "Greedy generation: 'Keman'\n",
            "Match scores: ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00']\n",
            "Match types: ['exact', 'exact']...\n",
            "Average score: 1.000\n",
            "Category: KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 2/1000 [00:17<2:26:28,  8.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 1: soru: Gulliver‚Äôin ilk adƒ± nedir?\n",
            "cevap:...\n",
            "Target answer: 'Lemuel' (2 tokens)\n",
            "Greedy generation: 'Lemuel Gulliver'\n",
            "Match scores: ['1.00', '0.95', '1.00', '0.95', '0.95', '0.95']\n",
            "Match types: ['exact', 'exact']...\n",
            "Average score: 0.967\n",
            "Category: KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 3/1000 [00:29<2:51:53, 10.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 2: soru: 1913-1938 yƒ±llarƒ± arasƒ±nda bir nikelin arka y√ºz√ºnde ha...\n",
            "Target answer: 'Buffalo' (2 tokens)\n",
            "Greedy generation: 'Bu sorunun cevabƒ± \"buffalo\" (bufalo) olmalƒ±'\n",
            "Match scores: ['0.95', '0.00', '0.95', '0.95', '0.95', '0.95']\n",
            "Match types: ['exact', 'no_match']...\n",
            "Average score: 0.792\n",
            "Category: PARTIAL\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 4/1000 [00:41<3:00:44, 10.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 3: soru: Albert Finney, 1983 yapƒ±mƒ± hangi filmde ‚ÄòSir‚Äô karakter...\n",
            "Target answer: 'The Dresser' (3 tokens)\n",
            "Greedy generation: 'Albert Finney, 1983 yapƒ±mƒ± \"The Dresser\" (T√ºrk'\n",
            "Match scores: ['0.95', '0.95', '0.95', '0.95', '0.95', '0.95']\n",
            "Match types: ['exact', 'exact']...\n",
            "Average score: 0.950\n",
            "Category: KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 5/1000 [00:48<2:38:56,  9.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 4: soru: 1909 yƒ±lƒ±nda yayƒ±mlanan ve d√∂nemin √ßaƒüda≈ü siyasi sorun...\n",
            "Target answer: 'H G Wells' (3 tokens)\n",
            "Greedy generation: 'H'\n",
            "Match scores: ['0.13', '0.13', '0.13', '0.13', '0.13', '0.13']\n",
            "Match types: ['partial', 'partial']...\n",
            "Average score: 0.133\n",
            "Category: NON-KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:50:45<00:00, 10.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Saved knowledge batch 8 to datasets/deepseek/turkish/batches/deepseek-chat_turkish_knowledge_batch_0008.json (575 examples)\n",
            "  ‚úì Saved partial_knowledge batch 8 to datasets/deepseek/turkish/batches/deepseek-chat_turkish_partial_knowledge_batch_0008.json (116 examples)\n",
            "  ‚úì Saved non_knowledge batch 8 to datasets/deepseek/turkish/batches/deepseek-chat_turkish_non_knowledge_batch_0008.json (309 examples)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üì¶ Batch 8 Summary\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Examples in batch: 1000\n",
            "Knowledge (‚â•0.8): 575 (57.5%)\n",
            "Partial (0.3-0.8): 116 (11.6%)\n",
            "Non-knowledge (<0.3): 309 (30.9%)\n",
            "Total processed: 15000/15000\n",
            "Running totals - K: 575, P: 116, NK: 309\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "\n",
            "‚úì Saved batch metadata to datasets/deepseek/turkish/deepseek-chat_turkish_metadata.json\n",
            "\n",
            "============================================================\n",
            "üéâ FINAL RESULTS\n",
            "============================================================\n",
            "Total batches created: 8\n",
            "Total Knowledge (avg score ‚â•0.8): 575 (57.5%)\n",
            "Total Partial Knowledge (0.3-0.8): 116 (11.6%)\n",
            "Total Non-knowledge (score <0.3): 309 (30.9%)\n",
            "Total examples processed: 1000\n",
            "\n",
            "--- Match Type Distribution ---\n",
            "  exact: 3663\n",
            "  partial: 364\n",
            "  fuzzy: 181\n",
            "  token_overlap_low: 57\n",
            "  token_overlap: 42\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from openai import OpenAI  # Use OpenAI SDK for DeepSeek\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "import sys\n",
        "\n",
        "# --- Transformers import for DeepSeek tokenizer ---\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "except ImportError:\n",
        "    print(\"Error: transformers library not installed.\")\n",
        "    print(\"Please install it using: pip install transformers\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- Colab-specific imports ---\n",
        "try:\n",
        "    from google.colab import userdata, files\n",
        "    import io\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Note: Not running in Google Colab. File upload functionality will be limited.\")\n",
        "\n",
        "\n",
        "class TurkishTextMatcher:\n",
        "    \"\"\"Advanced Turkish text matching with multi-tier strategy\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Turkish-specific character mappings\n",
        "        self.turkish_lower_map = {\n",
        "            'ƒ∞': 'i', 'I': 'ƒ±', 'ƒû': 'ƒü', '√ú': '√º',\n",
        "            '≈û': '≈ü', '√ñ': '√∂', '√á': '√ß'\n",
        "        }\n",
        "\n",
        "        # Common stop words to ignore in token matching (Turkish and English)\n",
        "        self.stop_words = {\n",
        "            # Turkish\n",
        "            've', 'veya', 'ile', 'i√ßin', 'bir', 'bu', 'da', 'de',\n",
        "            'mi', 'mu', 'mƒ±', 'm√º', 'ki', 'ne', 'ya', 'ama', 'fakat',\n",
        "            '√ß√ºnk√º', 'gibi', 'kadar', 'hem', 'daha', '√ßok', 'en',\n",
        "            # English\n",
        "            'the', 'of', 'and', 'a', 'an', 'in', 'on', 'at', 'to',\n",
        "            'for', 'with', 'by', 'from', 'as', 'or', 'but', 'is', 'was'\n",
        "        }\n",
        "\n",
        "        # Common abbreviations and variations\n",
        "        self.abbreviations = {\n",
        "            'dr': 'doktor',\n",
        "            'prof': 'profes√∂r',\n",
        "            'st': 'saint',\n",
        "            'abd': 'amerika birle≈üik devletleri',\n",
        "            'usa': 'amerika birle≈üik devletleri',\n",
        "            'uk': 'birle≈üik krallƒ±k',\n",
        "            'eu': 'avrupa birliƒüi'\n",
        "        }\n",
        "\n",
        "    def turkish_lower(self, text: str) -> str:\n",
        "        \"\"\"Properly convert Turkish text to lowercase\"\"\"\n",
        "        # First apply Turkish-specific mappings\n",
        "        for upper, lower in self.turkish_lower_map.items():\n",
        "            text = text.replace(upper, lower)\n",
        "        # Then apply standard lowercase\n",
        "        return text.lower()\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Normalize text for comparison\"\"\"\n",
        "        # Convert to lowercase with Turkish awareness\n",
        "        text = self.turkish_lower(text)\n",
        "\n",
        "        # Remove punctuation but keep spaces\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Expand common abbreviations\n",
        "        words = text.split()\n",
        "        words = [self.abbreviations.get(w, w) for w in words]\n",
        "        text = ' '.join(words)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_tokens(self, text: str, remove_stopwords: bool = True) -> List[str]:\n",
        "        \"\"\"Extract meaningful tokens from text\"\"\"\n",
        "        normalized = self.normalize_text(text)\n",
        "        tokens = normalized.split()\n",
        "\n",
        "        if remove_stopwords:\n",
        "            tokens = [t for t in tokens if t not in self.stop_words]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def exact_match(self, answer: str, generation: str) -> float:\n",
        "        \"\"\"Check for exact match after normalization\"\"\"\n",
        "        norm_answer = self.normalize_text(answer)\n",
        "        norm_generation = self.normalize_text(generation)\n",
        "\n",
        "        # Check if exact match or if answer is contained in generation\n",
        "        if norm_answer == norm_generation:\n",
        "            return 1.0\n",
        "        elif norm_answer in norm_generation:\n",
        "            # Penalize slightly if answer is just contained (not exact)\n",
        "            return 0.95\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def token_overlap_match(self, answer: str, generation: str) -> float:\n",
        "        \"\"\"Calculate token overlap score\"\"\"\n",
        "        answer_tokens = set(self.extract_tokens(answer, remove_stopwords=True))\n",
        "        generation_tokens = set(self.extract_tokens(generation, remove_stopwords=False))\n",
        "\n",
        "        if not answer_tokens:\n",
        "            return 0.0\n",
        "\n",
        "        # Check if all answer tokens appear in generation\n",
        "        overlap = answer_tokens.intersection(generation_tokens)\n",
        "        coverage = len(overlap) / len(answer_tokens)\n",
        "\n",
        "        # Bonus if tokens appear in same order\n",
        "        if coverage == 1.0:\n",
        "            answer_list = self.extract_tokens(answer, remove_stopwords=True)\n",
        "            gen_list = self.extract_tokens(generation, remove_stopwords=False)\n",
        "\n",
        "            # Check sequence preservation\n",
        "            try:\n",
        "                indices = [gen_list.index(token) for token in answer_list]\n",
        "                if indices == sorted(indices):\n",
        "                    return 0.9  # Full overlap with correct order\n",
        "                else:\n",
        "                    return 0.8  # Full overlap but different order\n",
        "            except ValueError:\n",
        "                return 0.8\n",
        "\n",
        "        return coverage * 0.8  # Partial overlap\n",
        "\n",
        "    def fuzzy_match(self, answer: str, generation: str, threshold: float = 0.85) -> float:\n",
        "        \"\"\"Fuzzy string matching using edit distance\"\"\"\n",
        "        norm_answer = self.normalize_text(answer)\n",
        "        norm_generation = self.normalize_text(generation)\n",
        "\n",
        "        # For short answers, check if it's contained with small variations\n",
        "        if len(norm_answer) <= 20:\n",
        "            # Use SequenceMatcher for similarity\n",
        "            matcher = SequenceMatcher(None, norm_answer, norm_generation)\n",
        "            similarity = matcher.ratio()\n",
        "\n",
        "            if similarity >= threshold:\n",
        "                return similarity * 0.6  # Scale to max 0.6 for fuzzy matches\n",
        "\n",
        "            # Also check if answer appears as substring with minor variations\n",
        "            words_in_gen = norm_generation.split()\n",
        "            for i in range(len(words_in_gen)):\n",
        "                for j in range(i+1, min(i+6, len(words_in_gen)+1)):\n",
        "                    substring = ' '.join(words_in_gen[i:j])\n",
        "                    matcher = SequenceMatcher(None, norm_answer, substring)\n",
        "                    if matcher.ratio() >= threshold:\n",
        "                        return matcher.ratio() * 0.6\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def partial_credit_match(self, answer: str, generation: str) -> float:\n",
        "        \"\"\"Give partial credit for partially correct answers\"\"\"\n",
        "        answer_tokens = self.extract_tokens(answer, remove_stopwords=False)\n",
        "\n",
        "        # For multi-word answers, check for important parts\n",
        "        if len(answer_tokens) >= 2:\n",
        "            # Check for proper nouns (capitalized in original)\n",
        "            important_parts = []\n",
        "\n",
        "            # Extract likely important parts (names, places, etc.)\n",
        "            original_words = answer.split()\n",
        "            for word in original_words:\n",
        "                if word and word[0].isupper():\n",
        "                    important_parts.append(self.turkish_lower(word))\n",
        "\n",
        "            if not important_parts:\n",
        "                # If no capitalized words, consider all non-stopwords important\n",
        "                important_parts = [t for t in answer_tokens if t not in self.stop_words]\n",
        "\n",
        "            if important_parts:\n",
        "                norm_generation = self.normalize_text(generation)\n",
        "                matches = sum(1 for part in important_parts if part in norm_generation)\n",
        "                return (matches / len(important_parts)) * 0.4\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def calculate_match_score(self, answer: str, generation: str) -> Tuple[float, str]:\n",
        "        \"\"\"\n",
        "        Calculate overall match score using multi-tier strategy\n",
        "        Returns: (score, match_type)\n",
        "        \"\"\"\n",
        "        # Tier 1: Exact match\n",
        "        exact_score = self.exact_match(answer, generation)\n",
        "        if exact_score > 0:\n",
        "            return exact_score, \"exact\"\n",
        "\n",
        "        # Tier 2: Token overlap\n",
        "        token_score = self.token_overlap_match(answer, generation)\n",
        "        if token_score >= 0.8:\n",
        "            return token_score, \"token_overlap\"\n",
        "\n",
        "        # Tier 3: Fuzzy match\n",
        "        fuzzy_score = self.fuzzy_match(answer, generation)\n",
        "        if fuzzy_score > 0:\n",
        "            return fuzzy_score, \"fuzzy\"\n",
        "\n",
        "        # Tier 4: Partial credit\n",
        "        partial_score = self.partial_credit_match(answer, generation)\n",
        "        if partial_score > 0:\n",
        "            return partial_score, \"partial\"\n",
        "\n",
        "        # Also return the best non-zero score if any\n",
        "        best_score = max(token_score, fuzzy_score, partial_score)\n",
        "        if best_score > 0:\n",
        "            if best_score == token_score:\n",
        "                return token_score, \"token_overlap_low\"\n",
        "            elif best_score == fuzzy_score:\n",
        "                return fuzzy_score, \"fuzzy_low\"\n",
        "            else:\n",
        "                return partial_score, \"partial_low\"\n",
        "\n",
        "        return 0.0, \"no_match\"\n",
        "\n",
        "\n",
        "class KnowledgeDatasetDeepSeek:\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_to_knowledge_dataset: str = \"datasets/\",\n",
        "        dataset_name: str = \"turkish\",\n",
        "        model_name: str = \"deepseek-chat\",  # DeepSeek model name for API\n",
        "        batch_size: int = 2000,\n",
        "        tokenizer_model: str = \"deepseek-ai/DeepSeek-V3\"  # DeepSeek V3 tokenizer\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the knowledge dataset creator for DeepSeek V3 with advanced Turkish matching\n",
        "\n",
        "        Args:\n",
        "            path_to_knowledge_dataset: Path to save the datasets\n",
        "            dataset_name: Name of the dataset (default: \"turkish\")\n",
        "            model_name: DeepSeek model name for API (deepseek-chat or deepseek-reasoner)\n",
        "            batch_size: Number of examples to process before saving a batch (default: 2000)\n",
        "            tokenizer_model: HuggingFace model ID for tokenizer (default: DeepSeek V3)\n",
        "        \"\"\"\n",
        "        print(f\"Initializing Turkish Knowledge Dataset Creator with DeepSeek V3...\")\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Tokenizer: {tokenizer_model}\")\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "        # --- Get API key securely ---\n",
        "        if IN_COLAB:\n",
        "            api_key = userdata.get('deepseek')\n",
        "            if not api_key:\n",
        "                raise ValueError(\"API key not found in Colab userdata. Please set it with userdata.set('deepseek', 'your_key').\")\n",
        "        else:\n",
        "            # For non-Colab environments, try environment variable\n",
        "            api_key = os.environ.get('DEEPSEEK_API_KEY')\n",
        "            if not api_key:\n",
        "                raise ValueError(\"DEEPSEEK_API_KEY environment variable not set.\")\n",
        "\n",
        "        # Set seeds for reproducibility\n",
        "        random.seed(42)\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Initialize OpenAI client with DeepSeek base URL\n",
        "        self.client = OpenAI(\n",
        "            api_key=api_key,\n",
        "            base_url=\"https://api.deepseek.com\"  # DeepSeek API endpoint\n",
        "        )\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize DeepSeek V3 tokenizer\n",
        "        print(f\"\\nLoading DeepSeek V3 tokenizer from {tokenizer_model}...\")\n",
        "        print(\"This may take a moment on first run as it downloads the tokenizer files...\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                tokenizer_model,\n",
        "                trust_remote_code=True,  # DeepSeek may use custom tokenizer code\n",
        "                cache_dir=os.path.join(path_to_knowledge_dataset, \".tokenizer_cache\")  # Cache tokenizer files\n",
        "            )\n",
        "            print(f\"‚úì Tokenizer loaded successfully!\")\n",
        "            print(f\"  Vocabulary size: {self.tokenizer.vocab_size}\")\n",
        "            print(f\"  Model max length: {self.tokenizer.model_max_length}\")\n",
        "\n",
        "            # Test tokenizer with Turkish text\n",
        "            test_text = \"Merhaba d√ºnya! ƒ∞stanbul'dan selamlar.\"\n",
        "            test_tokens = self.tokenizer.encode(test_text, add_special_tokens=False)\n",
        "            print(f\"  Turkish test: '{test_text}' -> {len(test_tokens)} tokens\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error loading DeepSeek V3 tokenizer: {e}\")\n",
        "            print(\"\\nTroubleshooting:\")\n",
        "            print(\"1. Ensure you have internet connection for first-time download\")\n",
        "            print(\"2. Try installing/updating transformers: pip install --upgrade transformers\")\n",
        "            print(\"3. If behind proxy, configure proxy settings\")\n",
        "            print(f\"4. Check if model '{tokenizer_model}' exists on HuggingFace\")\n",
        "            raise\n",
        "\n",
        "        # Initialize Turkish text matcher\n",
        "        self.matcher = TurkishTextMatcher()\n",
        "        print(\"\\n‚úì Turkish text matcher initialized with multi-tier matching strategy\")\n",
        "\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(path_to_knowledge_dataset, exist_ok=True)\n",
        "\n",
        "        # Create batch directory\n",
        "        self.batch_dir = os.path.join(path_to_knowledge_dataset, \"batches\")\n",
        "        os.makedirs(self.batch_dir, exist_ok=True)\n",
        "\n",
        "        # Load initial dataset\n",
        "        initial_dataset = self.load_manual_dataset()\n",
        "\n",
        "        # Create knowledge dataset with batch processing\n",
        "        self.create_knowledge_dataset(initial_dataset, path_to_knowledge_dataset)\n",
        "\n",
        "    def load_manual_dataset(self) -> List[Tuple]:\n",
        "        \"\"\"\n",
        "        Allow manual upload of a dataset file (turkish.json or .csv)\n",
        "        Each row should contain at least 'soru' and 'cevap'.\n",
        "        \"\"\"\n",
        "        if IN_COLAB:\n",
        "            print(\"Please upload your Turkish dataset file (JSON or CSV)...\")\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if not uploaded:\n",
        "                raise ValueError(\"No file uploaded. Please upload your Turkish dataset.\")\n",
        "\n",
        "            file_name = list(uploaded.keys())[0]\n",
        "            print(f\"Uploaded file: {file_name}\")\n",
        "\n",
        "            # --- Parse JSON ---\n",
        "            if file_name.endswith(\".json\"):\n",
        "                data = json.load(io.BytesIO(uploaded[file_name]))\n",
        "            # --- Parse CSV ---\n",
        "            elif file_name.endswith(\".csv\"):\n",
        "                import pandas as pd\n",
        "                df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "                data = df.to_dict(orient=\"records\")\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format. Please upload a JSON or CSV file.\")\n",
        "        else:\n",
        "            # Non-Colab: Look for local file\n",
        "            print(\"Looking for Turkish dataset file in current directory...\")\n",
        "            json_files = [f for f in os.listdir('.') if f.endswith('.json') and 'turkish' in f.lower()]\n",
        "            csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and 'turkish' in f.lower()]\n",
        "\n",
        "            if json_files:\n",
        "                file_name = json_files[0]\n",
        "                print(f\"Found JSON file: {file_name}\")\n",
        "                with open(file_name, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "            elif csv_files:\n",
        "                file_name = csv_files[0]\n",
        "                print(f\"Found CSV file: {file_name}\")\n",
        "                import pandas as pd\n",
        "                df = pd.read_csv(file_name, encoding='utf-8')\n",
        "                data = df.to_dict(orient=\"records\")\n",
        "            else:\n",
        "                raise ValueError(\"No Turkish dataset file found. Please place a .json or .csv file in the current directory.\")\n",
        "\n",
        "        print(f\"Loaded {len(data)} records from {file_name}\")\n",
        "\n",
        "        dataset = []\n",
        "        token_stats = []\n",
        "\n",
        "        for i, row in enumerate(data):\n",
        "            if \"soru\" not in row or \"cevap\" not in row:\n",
        "                print(f\"Skipping row {i}: missing 'soru' or 'cevap' field\")\n",
        "                continue\n",
        "\n",
        "            prompt = f\"soru: {row['soru']}\\ncevap:\"\n",
        "            cevap = str(row[\"cevap\"]).strip()\n",
        "            cevap_tokens = self.tokenize(cevap)\n",
        "            dataset.append([prompt, cevap, cevap_tokens])\n",
        "            token_stats.append(len(cevap_tokens))\n",
        "\n",
        "            if i < 5:\n",
        "                print(f\"Example {i}: {prompt[:50]}... -> {cevap} ({len(cevap_tokens)} tokens)\")\n",
        "\n",
        "        # Print tokenization statistics\n",
        "        if token_stats:\n",
        "            print(f\"\\nTokenization Statistics (DeepSeek V3):\")\n",
        "            print(f\"  Average tokens per answer: {np.mean(token_stats):.2f}\")\n",
        "            print(f\"  Min tokens: {np.min(token_stats)}\")\n",
        "            print(f\"  Max tokens: {np.max(token_stats)}\")\n",
        "            print(f\"  Median tokens: {np.median(token_stats):.2f}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Tokenize text using DeepSeek V3's tokenizer\n",
        "\n",
        "        Args:\n",
        "            text: Input text to tokenize (supports Turkish)\n",
        "\n",
        "        Returns:\n",
        "            List of token IDs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Use DeepSeek's tokenizer without special tokens for accurate count\n",
        "            tokens = self.tokenizer.encode(\n",
        "                text,\n",
        "                add_special_tokens=False,\n",
        "                truncation=False,  # Don't truncate to get full token count\n",
        "                return_tensors=None  # Return as list\n",
        "            )\n",
        "            return tokens\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Tokenization failed for text: {text[:50]}... Error: {e}\")\n",
        "            # Return empty list if tokenization fails\n",
        "            return []\n",
        "\n",
        "    def generate_with_temperature(self, prompt: str, temperature: float = 0.5, n: int = 5) -> List[str]:\n",
        "        \"\"\"Generate n completions with specified temperature using DeepSeek API\"\"\"\n",
        "        completions = []\n",
        "\n",
        "        # DeepSeek may not support n parameter directly, so we make multiple calls\n",
        "        for attempt in range(n):\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=20,\n",
        "                    temperature=temperature,\n",
        "                    stop=[\"\\n\", \".\", \"?\"]\n",
        "                )\n",
        "                # Extract just the answer part\n",
        "                full_response = response.choices[0].message.content.strip()\n",
        "                if \"cevap:\" in full_response:\n",
        "                    answer = full_response.split(\"cevap:\")[-1].strip()\n",
        "                else:\n",
        "                    answer = full_response\n",
        "                completions.append(answer)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during generation (attempt {attempt + 1}/{n}): {e}\")\n",
        "                time.sleep(2)\n",
        "                completions.append(\"\")\n",
        "\n",
        "        return completions\n",
        "\n",
        "    def generate_greedy(self, prompt: str) -> str:\n",
        "        \"\"\"Generate with greedy decoding (temperature=0)\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=0,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            # Extract just the answer part\n",
        "            full_response = response.choices[0].message.content.strip()\n",
        "            if \"cevap:\" in full_response:\n",
        "                return full_response.split(\"cevap:\")[-1].strip()\n",
        "            return full_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error during greedy generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return \"\"\n",
        "\n",
        "    def create_knowledge_dataset(self, initial_dataset: List[Tuple], path_to_save: str):\n",
        "        \"\"\"Create knowledge, non-knowledge, and partial knowledge datasets with advanced Turkish matching\"\"\"\n",
        "\n",
        "        # Batch tracking\n",
        "        batch_num = 0\n",
        "        examples_in_batch = 0\n",
        "\n",
        "        # Current batch data - now with three categories\n",
        "        knowledge_dataset = []\n",
        "        partial_knowledge_dataset = []\n",
        "        non_knowledge_dataset = []\n",
        "\n",
        "        # Overall statistics\n",
        "        total_knowledge = 0\n",
        "        total_partial = 0\n",
        "        total_non_knowledge = 0\n",
        "\n",
        "        # Match type statistics\n",
        "        match_type_stats = Counter()\n",
        "\n",
        "        # Metadata for tracking batches\n",
        "        batch_metadata = {\n",
        "            \"model\": self.model_name,\n",
        "            \"tokenizer\": \"DeepSeek-V3\",\n",
        "            \"dataset_name\": self.dataset_name,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"matching_strategy\": \"multi-tier Turkish-aware\",\n",
        "            \"tokenizer_vocab_size\": self.tokenizer.vocab_size,\n",
        "            \"batches\": []\n",
        "        }\n",
        "\n",
        "        few_shot_examples = [\n",
        "            \"soru: Fransa'nƒ±n ba≈ükenti neresidir?\\ncevap: Paris\\n\",\n",
        "            \"soru: Romeo ve Juliet'i kim yazdƒ±?\\ncevap: William Shakespeare\\n\",\n",
        "            \"soru: 64'√ºn karek√∂k√º nedir?\\ncevap: 8\\n\",\n",
        "            \"soru: Kimyasal sembol√º H olan element hangisidir?\\ncevap: Hidrojen\\n\",\n",
        "            \"soru: Japonya'nƒ±n para birimi nedir?\\ncevap: Japon Yeni\\n\"\n",
        "        ]\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {len(initial_dataset)} examples in batches of {self.batch_size}...\")\n",
        "        print(f\"Using advanced Turkish-aware string matching...\")\n",
        "        print(f\"Using DeepSeek model: {self.model_name}\")\n",
        "        print(f\"Using tokenizer: DeepSeek V3 (vocab size: {self.tokenizer.vocab_size})\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Allow resuming from a specific batch\n",
        "        start_batch = 8  # Change this to resume from a different batch\n",
        "        start_index = (start_batch - 1) * self.batch_size\n",
        "\n",
        "        if start_batch > 1:\n",
        "            print(f\"‚ö†Ô∏è  Resuming from batch {start_batch} (index {start_index})\")\n",
        "            initial_dataset = initial_dataset[start_index:]\n",
        "            batch_num = start_batch - 1\n",
        "\n",
        "        # Progress tracking\n",
        "        total_to_process = len(initial_dataset)\n",
        "\n",
        "        for idx, point in enumerate(tqdm(initial_dataset, desc=\"Processing examples\")):\n",
        "            prompt, target_cevap, cevap_tokens = point\n",
        "\n",
        "            # Generate few-shot prompt\n",
        "            few_shot_prompt = \"\".join(random.sample(few_shot_examples, 3))\n",
        "            full_prompt = few_shot_prompt + prompt\n",
        "\n",
        "            # Generate completions\n",
        "            temp_generations = self.generate_with_temperature(full_prompt, temperature=0.5, n=5)\n",
        "            greedy_generation = self.generate_greedy(full_prompt)\n",
        "\n",
        "            # Calculate match scores for all generations\n",
        "            all_generations = temp_generations + [greedy_generation]\n",
        "            scores = []\n",
        "            match_types = []\n",
        "\n",
        "            for gen in all_generations:\n",
        "                score, match_type = self.matcher.calculate_match_score(target_cevap, gen)\n",
        "                scores.append(score)\n",
        "                match_types.append(match_type)\n",
        "                if score > 0:\n",
        "                    match_type_stats[match_type] += 1\n",
        "\n",
        "            # Calculate average score\n",
        "            avg_score = np.mean(scores)\n",
        "            max_score = np.max(scores)\n",
        "            num_matches = sum(1 for s in scores if s > 0)\n",
        "\n",
        "            # Enhanced classification based on scores\n",
        "            example_data = {\n",
        "                \"prompt\": prompt,\n",
        "                \"target\": target_cevap,\n",
        "                \"tokens\": cevap_tokens,\n",
        "                \"avg_score\": avg_score,\n",
        "                \"max_score\": max_score,\n",
        "                \"num_matches\": num_matches,\n",
        "                \"scores\": scores,\n",
        "                \"match_types\": match_types,\n",
        "                \"generations\": all_generations[:3]  # Save first 3 generations for analysis\n",
        "            }\n",
        "\n",
        "            # Classify based on average score\n",
        "            if avg_score >= 0.8:\n",
        "                knowledge_dataset.append(example_data)\n",
        "                category = \"KNOWLEDGE\"\n",
        "            elif avg_score >= 0.3:\n",
        "                partial_knowledge_dataset.append(example_data)\n",
        "                category = \"PARTIAL\"\n",
        "            else:\n",
        "                non_knowledge_dataset.append(example_data)\n",
        "                category = \"NON-KNOWLEDGE\"\n",
        "\n",
        "            examples_in_batch += 1\n",
        "\n",
        "            # Print detailed info for first few examples\n",
        "            if idx < 5:\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(f\"Example {idx}: {prompt[:60]}...\")\n",
        "                print(f\"Target answer: '{target_cevap}' ({len(cevap_tokens)} tokens)\")\n",
        "                print(f\"Greedy generation: '{greedy_generation}'\")\n",
        "                print(f\"Match scores: {[f'{s:.2f}' for s in scores]}\")\n",
        "                print(f\"Match types: {match_types[:2]}...\")\n",
        "                print(f\"Average score: {avg_score:.3f}\")\n",
        "                print(f\"Category: {category}\")\n",
        "                print(f\"{'='*60}\")\n",
        "\n",
        "            # Check if we should save a batch\n",
        "            if examples_in_batch >= self.batch_size or idx == len(initial_dataset) - 1:\n",
        "                batch_num += 1\n",
        "\n",
        "                # Save current batch\n",
        "                batch_info = self._save_batch(\n",
        "                    knowledge_dataset,\n",
        "                    partial_knowledge_dataset,\n",
        "                    non_knowledge_dataset,\n",
        "                    batch_num,\n",
        "                    self.batch_dir\n",
        "                )\n",
        "\n",
        "                # Update totals\n",
        "                total_knowledge += len(knowledge_dataset)\n",
        "                total_partial += len(partial_knowledge_dataset)\n",
        "                total_non_knowledge += len(non_knowledge_dataset)\n",
        "\n",
        "                # Add batch info to metadata\n",
        "                batch_metadata[\"batches\"].append({\n",
        "                    \"batch_number\": batch_num,\n",
        "                    \"examples_processed\": examples_in_batch,\n",
        "                    \"knowledge_count\": len(knowledge_dataset),\n",
        "                    \"partial_count\": len(partial_knowledge_dataset),\n",
        "                    \"non_knowledge_count\": len(non_knowledge_dataset),\n",
        "                    \"total_processed\": idx + 1 + start_index,\n",
        "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                })\n",
        "\n",
        "                print(f\"\\n{'‚îÄ'*50}\")\n",
        "                print(f\"üì¶ Batch {batch_num} Summary\")\n",
        "                print(f\"{'‚îÄ'*50}\")\n",
        "                print(f\"Examples in batch: {examples_in_batch}\")\n",
        "                print(f\"Knowledge (‚â•0.8): {len(knowledge_dataset)} ({'%.1f' % (100*len(knowledge_dataset)/examples_in_batch if examples_in_batch > 0 else 0)}%)\")\n",
        "                print(f\"Partial (0.3-0.8): {len(partial_knowledge_dataset)} ({'%.1f' % (100*len(partial_knowledge_dataset)/examples_in_batch if examples_in_batch > 0 else 0)}%)\")\n",
        "                print(f\"Non-knowledge (<0.3): {len(non_knowledge_dataset)} ({'%.1f' % (100*len(non_knowledge_dataset)/examples_in_batch if examples_in_batch > 0 else 0)}%)\")\n",
        "                print(f\"Total processed: {idx + 1 + start_index}/{len(initial_dataset) + start_index}\")\n",
        "                print(f\"Running totals - K: {total_knowledge}, P: {total_partial}, NK: {total_non_knowledge}\")\n",
        "                print(f\"{'‚îÄ'*50}\\n\")\n",
        "\n",
        "                # Clear current batch data to free memory\n",
        "                knowledge_dataset = []\n",
        "                partial_knowledge_dataset = []\n",
        "                non_knowledge_dataset = []\n",
        "                examples_in_batch = 0\n",
        "\n",
        "                # Save metadata after each batch\n",
        "                metadata_path = os.path.join(path_to_save, f\"{self.model_name.replace('/', '_')}_{self.dataset_name}_metadata.json\")\n",
        "                with open(metadata_path, \"w\", encoding='utf-8') as f:\n",
        "                    json.dump(batch_metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            # Rate limiting to avoid API throttling\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Add match type statistics to metadata\n",
        "        batch_metadata[\"match_type_statistics\"] = dict(match_type_stats)\n",
        "\n",
        "        # Final save of metadata\n",
        "        metadata_path = os.path.join(path_to_save, f\"{self.model_name.replace('/', '_')}_{self.dataset_name}_metadata.json\")\n",
        "        with open(metadata_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(batch_metadata, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n‚úì Saved batch metadata to {metadata_path}\")\n",
        "\n",
        "        # Calculate final percentages\n",
        "        total_processed = total_knowledge + total_partial + total_non_knowledge\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üéâ FINAL RESULTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total batches created: {batch_num}\")\n",
        "        print(f\"Total Knowledge (avg score ‚â•0.8): {total_knowledge} ({'%.1f' % (100*total_knowledge/total_processed if total_processed > 0 else 0)}%)\")\n",
        "        print(f\"Total Partial Knowledge (0.3-0.8): {total_partial} ({'%.1f' % (100*total_partial/total_processed if total_processed > 0 else 0)}%)\")\n",
        "        print(f\"Total Non-knowledge (score <0.3): {total_non_knowledge} ({'%.1f' % (100*total_non_knowledge/total_processed if total_processed > 0 else 0)}%)\")\n",
        "        print(f\"Total examples processed: {total_processed}\")\n",
        "        print(f\"\\n--- Match Type Distribution ---\")\n",
        "        for match_type, count in match_type_stats.most_common():\n",
        "            print(f\"  {match_type}: {count}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Ask if user wants to consolidate batches\n",
        "        if IN_COLAB:\n",
        "            consolidate = input(\"\\nüìÇ Do you want to consolidate all batches into single files? (y/n): \")\n",
        "        else:\n",
        "            # Auto-consolidate in non-interactive environments\n",
        "            consolidate = 'y'\n",
        "            print(\"\\nüìÇ Auto-consolidating batches...\")\n",
        "\n",
        "        if consolidate.lower() == 'y':\n",
        "            self._consolidate_batches(batch_num, self.batch_dir, path_to_save)\n",
        "\n",
        "    def _save_batch(self, knowledge_dataset, partial_dataset, non_knowledge_dataset, batch_num, batch_dir):\n",
        "        \"\"\"Save a single batch of datasets in simple format\"\"\"\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        batch_info = {}\n",
        "\n",
        "        for name, data in {\n",
        "            \"knowledge\": knowledge_dataset,\n",
        "            \"partial_knowledge\": partial_dataset,\n",
        "            \"non_knowledge\": non_knowledge_dataset\n",
        "        }.items():\n",
        "            filename = f\"{model_name_safe}_{dataset_name_safe}_{name}_batch_{batch_num:04d}.json\"\n",
        "            path = os.path.join(batch_dir, filename)\n",
        "\n",
        "            # Convert to simple format: [prompt, target, tokens, match_count]\n",
        "            simple_data = []\n",
        "            for item in data:\n",
        "                # Create simple format matching original structure\n",
        "                simple_item = [\n",
        "                    item[\"prompt\"],\n",
        "                    item[\"target\"],\n",
        "                    item[\"tokens\"],\n",
        "                    item[\"num_matches\"]  # Number of generations that matched (0-6)\n",
        "                ]\n",
        "                simple_data.append(simple_item)\n",
        "\n",
        "            # Save with metadata header for better tracking\n",
        "            batch_data = {\n",
        "                \"batch_number\": batch_num,\n",
        "                \"category\": name,\n",
        "                \"count\": len(simple_data),\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"tokenizer\": \"DeepSeek-V3\",\n",
        "                \"data\": simple_data\n",
        "            }\n",
        "\n",
        "            with open(path, \"w\", encoding='utf-8') as f:\n",
        "                json.dump(batch_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            batch_info[name] = {\n",
        "                \"filename\": filename,\n",
        "                \"count\": len(data),\n",
        "                \"path\": path\n",
        "            }\n",
        "\n",
        "            print(f\"  ‚úì Saved {name} batch {batch_num} to {path} ({len(data)} examples)\")\n",
        "\n",
        "        return batch_info\n",
        "\n",
        "    def _consolidate_batches(self, total_batches: int, batch_dir: str, final_dir: str):\n",
        "        \"\"\"Consolidate all batch files into single files for each category\"\"\"\n",
        "        print(\"\\nüìä Consolidating batches...\")\n",
        "        print(f\"{'‚îÄ'*50}\")\n",
        "\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        consolidated_stats = {}\n",
        "\n",
        "        for category in [\"knowledge\", \"partial_knowledge\", \"non_knowledge\"]:\n",
        "            consolidated_data = []\n",
        "            batch_count = 0\n",
        "            token_counts = []\n",
        "\n",
        "            # Read all batch files for this category\n",
        "            for batch_num in range(1, total_batches + 1):\n",
        "                batch_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_batch_{batch_num:04d}.json\"\n",
        "                batch_path = os.path.join(batch_dir, batch_filename)\n",
        "\n",
        "                if os.path.exists(batch_path):\n",
        "                    with open(batch_path, \"r\", encoding='utf-8') as f:\n",
        "                        batch_content = json.load(f)\n",
        "\n",
        "                        # Extract data from the new batch structure\n",
        "                        if isinstance(batch_content, dict) and \"data\" in batch_content:\n",
        "                            batch_data = batch_content[\"data\"]\n",
        "                        else:\n",
        "                            # Fallback for old format\n",
        "                            batch_data = batch_content\n",
        "\n",
        "                        consolidated_data.extend(batch_data)\n",
        "                        batch_count += 1\n",
        "\n",
        "                        # Collect token counts\n",
        "                        for item in batch_data:\n",
        "                            if len(item) > 2 and item[2]:  # Check if tokens field exists\n",
        "                                token_counts.append(len(item[2]))\n",
        "\n",
        "                    print(f\"  ‚Ä¢ Loaded {len(batch_data)} examples from batch {batch_num} for {category}\")\n",
        "\n",
        "            # Calculate statistics for this category\n",
        "            if consolidated_data:\n",
        "                match_counts = [item[3] for item in consolidated_data if len(item) > 3]  # 4th element is match count\n",
        "\n",
        "                consolidated_stats[category] = {\n",
        "                    \"count\": len(consolidated_data),\n",
        "                    \"avg_matches\": float(np.mean(match_counts)) if match_counts else 0,\n",
        "                    \"min_matches\": int(np.min(match_counts)) if match_counts else 0,\n",
        "                    \"max_matches\": int(np.max(match_counts)) if match_counts else 0,\n",
        "                    \"token_stats\": {\n",
        "                        \"avg_tokens\": float(np.mean(token_counts)) if token_counts else 0,\n",
        "                        \"min_tokens\": int(np.min(token_counts)) if token_counts else 0,\n",
        "                        \"max_tokens\": int(np.max(token_counts)) if token_counts else 0,\n",
        "                        \"median_tokens\": float(np.median(token_counts)) if token_counts else 0\n",
        "                    }\n",
        "                }\n",
        "\n",
        "            # Save consolidated file with metadata\n",
        "            final_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_dataset_consolidated.json\"\n",
        "            final_path = os.path.join(final_dir, final_filename)\n",
        "\n",
        "            consolidated_file = {\n",
        "                \"metadata\": {\n",
        "                    \"category\": category,\n",
        "                    \"total_examples\": len(consolidated_data),\n",
        "                    \"total_batches\": batch_count,\n",
        "                    \"tokenizer\": \"DeepSeek-V3\",\n",
        "                    \"consolidation_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    \"statistics\": consolidated_stats.get(category, {})\n",
        "                },\n",
        "                \"data\": consolidated_data\n",
        "            }\n",
        "\n",
        "            with open(final_path, \"w\", encoding='utf-8') as f:\n",
        "                json.dump(consolidated_file, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"\\n‚úì Saved consolidated {category} dataset:\")\n",
        "            print(f\"  Path: {final_path}\")\n",
        "            print(f\"  Examples: {len(consolidated_data)}\")\n",
        "            if category in consolidated_stats:\n",
        "                stats = consolidated_stats[category]\n",
        "                print(f\"  Match stats - Avg: {stats['avg_matches']:.1f}/6, Min: {stats['min_matches']}, Max: {stats['max_matches']}\")\n",
        "                if stats['token_stats']['avg_tokens'] > 0:\n",
        "                    print(f\"  Token stats - Avg: {stats['token_stats']['avg_tokens']:.1f}, Median: {stats['token_stats']['median_tokens']:.1f}\")\n",
        "\n",
        "        # Save consolidated statistics separately\n",
        "        stats_path = os.path.join(final_dir, f\"{model_name_safe}_{dataset_name_safe}_consolidated_stats.json\")\n",
        "        with open(stats_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(consolidated_stats, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n‚úì Saved consolidated statistics to {stats_path}\")\n",
        "\n",
        "        print(f\"\\n{'‚îÄ'*50}\")\n",
        "        print(\"‚úÖ Consolidation complete!\")\n",
        "        print(f\"{'‚îÄ'*50}\")\n",
        "        for category, stats in consolidated_stats.items():\n",
        "            print(f\"  {category.replace('_', ' ').title()}: {stats['count']} examples\")\n",
        "\n",
        "        # Ask if user wants to delete batch files\n",
        "        if IN_COLAB:\n",
        "            delete_batches = input(\"\\nüóëÔ∏è  Do you want to delete the individual batch files to save space? (y/n): \")\n",
        "        else:\n",
        "            delete_batches = input(\"\\nüóëÔ∏è  Delete individual batch files? (y/n): \")\n",
        "\n",
        "        if delete_batches.lower() == 'y':\n",
        "            import shutil\n",
        "            try:\n",
        "                shutil.rmtree(batch_dir)\n",
        "                os.makedirs(batch_dir, exist_ok=True)  # Recreate empty batch directory\n",
        "                print(\"‚úì Batch files deleted successfully.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error deleting batch files: {e}\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Batch files retained in:\", batch_dir)\n",
        "\n",
        "    def analyze_results(self, consolidated_file: str):\n",
        "        \"\"\"Analyze a consolidated dataset file to understand matching patterns\"\"\"\n",
        "        print(f\"\\nAnalyzing {consolidated_file}...\")\n",
        "\n",
        "        with open(consolidated_file, 'r', encoding='utf-8') as f:\n",
        "            file_content = json.load(f)\n",
        "\n",
        "        # Handle new consolidated format with metadata\n",
        "        if isinstance(file_content, dict) and \"data\" in file_content:\n",
        "            data = file_content[\"data\"]\n",
        "            metadata = file_content.get(\"metadata\", {})\n",
        "\n",
        "            print(\"\\n--- File Metadata ---\")\n",
        "            print(f\"Category: {metadata.get('category', 'Unknown')}\")\n",
        "            print(f\"Total examples: {metadata.get('total_examples', len(data))}\")\n",
        "            print(f\"Tokenizer: {metadata.get('tokenizer', 'Unknown')}\")\n",
        "\n",
        "            if \"statistics\" in metadata and metadata[\"statistics\"]:\n",
        "                stats = metadata[\"statistics\"]\n",
        "                print(f\"\\n--- Match Statistics ---\")\n",
        "                print(f\"Average matches: {stats.get('avg_matches', 0):.1f}/6\")\n",
        "                print(f\"Min matches: {stats.get('min_matches', 0)}\")\n",
        "                print(f\"Max matches: {stats.get('max_matches', 0)}\")\n",
        "\n",
        "                if \"token_stats\" in stats:\n",
        "                    token_stats = stats[\"token_stats\"]\n",
        "                    print(f\"\\n--- Token Statistics (DeepSeek V3) ---\")\n",
        "                    print(f\"Average tokens: {token_stats.get('avg_tokens', 0):.1f}\")\n",
        "                    print(f\"Median tokens: {token_stats.get('median_tokens', 0):.1f}\")\n",
        "                    print(f\"Min tokens: {token_stats.get('min_tokens', 0)}\")\n",
        "                    print(f\"Max tokens: {token_stats.get('max_tokens', 0)}\")\n",
        "        else:\n",
        "            # Handle old format\n",
        "            data = file_content\n",
        "\n",
        "        if not data:\n",
        "            print(\"No data found in file.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nTotal examples in dataset: {len(data)}\")\n",
        "\n",
        "        # Analyze match counts distribution\n",
        "        match_distribution = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
        "\n",
        "        for item in data:\n",
        "            # item format: [prompt, target, tokens, match_count]\n",
        "            if len(item) > 3:\n",
        "                match_count = item[3]\n",
        "                if match_count in match_distribution:\n",
        "                    match_distribution[match_count] += 1\n",
        "\n",
        "        print(\"\\n--- Match Count Distribution ---\")\n",
        "        for count, freq in sorted(match_distribution.items()):\n",
        "            percentage = (freq / len(data)) * 100 if len(data) > 0 else 0\n",
        "            bar = \"‚ñà\" * int(percentage / 2) if percentage > 0 else \"\"\n",
        "            print(f\"{count}/6 matches: {freq:5d} ({percentage:5.1f}%) {bar}\")\n",
        "\n",
        "        # Show some examples with Turkish text\n",
        "        print(\"\\n--- Sample Examples (Turkish Dataset) ---\")\n",
        "        for i, item in enumerate(data[:5]):\n",
        "            print(f\"\\n√ñrnek {i+1}:\")\n",
        "            print(f\"  Soru: {item[0][5:80]}...\")  # Skip \"soru:\" prefix\n",
        "            print(f\"  Cevap: {item[1]}\")\n",
        "            print(f\"  E≈üle≈ümeler: {item[3]}/6\")\n",
        "            if len(item) > 2 and item[2]:\n",
        "                print(f\"  Token sayƒ±sƒ±: {len(item[2])}\")\n",
        "\n",
        "    def get_token_info(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Get detailed tokenization information for Turkish text\n",
        "\n",
        "        Args:\n",
        "            text: Input text to analyze (supports Turkish)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with token information\n",
        "        \"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "\n",
        "        # Test Turkish-specific characters\n",
        "        turkish_chars = sum(1 for c in text if c in 'ƒüƒûƒ±ƒ∞√∂√ñ≈ü≈û√º√ú√ß√á')\n",
        "\n",
        "        return {\n",
        "            \"text\": text[:100] + \"...\" if len(text) > 100 else text,\n",
        "            \"num_tokens\": len(tokens),\n",
        "            \"token_ids\": tokens[:10] + [\"...\"] if len(tokens) > 10 else tokens,\n",
        "            \"avg_chars_per_token\": len(text) / len(tokens) if tokens else 0,\n",
        "            \"contains_turkish_chars\": turkish_chars > 0,\n",
        "            \"turkish_char_count\": turkish_chars\n",
        "        }\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the Turkish knowledge dataset creator\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Turkish Knowledge Dataset Creator with DeepSeek V3\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"path_to_knowledge_dataset\": \"datasets/deepseek/turkish/\",\n",
        "        \"dataset_name\": \"turkish\",\n",
        "        \"model_name\": \"deepseek-chat\",  # or \"deepseek-reasoner\" for reasoning mode\n",
        "        \"batch_size\": 2000,  # Save every 2000 examples\n",
        "        \"tokenizer_model\": \"deepseek-ai/DeepSeek-V3\"  # DeepSeek V3 tokenizer\n",
        "    }\n",
        "\n",
        "    print(\"\\nConfiguration:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print()\n",
        "\n",
        "    # Initialize and run\n",
        "    try:\n",
        "        dataset_creator = KnowledgeDatasetDeepSeek(**config)\n",
        "        print(\"\\n‚úÖ Turkish dataset creation completed successfully!\")\n",
        "\n",
        "        # Optional: Analyze results after processing\n",
        "        # Uncomment to analyze consolidated files\n",
        "        # dataset_creator.analyze_results(\"datasets/deepseek/turkish/deepseek-chat_turkish_knowledge_dataset_consolidated.json\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during dataset creation: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09bfad5df938409b8cfb19d0cbac71c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc60daa4541d423f8630ef03cbd1cfff",
              "IPY_MODEL_8eb246a7e41d426c9fe5ec0cfc067a8d",
              "IPY_MODEL_8e8ba9ddf4574576bdd4fa3b1acf06fa"
            ],
            "layout": "IPY_MODEL_e2f17efc82b14cfc8eb274e1a93a373c"
          }
        },
        "18d3f8133e6f4da28f46fc9e51dbfa39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21258d11c33e4acda3eb7d942123db2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "237b39308a694f13a2080b83c8bfc693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d37298ab14bc4b968cbbc240704e35d0",
              "IPY_MODEL_80b0127198774af8bddbaaec653e0677",
              "IPY_MODEL_79c07308c3e04f62977c5bac6bac3ddc"
            ],
            "layout": "IPY_MODEL_f4c537aba6ac49349d86c3dce8114e18"
          }
        },
        "27422c114358429fb29f43d93fc0bfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29ac1e68fb6f402aa40edf97e703c4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5d55feaee35a4484a0857652125875a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77b237734314458a813fdf163c54eca2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79c07308c3e04f62977c5bac6bac3ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eccdf828247643529b046c64edb4d4a9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5d55feaee35a4484a0857652125875a9",
            "value": "‚Äá3.13k/?‚Äá[00:00&lt;00:00,‚Äá32.1kB/s]"
          }
        },
        "7aee3a31ffda4ef0a50c1608fff4770e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b0127198774af8bddbaaec653e0677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f306fa9a80cd4b2d9bdd9ddb3babcfb0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0e965067422467ab3554535086b2dd4",
            "value": 1
          }
        },
        "8e8ba9ddf4574576bdd4fa3b1acf06fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b237734314458a813fdf163c54eca2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_27422c114358429fb29f43d93fc0bfc2",
            "value": "‚Äá7.85M/?‚Äá[00:00&lt;00:00,‚Äá28.1MB/s]"
          }
        },
        "8eb246a7e41d426c9fe5ec0cfc067a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29ac1e68fb6f402aa40edf97e703c4c5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21258d11c33e4acda3eb7d942123db2f",
            "value": 1
          }
        },
        "d0e965067422467ab3554535086b2dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d37298ab14bc4b968cbbc240704e35d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a511170a624dafb2a8f6a73d95c372",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_18d3f8133e6f4da28f46fc9e51dbfa39",
            "value": "tokenizer_config.json:‚Äá"
          }
        },
        "d815bdb548364fe1ae67b2f6da44d924": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2f17efc82b14cfc8eb274e1a93a373c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4a511170a624dafb2a8f6a73d95c372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eccdf828247643529b046c64edb4d4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f306fa9a80cd4b2d9bdd9ddb3babcfb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f4c537aba6ac49349d86c3dce8114e18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc60daa4541d423f8630ef03cbd1cfff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aee3a31ffda4ef0a50c1608fff4770e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d815bdb548364fe1ae67b2f6da44d924",
            "value": "tokenizer.json:‚Äá"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}