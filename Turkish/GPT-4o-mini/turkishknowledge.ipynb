{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNbpJcdFx-jP",
        "outputId": "d8509d4e-49ef-4621-d0a5-e2470676847f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 15000 rows from turkish.csv to turkish_dataset.json\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple Usage Example for CSV to JSON Converter\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Simple, direct approach\n",
        "def simple_csv_to_json(csv_file, json_file, max_rows=15000):\n",
        "    \"\"\"\n",
        "    Simplest possible implementation\n",
        "    \"\"\"\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Keep first 15000 rows (delete everything from row 15001 onwards)\n",
        "    df = df.iloc[:max_rows]\n",
        "\n",
        "    # Convert to JSON (array of objects format)\n",
        "    df.to_json(json_file, orient='records', indent=2)\n",
        "\n",
        "    print(f\"Converted {len(df)} rows from {csv_file} to {json_file}\")\n",
        "\n",
        "\n",
        "\n",
        "def oneliner_csv_to_json(csv_file, json_file):\n",
        "    \"\"\"Ultra-compact version\"\"\"\n",
        "    pd.read_csv(csv_file).iloc[:15000].to_json(json_file, orient='records', indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Simple usage\n",
        "    simple_csv_to_json('turkish.csv', 'turkish_dataset.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "\n",
        "# --- Colab-specific imports ---\n",
        "from google.colab import userdata, files\n",
        "import io\n",
        "\n",
        "class TurkishTextMatcher:\n",
        "    \"\"\"Advanced Turkish text matching with multi-tier strategy\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Turkish-specific character mappings\n",
        "        self.turkish_lower_map = {\n",
        "            'İ': 'i', 'I': 'ı', 'Ğ': 'ğ', 'Ü': 'ü',\n",
        "            'Ş': 'ş', 'Ö': 'ö', 'Ç': 'ç'\n",
        "        }\n",
        "\n",
        "        # Common stop words to ignore in token matching (Turkish and English)\n",
        "        self.stop_words = {\n",
        "            # Turkish\n",
        "            've', 'veya', 'ile', 'için', 'bir', 'bu', 'da', 'de',\n",
        "            'mi', 'mu', 'mı', 'mü', 'ki', 'ne', 'ya', 'ama', 'fakat',\n",
        "            'çünkü', 'gibi', 'kadar', 'hem', 'daha', 'çok', 'en',\n",
        "            # English\n",
        "            'the', 'of', 'and', 'a', 'an', 'in', 'on', 'at', 'to',\n",
        "            'for', 'with', 'by', 'from', 'as', 'or', 'but', 'is', 'was'\n",
        "        }\n",
        "\n",
        "        # Common abbreviations and variations\n",
        "        self.abbreviations = {\n",
        "            'dr': 'doktor',\n",
        "            'prof': 'profesör',\n",
        "            'st': 'saint',\n",
        "            'abd': 'amerika birleşik devletleri',\n",
        "            'usa': 'amerika birleşik devletleri',\n",
        "            'uk': 'birleşik krallık',\n",
        "            'eu': 'avrupa birliği'\n",
        "        }\n",
        "\n",
        "    def turkish_lower(self, text: str) -> str:\n",
        "        \"\"\"Properly convert Turkish text to lowercase\"\"\"\n",
        "        # First apply Turkish-specific mappings\n",
        "        for upper, lower in self.turkish_lower_map.items():\n",
        "            text = text.replace(upper, lower)\n",
        "        # Then apply standard lowercase\n",
        "        return text.lower()\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Normalize text for comparison\"\"\"\n",
        "        # Convert to lowercase with Turkish awareness\n",
        "        text = self.turkish_lower(text)\n",
        "\n",
        "        # Remove punctuation but keep spaces\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Expand common abbreviations\n",
        "        words = text.split()\n",
        "        words = [self.abbreviations.get(w, w) for w in words]\n",
        "        text = ' '.join(words)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_tokens(self, text: str, remove_stopwords: bool = True) -> List[str]:\n",
        "        \"\"\"Extract meaningful tokens from text\"\"\"\n",
        "        normalized = self.normalize_text(text)\n",
        "        tokens = normalized.split()\n",
        "\n",
        "        if remove_stopwords:\n",
        "            tokens = [t for t in tokens if t not in self.stop_words]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def exact_match(self, answer: str, generation: str) -> float:\n",
        "        \"\"\"Check for exact match after normalization\"\"\"\n",
        "        norm_answer = self.normalize_text(answer)\n",
        "        norm_generation = self.normalize_text(generation)\n",
        "\n",
        "        # Check if exact match or if answer is contained in generation\n",
        "        if norm_answer == norm_generation:\n",
        "            return 1.0\n",
        "        elif norm_answer in norm_generation:\n",
        "            # Penalize slightly if answer is just contained (not exact)\n",
        "            return 0.95\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def token_overlap_match(self, answer: str, generation: str) -> float:\n",
        "        \"\"\"Calculate token overlap score\"\"\"\n",
        "        answer_tokens = set(self.extract_tokens(answer, remove_stopwords=True))\n",
        "        generation_tokens = set(self.extract_tokens(generation, remove_stopwords=False))\n",
        "\n",
        "        if not answer_tokens:\n",
        "            return 0.0\n",
        "\n",
        "        # Check if all answer tokens appear in generation\n",
        "        overlap = answer_tokens.intersection(generation_tokens)\n",
        "        coverage = len(overlap) / len(answer_tokens)\n",
        "\n",
        "        # Bonus if tokens appear in same order\n",
        "        if coverage == 1.0:\n",
        "            answer_list = self.extract_tokens(answer, remove_stopwords=True)\n",
        "            gen_list = self.extract_tokens(generation, remove_stopwords=False)\n",
        "\n",
        "            # Check sequence preservation\n",
        "            try:\n",
        "                indices = [gen_list.index(token) for token in answer_list]\n",
        "                if indices == sorted(indices):\n",
        "                    return 0.9  # Full overlap with correct order\n",
        "                else:\n",
        "                    return 0.8  # Full overlap but different order\n",
        "            except ValueError:\n",
        "                return 0.8\n",
        "\n",
        "        return coverage * 0.8  # Partial overlap\n",
        "\n",
        "    def fuzzy_match(self, answer: str, generation: str, threshold: float = 0.85) -> float:\n",
        "        \"\"\"Fuzzy string matching using edit distance\"\"\"\n",
        "        norm_answer = self.normalize_text(answer)\n",
        "        norm_generation = self.normalize_text(generation)\n",
        "\n",
        "        # For short answers, check if it's contained with small variations\n",
        "        if len(norm_answer) <= 20:\n",
        "            # Use SequenceMatcher for similarity\n",
        "            matcher = SequenceMatcher(None, norm_answer, norm_generation)\n",
        "            similarity = matcher.ratio()\n",
        "\n",
        "            if similarity >= threshold:\n",
        "                return similarity * 0.6  # Scale to max 0.6 for fuzzy matches\n",
        "\n",
        "            # Also check if answer appears as substring with minor variations\n",
        "            words_in_gen = norm_generation.split()\n",
        "            for i in range(len(words_in_gen)):\n",
        "                for j in range(i+1, min(i+6, len(words_in_gen)+1)):\n",
        "                    substring = ' '.join(words_in_gen[i:j])\n",
        "                    matcher = SequenceMatcher(None, norm_answer, substring)\n",
        "                    if matcher.ratio() >= threshold:\n",
        "                        return matcher.ratio() * 0.6\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def partial_credit_match(self, answer: str, generation: str) -> float:\n",
        "        \"\"\"Give partial credit for partially correct answers\"\"\"\n",
        "        answer_tokens = self.extract_tokens(answer, remove_stopwords=False)\n",
        "\n",
        "        # For multi-word answers, check for important parts\n",
        "        if len(answer_tokens) >= 2:\n",
        "            # Check for proper nouns (capitalized in original)\n",
        "            important_parts = []\n",
        "\n",
        "            # Extract likely important parts (names, places, etc.)\n",
        "            original_words = answer.split()\n",
        "            for word in original_words:\n",
        "                if word and word[0].isupper():\n",
        "                    important_parts.append(self.turkish_lower(word))\n",
        "\n",
        "            if not important_parts:\n",
        "                # If no capitalized words, consider all non-stopwords important\n",
        "                important_parts = [t for t in answer_tokens if t not in self.stop_words]\n",
        "\n",
        "            if important_parts:\n",
        "                norm_generation = self.normalize_text(generation)\n",
        "                matches = sum(1 for part in important_parts if part in norm_generation)\n",
        "                return (matches / len(important_parts)) * 0.4\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def calculate_match_score(self, answer: str, generation: str) -> Tuple[float, str]:\n",
        "        \"\"\"\n",
        "        Calculate overall match score using multi-tier strategy\n",
        "        Returns: (score, match_type)\n",
        "        \"\"\"\n",
        "        # Tier 1: Exact match\n",
        "        exact_score = self.exact_match(answer, generation)\n",
        "        if exact_score > 0:\n",
        "            return exact_score, \"exact\"\n",
        "\n",
        "        # Tier 2: Token overlap\n",
        "        token_score = self.token_overlap_match(answer, generation)\n",
        "        if token_score >= 0.8:\n",
        "            return token_score, \"token_overlap\"\n",
        "\n",
        "        # Tier 3: Fuzzy match\n",
        "        fuzzy_score = self.fuzzy_match(answer, generation)\n",
        "        if fuzzy_score > 0:\n",
        "            return fuzzy_score, \"fuzzy\"\n",
        "\n",
        "        # Tier 4: Partial credit\n",
        "        partial_score = self.partial_credit_match(answer, generation)\n",
        "        if partial_score > 0:\n",
        "            return partial_score, \"partial\"\n",
        "\n",
        "        # Also return the best non-zero score if any\n",
        "        best_score = max(token_score, fuzzy_score, partial_score)\n",
        "        if best_score > 0:\n",
        "            if best_score == token_score:\n",
        "                return token_score, \"token_overlap_low\"\n",
        "            elif best_score == fuzzy_score:\n",
        "                return fuzzy_score, \"fuzzy_low\"\n",
        "            else:\n",
        "                return partial_score, \"partial_low\"\n",
        "\n",
        "        return 0.0, \"no_match\"\n",
        "\n",
        "\n",
        "class KnowledgeDatasetGPT:\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_to_knowledge_dataset: str = \"datasets/\",\n",
        "        dataset_name: str = \"turkish\",\n",
        "        model_name: str = \"gpt-4o-mini\",\n",
        "        batch_size: int = 2000\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the knowledge dataset creator for GPT-4o-mini with advanced Turkish matching\n",
        "\n",
        "        Args:\n",
        "            path_to_knowledge_dataset: Path to save the datasets\n",
        "            dataset_name: Name of the dataset (default: \"turkish\")\n",
        "            model_name: OpenAI model name\n",
        "            batch_size: Number of examples to process before saving a batch (default: 2000)\n",
        "        \"\"\"\n",
        "        # --- Get API key securely from Colab ---\n",
        "        api_key = userdata.get('gptapi')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key not found in Colab userdata. Please set it with userdata.set('gptapi', 'your_key').\")\n",
        "\n",
        "        # Set seeds for reproducibility\n",
        "        random.seed(42)\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize tiktoken tokenizer\n",
        "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "        # Initialize Turkish text matcher\n",
        "        self.matcher = TurkishTextMatcher()\n",
        "\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(path_to_knowledge_dataset, exist_ok=True)\n",
        "\n",
        "        # Create batch directory\n",
        "        self.batch_dir = os.path.join(path_to_knowledge_dataset, \"batches\")\n",
        "        os.makedirs(self.batch_dir, exist_ok=True)\n",
        "\n",
        "        # Load initial dataset\n",
        "        initial_dataset = self.load_manual_dataset()\n",
        "\n",
        "        # Create knowledge dataset with batch processing\n",
        "        self.create_knowledge_dataset(initial_dataset, path_to_knowledge_dataset)\n",
        "\n",
        "\n",
        "    def load_manual_dataset(self) -> List[Tuple]:\n",
        "        \"\"\"\n",
        "        Allow manual upload of a dataset file (turkish.json or .csv)\n",
        "        Each row should contain at least 'soru' and 'cevap'.\n",
        "        \"\"\"\n",
        "        print(\"Please upload your dataset file (JSON or CSV)...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded. Please upload your turkish dataset.\")\n",
        "\n",
        "        file_name = list(uploaded.keys())[0]\n",
        "        print(f\"Uploaded file: {file_name}\")\n",
        "\n",
        "        # --- Parse JSON ---\n",
        "        if file_name.endswith(\".json\"):\n",
        "            data = json.load(io.BytesIO(uploaded[file_name]))\n",
        "        # --- Parse CSV ---\n",
        "        elif file_name.endswith(\".csv\"):\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "            data = df.to_dict(orient=\"records\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Please upload a JSON or CSV file.\")\n",
        "\n",
        "        print(f\"Loaded {len(data)} records from {file_name}\")\n",
        "\n",
        "        dataset = []\n",
        "        for i, row in enumerate(data):\n",
        "            if \"soru\" not in row or \"cevap\" not in row:\n",
        "                continue\n",
        "\n",
        "            prompt = f\"soru: {row['soru']}\\ncevap:\"\n",
        "            cevap = str(row[\"cevap\"]).strip()\n",
        "            cevap_tokens = self.tokenize(cevap)\n",
        "            dataset.append([prompt, cevap, cevap_tokens])\n",
        "\n",
        "            if i < 5:\n",
        "                print(f\"Example {i}: {prompt[:50]}... -> {cevap}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"Tokenize text using tiktoken\"\"\"\n",
        "        return self.tokenizer.encode(text)\n",
        "\n",
        "\n",
        "    def generate_with_temperature(self, prompt: str, temperature: float = 0.5, n: int = 5) -> List[str]:\n",
        "        \"\"\"Generate n completions with specified temperature using OpenAI API\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=temperature,\n",
        "                n=n,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            return [choice.message.content.strip() for choice in response.choices]\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return [\"\"] * n\n",
        "\n",
        "\n",
        "    def generate_greedy(self, prompt: str) -> str:\n",
        "        \"\"\"Generate with greedy decoding (temperature=0)\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=20,\n",
        "                temperature=0,\n",
        "                n=1,\n",
        "                stop=[\"\\n\", \".\", \"?\"]\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during greedy generation: {e}\")\n",
        "            time.sleep(2)\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "    def create_knowledge_dataset(self, initial_dataset: List[Tuple], path_to_save: str):\n",
        "        \"\"\"Create knowledge, non-knowledge, and partial knowledge datasets with advanced matching\"\"\"\n",
        "\n",
        "        # Batch tracking\n",
        "        batch_num = 0\n",
        "        examples_in_batch = 0\n",
        "\n",
        "        # Current batch data - now with three categories\n",
        "        knowledge_dataset = []\n",
        "        partial_knowledge_dataset = []\n",
        "        non_knowledge_dataset = []\n",
        "\n",
        "        # Overall statistics\n",
        "        total_knowledge = 0\n",
        "        total_partial = 0\n",
        "        total_non_knowledge = 0\n",
        "\n",
        "        # Match type statistics\n",
        "        match_type_stats = Counter()\n",
        "\n",
        "        # Metadata for tracking batches\n",
        "        batch_metadata = {\n",
        "            \"model\": self.model_name,\n",
        "            \"dataset_name\": self.dataset_name,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"matching_strategy\": \"multi-tier Turkish-aware\",\n",
        "            \"batches\": []\n",
        "        }\n",
        "\n",
        "        few_shot_examples = [\n",
        "            \"soru: Fransa'nın başkenti neresidir?\\ncevap: Paris\\n\",\n",
        "            \"soru: Romeo ve Juliet'i kim yazdı?\\ncevap: William Shakespeare\\n\",\n",
        "            \"soru: 64'ün karekökü nedir?\\ncevap: 8\\n\",\n",
        "            \"soru: Kimyasal sembolü H olan element hangisidir?\\ncevap: Hidrojen\\n\",\n",
        "            \"soru: Japonya'nın para birimi nedir?\\ncevap: Japon Yeni\\n\"\n",
        "        ]\n",
        "\n",
        "        print(f\"Processing {len(initial_dataset)} examples in batches of {self.batch_size}...\")\n",
        "        print(\"Using advanced Turkish-aware string matching...\")\n",
        "\n",
        "        # Allow resuming from a specific batch\n",
        "        start_batch = 1  # Change this to resume from a different batch\n",
        "        start_index = (start_batch - 1) * self.batch_size\n",
        "        initial_dataset = initial_dataset[start_index:]\n",
        "        batch_num = start_batch - 1\n",
        "\n",
        "        for idx, point in enumerate(tqdm(initial_dataset, desc=\"Processing examples\")):\n",
        "            prompt, target_cevap, cevap_tokens = point\n",
        "\n",
        "            # Generate few-shot prompt\n",
        "            few_shot_prompt = \"\".join(random.sample(few_shot_examples, 3))\n",
        "            full_prompt =  few_shot_prompt + prompt\n",
        "\n",
        "            # Generate completions\n",
        "            temp_generations = self.generate_with_temperature(full_prompt, temperature=0.5, n=5)\n",
        "            greedy_generation = self.generate_greedy(full_prompt)\n",
        "\n",
        "            # Calculate match scores for all generations\n",
        "            all_generations = temp_generations + [greedy_generation]\n",
        "            scores = []\n",
        "            match_types = []\n",
        "\n",
        "            for gen in all_generations:\n",
        "                score, match_type = self.matcher.calculate_match_score(target_cevap, gen)\n",
        "                scores.append(score)\n",
        "                match_types.append(match_type)\n",
        "                if score > 0:\n",
        "                    match_type_stats[match_type] += 1\n",
        "\n",
        "            # Calculate average score\n",
        "            avg_score = np.mean(scores)\n",
        "            max_score = np.max(scores)\n",
        "            num_matches = sum(1 for s in scores if s > 0)\n",
        "\n",
        "            # Enhanced classification based on scores\n",
        "            example_data = {\n",
        "                \"prompt\": prompt,\n",
        "                \"target\": target_cevap,\n",
        "                \"tokens\": cevap_tokens,\n",
        "                \"avg_score\": avg_score,\n",
        "                \"max_score\": max_score,\n",
        "                \"num_matches\": num_matches,\n",
        "                \"scores\": scores,\n",
        "                \"match_types\": match_types,\n",
        "                \"generations\": all_generations[:3]  # Save first 3 generations for analysis\n",
        "            }\n",
        "\n",
        "            # Classify based on average score\n",
        "            if avg_score >= 0.8:\n",
        "                knowledge_dataset.append(example_data)\n",
        "                category = \"KNOWLEDGE\"\n",
        "            elif avg_score >= 0.3:\n",
        "                partial_knowledge_dataset.append(example_data)\n",
        "                category = \"PARTIAL\"\n",
        "            else:\n",
        "                non_knowledge_dataset.append(example_data)\n",
        "                category = \"NON-KNOWLEDGE\"\n",
        "\n",
        "            examples_in_batch += 1\n",
        "\n",
        "            # Print detailed info for first few examples\n",
        "            if idx < 5:\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(f\"Example {idx}: {prompt[:60]}...\")\n",
        "                print(f\"Target answer: '{target_cevap}'\")\n",
        "                print(f\"Greedy generation: '{greedy_generation}'\")\n",
        "                print(f\"Match scores: {[f'{s:.2f}' for s in scores]}\")\n",
        "                print(f\"Match types: {match_types[:2]}...\")\n",
        "                print(f\"Average score: {avg_score:.3f}\")\n",
        "                print(f\"Category: {category}\")\n",
        "                print(f\"{'='*60}\")\n",
        "\n",
        "            # Check if we should save a batch\n",
        "            if examples_in_batch >= self.batch_size or idx == len(initial_dataset) - 1:\n",
        "                batch_num += 1\n",
        "\n",
        "                # Save current batch\n",
        "                batch_info = self._save_batch(\n",
        "                    knowledge_dataset,\n",
        "                    partial_knowledge_dataset,\n",
        "                    non_knowledge_dataset,\n",
        "                    batch_num,\n",
        "                    self.batch_dir\n",
        "                )\n",
        "\n",
        "                # Update totals\n",
        "                total_knowledge += len(knowledge_dataset)\n",
        "                total_partial += len(partial_knowledge_dataset)\n",
        "                total_non_knowledge += len(non_knowledge_dataset)\n",
        "\n",
        "                # Add batch info to metadata\n",
        "                batch_metadata[\"batches\"].append({\n",
        "                    \"batch_number\": batch_num,\n",
        "                    \"examples_processed\": examples_in_batch,\n",
        "                    \"knowledge_count\": len(knowledge_dataset),\n",
        "                    \"partial_count\": len(partial_knowledge_dataset),\n",
        "                    \"non_knowledge_count\": len(non_knowledge_dataset),\n",
        "                    \"total_processed\": idx + 1 + start_index\n",
        "                })\n",
        "\n",
        "                print(f\"\\n--- Batch {batch_num} Summary ---\")\n",
        "                print(f\"Examples in batch: {examples_in_batch}\")\n",
        "                print(f\"Knowledge (≥0.8): {len(knowledge_dataset)}\")\n",
        "                print(f\"Partial (0.3-0.8): {len(partial_knowledge_dataset)}\")\n",
        "                print(f\"Non-knowledge (<0.3): {len(non_knowledge_dataset)}\")\n",
        "                print(f\"Total processed so far: {idx + 1 + start_index}/{len(initial_dataset) + start_index}\")\n",
        "                print(f\"Running totals - K: {total_knowledge}, P: {total_partial}, NK: {total_non_knowledge}\")\n",
        "\n",
        "                # Clear current batch data to free memory\n",
        "                knowledge_dataset = []\n",
        "                partial_knowledge_dataset = []\n",
        "                non_knowledge_dataset = []\n",
        "                examples_in_batch = 0\n",
        "\n",
        "            time.sleep(0.1)  # Rate limiting\n",
        "\n",
        "        # Add match type statistics to metadata\n",
        "        batch_metadata[\"match_type_statistics\"] = dict(match_type_stats)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata_path = os.path.join(path_to_save, f\"{self.model_name.replace('/', '_')}_{self.dataset_name}_metadata.json\")\n",
        "        with open(metadata_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(batch_metadata, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\nSaved batch metadata to {metadata_path}\")\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"=== FINAL RESULTS ===\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total batches created: {batch_num}\")\n",
        "        print(f\"Total Knowledge (avg score ≥0.8): {total_knowledge}\")\n",
        "        print(f\"Total Partial Knowledge (0.3-0.8): {total_partial}\")\n",
        "        print(f\"Total Non-knowledge (score <0.3): {total_non_knowledge}\")\n",
        "        print(f\"Total examples processed: {total_knowledge + total_partial + total_non_knowledge}\")\n",
        "        print(f\"\\n--- Match Type Distribution ---\")\n",
        "        for match_type, count in match_type_stats.most_common():\n",
        "            print(f\"{match_type}: {count}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Ask if user wants to consolidate batches\n",
        "        consolidate = input(\"\\nDo you want to consolidate all batches into single files? (y/n): \")\n",
        "        if consolidate.lower() == 'y':\n",
        "            self._consolidate_batches(batch_num, self.batch_dir, path_to_save)\n",
        "\n",
        "\n",
        "    def _save_batch(self, knowledge_dataset, partial_dataset, non_knowledge_dataset, batch_num, batch_dir):\n",
        "        \"\"\"Save a single batch of datasets in simple format\"\"\"\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        batch_info = {}\n",
        "\n",
        "        for name, data in {\n",
        "            \"knowledge\": knowledge_dataset,\n",
        "            \"partial_knowledge\": partial_dataset,\n",
        "            \"non_knowledge\": non_knowledge_dataset\n",
        "        }.items():\n",
        "            filename = f\"{model_name_safe}_{dataset_name_safe}_{name}_batch_{batch_num:04d}.json\"\n",
        "            path = os.path.join(batch_dir, filename)\n",
        "\n",
        "            # Convert to simple format: [prompt, target, tokens, match_count]\n",
        "            simple_data = []\n",
        "            for item in data:\n",
        "                # Create simple format matching original structure\n",
        "                simple_item = [\n",
        "                    item[\"prompt\"],\n",
        "                    item[\"target\"],\n",
        "                    item[\"tokens\"],\n",
        "                    item[\"num_matches\"]  # Number of generations that matched (0-6)\n",
        "                ]\n",
        "                simple_data.append(simple_item)\n",
        "\n",
        "            with open(path, \"w\", encoding='utf-8') as f:\n",
        "                json.dump(simple_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            batch_info[name] = {\n",
        "                \"filename\": filename,\n",
        "                \"count\": len(data),\n",
        "                \"path\": path\n",
        "            }\n",
        "\n",
        "            print(f\"Saved {name} batch {batch_num} to {path} ({len(data)} examples)\")\n",
        "\n",
        "        return batch_info\n",
        "\n",
        "\n",
        "    def _consolidate_batches(self, total_batches: int, batch_dir: str, final_dir: str):\n",
        "        \"\"\"Consolidate all batch files into single files for each category\"\"\"\n",
        "        print(\"\\nConsolidating batches...\")\n",
        "\n",
        "        model_name_safe = self.model_name.replace(\"/\", \"_\")\n",
        "        dataset_name_safe = self.dataset_name.replace(\" \", \"_\")\n",
        "\n",
        "        consolidated_stats = {}\n",
        "\n",
        "        for category in [\"knowledge\", \"partial_knowledge\", \"non_knowledge\"]:\n",
        "            consolidated_data = []\n",
        "\n",
        "            # Read all batch files for this category\n",
        "            for batch_num in range(1, total_batches + 1):\n",
        "                batch_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_batch_{batch_num:04d}.json\"\n",
        "                batch_path = os.path.join(batch_dir, batch_filename)\n",
        "\n",
        "                if os.path.exists(batch_path):\n",
        "                    with open(batch_path, \"r\", encoding='utf-8') as f:\n",
        "                        batch_data = json.load(f)\n",
        "                        consolidated_data.extend(batch_data)\n",
        "\n",
        "                    print(f\"Loaded {len(batch_data)} examples from batch {batch_num} for {category}\")\n",
        "\n",
        "            # Calculate statistics for this category\n",
        "            if consolidated_data:\n",
        "                match_counts = [item[3] for item in consolidated_data]  # 4th element is match count\n",
        "                consolidated_stats[category] = {\n",
        "                    \"count\": len(consolidated_data),\n",
        "                    \"avg_matches\": np.mean(match_counts),\n",
        "                    \"min_matches\": np.min(match_counts),\n",
        "                    \"max_matches\": np.max(match_counts)\n",
        "                }\n",
        "\n",
        "            # Save consolidated file in simple format\n",
        "            final_filename = f\"{model_name_safe}_{dataset_name_safe}_{category}_dataset_consolidated.json\"\n",
        "            final_path = os.path.join(final_dir, final_filename)\n",
        "\n",
        "            with open(final_path, \"w\", encoding='utf-8') as f:\n",
        "                json.dump(consolidated_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"Saved consolidated {category} dataset to {final_path}\")\n",
        "            print(f\"  Total examples: {len(consolidated_data)}\")\n",
        "            if category in consolidated_stats:\n",
        "                stats = consolidated_stats[category]\n",
        "                print(f\"  Match stats - Avg: {stats['avg_matches']:.1f}/6, \"\n",
        "                      f\"Min: {stats['min_matches']}, Max: {stats['max_matches']}\")\n",
        "\n",
        "        # Save consolidated statistics\n",
        "        stats_path = os.path.join(final_dir, f\"{model_name_safe}_{dataset_name_safe}_consolidated_stats.json\")\n",
        "        with open(stats_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(consolidated_stats, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\nSaved consolidated statistics to {stats_path}\")\n",
        "\n",
        "        print(\"\\nConsolidation complete!\")\n",
        "\n",
        "        # Ask if user wants to delete batch files\n",
        "        delete_batches = input(\"\\nDo you want to delete the individual batch files? (y/n): \")\n",
        "        if delete_batches.lower() == 'y':\n",
        "            import shutil\n",
        "            shutil.rmtree(batch_dir)\n",
        "            print(\"Batch files deleted.\")\n",
        "\n",
        "\n",
        "    def analyze_results(self, consolidated_file: str):\n",
        "        \"\"\"Analyze a consolidated dataset file to understand matching patterns\"\"\"\n",
        "        print(f\"\\nAnalyzing {consolidated_file}...\")\n",
        "\n",
        "        with open(consolidated_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if not data:\n",
        "            print(\"No data found in file.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Total examples: {len(data)}\")\n",
        "\n",
        "        # Analyze match counts distribution\n",
        "        match_distribution = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
        "\n",
        "        for item in data:\n",
        "            # item format: [prompt, target, tokens, match_count]\n",
        "            match_count = item[3]\n",
        "            if match_count in match_distribution:\n",
        "                match_distribution[match_count] += 1\n",
        "\n",
        "        print(\"\\n--- Match Count Distribution ---\")\n",
        "        for count, freq in sorted(match_distribution.items()):\n",
        "            percentage = (freq / len(data)) * 100\n",
        "            print(f\"{count}/6 matches: {freq} examples ({percentage:.1f}%)\")\n",
        "\n",
        "        # Show some examples\n",
        "        print(\"\\n--- Sample Examples ---\")\n",
        "        for i, item in enumerate(data[:5]):\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"  Question: {item[0][:80]}...\")\n",
        "            print(f\"  Answer: {item[1]}\")\n",
        "            print(f\"  Matches: {item[3]}/6\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize with batch_size parameter\n",
        "    dataset_creator = KnowledgeDatasetGPT(\n",
        "        path_to_knowledge_dataset=\"datasets/gpt4o_mini/\",\n",
        "        dataset_name=\"turkish\",\n",
        "        model_name=\"gpt-4o-mini\",\n",
        "        batch_size=2000  # Save every 2000 examples\n",
        "    )\n",
        "\n",
        "    # Optional: Analyze results after processing\n",
        "    # dataset_creator.analyze_results(\"datasets/gpt4o_mini/gpt-4o-mini_turkish_knowledge_dataset_consolidated.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bYVCShOisuLl",
        "outputId": "8cc4ee13-05f8-41c4-b079-83634dff2b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your dataset file (JSON or CSV)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9bb29dc-5cca-4d9f-ad6a-78b840dfcaf4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9bb29dc-5cca-4d9f-ad6a-78b840dfcaf4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving turkish_dataset.json to turkish_dataset (1).json\n",
            "Uploaded file: turkish_dataset (1).json\n",
            "Loaded 15000 records from turkish_dataset (1).json\n",
            "Example 0: soru: Henry Ford, Magic Johnson ve Berry Gordy han... -> Michigan\n",
            "Example 1: soru: Ring of Fire' hangi okyanusta bulunuyor?\n",
            "cev... -> Pasifik Okyanusu\n",
            "Example 2: soru: Doğal gazın ana bileşeni nedir?\n",
            "cevap:... -> Metan\n",
            "Example 3: soru: \"Zamanın Müziğine Dans\" adlı 12 ciltlik roma... -> Anthony Powell\n",
            "Example 4: soru: 1999 yılında Avustralya’nın Melbourne kentin... -> Bıyıklar\n",
            "Processing 15000 examples in batches of 2000...\n",
            "Using advanced Turkish-aware string matching...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:   0%|          | 1/15000 [00:01<6:47:44,  1.63s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 0: soru: Henry Ford, Magic Johnson ve Berry Gordy hangi ABD eya...\n",
            "Target answer: 'Michigan'\n",
            "Greedy generation: 'Michigan'\n",
            "Match scores: ['0.95', '0.95', '1.00', '0.95', '0.95', '1.00']\n",
            "Match types: ['exact', 'exact']...\n",
            "Average score: 0.967\n",
            "Category: KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 2/15000 [00:03<6:39:34,  1.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 1: soru: Ring of Fire' hangi okyanusta bulunuyor?\n",
            "cevap:...\n",
            "Target answer: 'Pasifik Okyanusu'\n",
            "Greedy generation: 'cevap: Pasifik Okyanusu'\n",
            "Match scores: ['0.95', '0.95', '0.95', '0.95', '0.95', '0.95']\n",
            "Match types: ['exact', 'exact']...\n",
            "Average score: 0.950\n",
            "Category: KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 3/15000 [00:04<6:39:07,  1.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 2: soru: Doğal gazın ana bileşeni nedir?\n",
            "cevap:...\n",
            "Target answer: 'Metan'\n",
            "Greedy generation: 'cevap: Metan (CH₄)'\n",
            "Match scores: ['0.95', '0.95', '0.95', '0.95', '0.95', '0.95']\n",
            "Match types: ['exact', 'exact']...\n",
            "Average score: 0.950\n",
            "Category: KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 4/15000 [00:07<7:42:20,  1.85s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 3: soru: \"Zamanın Müziğine Dans\" adlı 12 ciltlik roman serisini...\n",
            "Target answer: 'Anthony Powell'\n",
            "Greedy generation: 'cevap: \"Zamanın Müziğine Dans\" adlı roman serisini yazan yazar'\n",
            "Match scores: ['0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
            "Match types: ['no_match', 'no_match']...\n",
            "Average score: 0.000\n",
            "Category: NON-KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing examples:   0%|          | 5/15000 [00:08<7:01:05,  1.68s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Example 4: soru: 1999 yılında Avustralya’nın Melbourne kentinde ortaya ...\n",
            "Target answer: 'Bıyıklar'\n",
            "Greedy generation: 'Movember kampanyası, erkeklerin bıyık bırakmalarını içerir'\n",
            "Match scores: ['0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
            "Match types: ['no_match', 'no_match']...\n",
            "Average score: 0.000\n",
            "Category: NON-KNOWLEDGE\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  13%|█▎        | 2000/15000 [50:07<4:46:52,  1.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 1 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0001.json (1044 examples)\n",
            "Saved partial_knowledge batch 1 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0001.json (285 examples)\n",
            "Saved non_knowledge batch 1 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0001.json (671 examples)\n",
            "\n",
            "--- Batch 1 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1044\n",
            "Partial (0.3-0.8): 285\n",
            "Non-knowledge (<0.3): 671\n",
            "Total processed so far: 2000/15000\n",
            "Running totals - K: 1044, P: 285, NK: 671\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  27%|██▋       | 4000/15000 [1:42:08<4:55:16,  1.61s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 2 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0002.json (1066 examples)\n",
            "Saved partial_knowledge batch 2 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0002.json (269 examples)\n",
            "Saved non_knowledge batch 2 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0002.json (665 examples)\n",
            "\n",
            "--- Batch 2 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1066\n",
            "Partial (0.3-0.8): 269\n",
            "Non-knowledge (<0.3): 665\n",
            "Total processed so far: 4000/15000\n",
            "Running totals - K: 2110, P: 554, NK: 1336\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  40%|████      | 6000/15000 [2:45:19<4:07:40,  1.65s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 3 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0003.json (1098 examples)\n",
            "Saved partial_knowledge batch 3 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0003.json (267 examples)\n",
            "Saved non_knowledge batch 3 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0003.json (635 examples)\n",
            "\n",
            "--- Batch 3 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1098\n",
            "Partial (0.3-0.8): 267\n",
            "Non-knowledge (<0.3): 635\n",
            "Total processed so far: 6000/15000\n",
            "Running totals - K: 3208, P: 821, NK: 1971\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing examples:  53%|█████▎    | 8000/15000 [3:46:04<3:11:34,  1.64s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved knowledge batch 4 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0004.json (1022 examples)\n",
            "Saved partial_knowledge batch 4 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0004.json (283 examples)\n",
            "Saved non_knowledge batch 4 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0004.json (695 examples)\n",
            "\n",
            "--- Batch 4 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1022\n",
            "Partial (0.3-0.8): 283\n",
            "Non-knowledge (<0.3): 695\n",
            "Total processed so far: 8000/15000\n",
            "Running totals - K: 4230, P: 1104, NK: 2666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing examples:  67%|██████▋   | 10000/15000 [4:47:32<2:11:50,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved knowledge batch 5 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0005.json (1048 examples)\n",
            "Saved partial_knowledge batch 5 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0005.json (276 examples)\n",
            "Saved non_knowledge batch 5 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0005.json (676 examples)\n",
            "\n",
            "--- Batch 5 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1048\n",
            "Partial (0.3-0.8): 276\n",
            "Non-knowledge (<0.3): 676\n",
            "Total processed so far: 10000/15000\n",
            "Running totals - K: 5278, P: 1380, NK: 3342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing examples:  80%|████████  | 12000/15000 [5:47:58<1:30:16,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved knowledge batch 6 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0006.json (1076 examples)\n",
            "Saved partial_knowledge batch 6 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0006.json (260 examples)\n",
            "Saved non_knowledge batch 6 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0006.json (664 examples)\n",
            "\n",
            "--- Batch 6 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1076\n",
            "Partial (0.3-0.8): 260\n",
            "Non-knowledge (<0.3): 664\n",
            "Total processed so far: 12000/15000\n",
            "Running totals - K: 6354, P: 1640, NK: 4006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing examples:  93%|█████████▎| 14000/15000 [6:46:42<25:46,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved knowledge batch 7 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0007.json (1089 examples)\n",
            "Saved partial_knowledge batch 7 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0007.json (278 examples)\n",
            "Saved non_knowledge batch 7 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0007.json (633 examples)\n",
            "\n",
            "--- Batch 7 Summary ---\n",
            "Examples in batch: 2000\n",
            "Knowledge (≥0.8): 1089\n",
            "Partial (0.3-0.8): 278\n",
            "Non-knowledge (<0.3): 633\n",
            "Total processed so far: 14000/15000\n",
            "Running totals - K: 7443, P: 1918, NK: 4639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing examples: 100%|██████████| 15000/15000 [7:12:33<00:00,  1.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved knowledge batch 8 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_knowledge_batch_0008.json (513 examples)\n",
            "Saved partial_knowledge batch 8 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_partial_knowledge_batch_0008.json (138 examples)\n",
            "Saved non_knowledge batch 8 to datasets/gpt4o_mini/batches/gpt-4o-mini_turkish_non_knowledge_batch_0008.json (349 examples)\n",
            "\n",
            "--- Batch 8 Summary ---\n",
            "Examples in batch: 1000\n",
            "Knowledge (≥0.8): 513\n",
            "Partial (0.3-0.8): 138\n",
            "Non-knowledge (<0.3): 349\n",
            "Total processed so far: 15000/15000\n",
            "Running totals - K: 7956, P: 2056, NK: 4988\n",
            "\n",
            "Saved batch metadata to datasets/gpt4o_mini/gpt-4o-mini_turkish_metadata.json\n",
            "\n",
            "============================================================\n",
            "=== FINAL RESULTS ===\n",
            "============================================================\n",
            "Total batches created: 8\n",
            "Total Knowledge (avg score ≥0.8): 7956\n",
            "Total Partial Knowledge (0.3-0.8): 2056\n",
            "Total Non-knowledge (score <0.3): 4988\n",
            "Total examples processed: 15000\n",
            "\n",
            "--- Match Type Distribution ---\n",
            "exact: 52603\n",
            "partial: 3846\n",
            "fuzzy: 2170\n",
            "token_overlap_low: 793\n",
            "token_overlap: 649\n",
            "============================================================\n",
            "\n",
            "Do you want to consolidate all batches into single files? (y/n): y\n",
            "\n",
            "Consolidating batches...\n",
            "Loaded 1044 examples from batch 1 for knowledge\n",
            "Loaded 1066 examples from batch 2 for knowledge\n",
            "Loaded 1098 examples from batch 3 for knowledge\n",
            "Loaded 1022 examples from batch 4 for knowledge\n",
            "Loaded 1048 examples from batch 5 for knowledge\n",
            "Loaded 1076 examples from batch 6 for knowledge\n",
            "Loaded 1089 examples from batch 7 for knowledge\n",
            "Loaded 513 examples from batch 8 for knowledge\n",
            "Saved consolidated knowledge dataset to datasets/gpt4o_mini/gpt-4o-mini_turkish_knowledge_dataset_consolidated.json\n",
            "  Total examples: 7956\n",
            "  Match stats - Avg: 6.0/6, Min: 5, Max: 6\n",
            "Loaded 285 examples from batch 1 for partial_knowledge\n",
            "Loaded 269 examples from batch 2 for partial_knowledge\n",
            "Loaded 267 examples from batch 3 for partial_knowledge\n",
            "Loaded 283 examples from batch 4 for partial_knowledge\n",
            "Loaded 276 examples from batch 5 for partial_knowledge\n",
            "Loaded 260 examples from batch 6 for partial_knowledge\n",
            "Loaded 278 examples from batch 7 for partial_knowledge\n",
            "Loaded 138 examples from batch 8 for partial_knowledge\n",
            "Saved consolidated partial_knowledge dataset to datasets/gpt4o_mini/gpt-4o-mini_turkish_partial_knowledge_dataset_consolidated.json\n",
            "  Total examples: 2056\n",
            "  Match stats - Avg: 4.5/6, Min: 2, Max: 6\n",
            "Loaded 671 examples from batch 1 for non_knowledge\n",
            "Loaded 665 examples from batch 2 for non_knowledge\n",
            "Loaded 635 examples from batch 3 for non_knowledge\n",
            "Loaded 695 examples from batch 4 for non_knowledge\n",
            "Loaded 676 examples from batch 5 for non_knowledge\n",
            "Loaded 664 examples from batch 6 for non_knowledge\n",
            "Loaded 633 examples from batch 7 for non_knowledge\n",
            "Loaded 349 examples from batch 8 for non_knowledge\n",
            "Saved consolidated non_knowledge dataset to datasets/gpt4o_mini/gpt-4o-mini_turkish_non_knowledge_dataset_consolidated.json\n",
            "  Total examples: 4988\n",
            "  Match stats - Avg: 0.6/6, Min: 0, Max: 6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3164806282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;31m# Initialize with batch_size parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m     dataset_creator = KnowledgeDatasetGPT(\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0mpath_to_knowledge_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"datasets/gpt4o_mini/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"turkish\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3164806282.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_knowledge_dataset, dataset_name, model_name, batch_size)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Create knowledge dataset with batch processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_knowledge_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_knowledge_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3164806282.py\u001b[0m in \u001b[0;36mcreate_knowledge_dataset\u001b[0;34m(self, initial_dataset, path_to_save)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mconsolidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDo you want to consolidate all batches into single files? (y/n): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3164806282.py\u001b[0m in \u001b[0;36m_consolidate_batches\u001b[0;34m(self, total_batches, batch_dir, final_dir)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mstats_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{model_name_safe}_{dataset_name_safe}_consolidated_stats.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nSaved consolidated statistics to {stats_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Script to combine multiple JSON batch files into a single JSON file.\n",
        "Combines files: gpt-4o-mini_turkish_knowledge_batch_0001.json through batch_0008.json\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Union\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def combine_json_batch_files(\n",
        "    input_directory: str,\n",
        "    output_file: str,\n",
        "    file_prefix: str = \"gpt-4o-mini_turkish_knowledge_batch_\",\n",
        "    start_num: int = 1,\n",
        "    end_num: int = 8\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Combine multiple JSON batch files into a single JSON file.\n",
        "\n",
        "    Args:\n",
        "        input_directory: Directory containing the batch files\n",
        "        output_file: Path for the combined output file\n",
        "        file_prefix: Common prefix for batch files\n",
        "        start_num: Starting batch number (inclusive)\n",
        "        end_num: Ending batch number (inclusive)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with statistics about the combination process\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize statistics\n",
        "    stats = {\n",
        "        'files_processed': 0,\n",
        "        'total_items': 0,\n",
        "        'errors': [],\n",
        "        'missing_files': [],\n",
        "        'output_file': output_file\n",
        "    }\n",
        "\n",
        "    # Initialize combined data container\n",
        "    combined_data = []\n",
        "\n",
        "    # Create Path object for input directory\n",
        "    input_path = Path(input_directory)\n",
        "\n",
        "    # Check if input directory exists\n",
        "    if not input_path.exists():\n",
        "        error_msg = f\"Input directory does not exist: {input_directory}\"\n",
        "        logger.error(error_msg)\n",
        "        raise FileNotFoundError(error_msg)\n",
        "\n",
        "    # Process each batch file\n",
        "    for batch_num in range(start_num, end_num + 1):\n",
        "        # Construct filename with zero-padded number\n",
        "        filename = f\"{file_prefix}{batch_num:04d}.json\"\n",
        "        file_path = input_path / filename\n",
        "\n",
        "        logger.info(f\"Processing file {batch_num}/{end_num}: {filename}\")\n",
        "\n",
        "        # Check if file exists\n",
        "        if not file_path.exists():\n",
        "            logger.warning(f\"File not found: {file_path}\")\n",
        "            stats['missing_files'].append(str(file_path))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Read and parse JSON file\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "                # Handle different JSON structures\n",
        "                if isinstance(data, list):\n",
        "                    # If it's an array, extend the combined data\n",
        "                    combined_data.extend(data)\n",
        "                    items_added = len(data)\n",
        "\n",
        "                elif isinstance(data, dict):\n",
        "                    # If it's an object, check for common patterns\n",
        "                    if 'data' in data and isinstance(data['data'], list):\n",
        "                        # Common pattern: {\"data\": [...]}\n",
        "                        combined_data.extend(data['data'])\n",
        "                        items_added = len(data['data'])\n",
        "                    elif 'items' in data and isinstance(data['items'], list):\n",
        "                        # Alternative pattern: {\"items\": [...]}\n",
        "                        combined_data.extend(data['items'])\n",
        "                        items_added = len(data['items'])\n",
        "                    else:\n",
        "                        # Single object, add as item\n",
        "                        combined_data.append(data)\n",
        "                        items_added = 1\n",
        "                else:\n",
        "                    # Unknown structure, add as is\n",
        "                    combined_data.append(data)\n",
        "                    items_added = 1\n",
        "\n",
        "                stats['files_processed'] += 1\n",
        "                stats['total_items'] += items_added\n",
        "                logger.info(f\"  Added {items_added} items from {filename}\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            error_msg = f\"JSON decode error in {file_path}: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            stats['errors'].append(error_msg)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Unexpected error reading {file_path}: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            stats['errors'].append(error_msg)\n",
        "\n",
        "    # Write combined data to output file\n",
        "    try:\n",
        "        output_path = Path(output_file)\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Write combined JSON\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"Successfully wrote combined data to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error writing output file: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        stats['errors'].append(error_msg)\n",
        "        raise\n",
        "\n",
        "    # Log summary statistics\n",
        "    logger.info(\"=\" * 50)\n",
        "    logger.info(\"COMBINATION SUMMARY:\")\n",
        "    logger.info(f\"  Files processed: {stats['files_processed']}/{end_num - start_num + 1}\")\n",
        "    logger.info(f\"  Total items combined: {stats['total_items']}\")\n",
        "    logger.info(f\"  Output file: {stats['output_file']}\")\n",
        "\n",
        "    if stats['missing_files']:\n",
        "        logger.warning(f\"  Missing files: {len(stats['missing_files'])}\")\n",
        "        for file in stats['missing_files']:\n",
        "            logger.warning(f\"    - {file}\")\n",
        "\n",
        "    if stats['errors']:\n",
        "        logger.error(f\"  Errors encountered: {len(stats['errors'])}\")\n",
        "        for error in stats['errors']:\n",
        "            logger.error(f\"    - {error}\")\n",
        "\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the batch combination process.\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    INPUT_DIR = \"/content/datasets/gpt4o_mini/batches\"\n",
        "    OUTPUT_FILE = \"/content/datasets/gpt4o_mini/combined_turkish_knowledge.json\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Run the combination process\n",
        "        stats = combine_json_batch_files(\n",
        "            input_directory=INPUT_DIR,\n",
        "            output_file=OUTPUT_FILE,\n",
        "            file_prefix=\"gpt-4o-mini_turkish_knowledge_batch_\",\n",
        "            start_num=1,\n",
        "            end_num=8\n",
        "        )\n",
        "\n",
        "        # Print final success message\n",
        "        print(f\"\\n✅ Successfully combined {stats['files_processed']} files\")\n",
        "        print(f\"📊 Total items in combined file: {stats['total_items']}\")\n",
        "        print(f\"📁 Output saved to: {stats['output_file']}\")\n",
        "\n",
        "        # Return success code\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Failed to combine files: {str(e)}\")\n",
        "        return 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    exit(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrkX6RrYWV0G",
        "outputId": "6f5031ca-8b8f-4b76-d619-b7e73fd2264c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Successfully combined 8 files\n",
            "📊 Total items in combined file: 7956\n",
            "📁 Output saved to: /content/datasets/gpt4o_mini/combined_turkish_knowledge.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjXuG3YjXj7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Script to combine multiple JSON batch files into a single JSON file.\n",
        "Combines files: gpt-4o-mini_turkish_knowledge_batch_0001.json through batch_0008.json\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Union\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def combine_json_batch_files(\n",
        "    input_directory: str,\n",
        "    output_file: str,\n",
        "    file_prefix: str = \"gpt-4o-mini_turkish_non_knowledge_batch_\",\n",
        "    start_num: int = 1,\n",
        "    end_num: int = 8\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Combine multiple JSON batch files into a single JSON file.\n",
        "\n",
        "    Args:\n",
        "        input_directory: Directory containing the batch files\n",
        "        output_file: Path for the combined output file\n",
        "        file_prefix: Common prefix for batch files\n",
        "        start_num: Starting batch number (inclusive)\n",
        "        end_num: Ending batch number (inclusive)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with statistics about the combination process\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize statistics\n",
        "    stats = {\n",
        "        'files_processed': 0,\n",
        "        'total_items': 0,\n",
        "        'errors': [],\n",
        "        'missing_files': [],\n",
        "        'output_file': output_file\n",
        "    }\n",
        "\n",
        "    # Initialize combined data container\n",
        "    combined_data = []\n",
        "\n",
        "    # Create Path object for input directory\n",
        "    input_path = Path(input_directory)\n",
        "\n",
        "    # Check if input directory exists\n",
        "    if not input_path.exists():\n",
        "        error_msg = f\"Input directory does not exist: {input_directory}\"\n",
        "        logger.error(error_msg)\n",
        "        raise FileNotFoundError(error_msg)\n",
        "\n",
        "    # Process each batch file\n",
        "    for batch_num in range(start_num, end_num + 1):\n",
        "        # Construct filename with zero-padded number\n",
        "        filename = f\"{file_prefix}{batch_num:04d}.json\"\n",
        "        file_path = input_path / filename\n",
        "\n",
        "        logger.info(f\"Processing file {batch_num}/{end_num}: {filename}\")\n",
        "\n",
        "        # Check if file exists\n",
        "        if not file_path.exists():\n",
        "            logger.warning(f\"File not found: {file_path}\")\n",
        "            stats['missing_files'].append(str(file_path))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Read and parse JSON file\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "                # Handle different JSON structures\n",
        "                if isinstance(data, list):\n",
        "                    # If it's an array, extend the combined data\n",
        "                    combined_data.extend(data)\n",
        "                    items_added = len(data)\n",
        "\n",
        "                elif isinstance(data, dict):\n",
        "                    # If it's an object, check for common patterns\n",
        "                    if 'data' in data and isinstance(data['data'], list):\n",
        "                        # Common pattern: {\"data\": [...]}\n",
        "                        combined_data.extend(data['data'])\n",
        "                        items_added = len(data['data'])\n",
        "                    elif 'items' in data and isinstance(data['items'], list):\n",
        "                        # Alternative pattern: {\"items\": [...]}\n",
        "                        combined_data.extend(data['items'])\n",
        "                        items_added = len(data['items'])\n",
        "                    else:\n",
        "                        # Single object, add as item\n",
        "                        combined_data.append(data)\n",
        "                        items_added = 1\n",
        "                else:\n",
        "                    # Unknown structure, add as is\n",
        "                    combined_data.append(data)\n",
        "                    items_added = 1\n",
        "\n",
        "                stats['files_processed'] += 1\n",
        "                stats['total_items'] += items_added\n",
        "                logger.info(f\"  Added {items_added} items from {filename}\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            error_msg = f\"JSON decode error in {file_path}: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            stats['errors'].append(error_msg)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Unexpected error reading {file_path}: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            stats['errors'].append(error_msg)\n",
        "\n",
        "    # Write combined data to output file\n",
        "    try:\n",
        "        output_path = Path(output_file)\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Write combined JSON\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"Successfully wrote combined data to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error writing output file: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        stats['errors'].append(error_msg)\n",
        "        raise\n",
        "\n",
        "    # Log summary statistics\n",
        "    logger.info(\"=\" * 50)\n",
        "    logger.info(\"COMBINATION SUMMARY:\")\n",
        "    logger.info(f\"  Files processed: {stats['files_processed']}/{end_num - start_num + 1}\")\n",
        "    logger.info(f\"  Total items combined: {stats['total_items']}\")\n",
        "    logger.info(f\"  Output file: {stats['output_file']}\")\n",
        "\n",
        "    if stats['missing_files']:\n",
        "        logger.warning(f\"  Missing files: {len(stats['missing_files'])}\")\n",
        "        for file in stats['missing_files']:\n",
        "            logger.warning(f\"    - {file}\")\n",
        "\n",
        "    if stats['errors']:\n",
        "        logger.error(f\"  Errors encountered: {len(stats['errors'])}\")\n",
        "        for error in stats['errors']:\n",
        "            logger.error(f\"    - {error}\")\n",
        "\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the batch combination process.\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    INPUT_DIR = \"/content/datasets/gpt4o_mini/batches\"\n",
        "    OUTPUT_FILE = \"/content/datasets/gpt4o_mini/combined_turkish_non_knowledge.json\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Run the combination process\n",
        "        stats = combine_json_batch_files(\n",
        "            input_directory=INPUT_DIR,\n",
        "            output_file=OUTPUT_FILE,\n",
        "            file_prefix=\"gpt-4o-mini_turkish_non_knowledge_batch_\",\n",
        "            start_num=1,\n",
        "            end_num=8\n",
        "        )\n",
        "\n",
        "        # Print final success message\n",
        "        print(f\"\\n✅ Successfully combined {stats['files_processed']} files\")\n",
        "        print(f\"📊 Total items in combined file: {stats['total_items']}\")\n",
        "        print(f\"📁 Output saved to: {stats['output_file']}\")\n",
        "\n",
        "        # Return success code\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Failed to combine files: {str(e)}\")\n",
        "        return 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    exit(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7a7922-7357-43e1-dc03-a913db9164f0",
        "id": "Cbgd7BkWXkKr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Successfully combined 8 files\n",
            "📊 Total items in combined file: 4988\n",
            "📁 Output saved to: /content/datasets/gpt4o_mini/combined_turkish_non_knowledge.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}